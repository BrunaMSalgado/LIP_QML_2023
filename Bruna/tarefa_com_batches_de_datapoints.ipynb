{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.optimize import NesterovMomentumOptimizer\n",
    "from pennylane.optimize import AdamOptimizer\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.templates.embeddings import AngleEmbedding\n",
    "from tqdm.notebook import tqdm, trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 90548\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Electron_Multi</th>\n",
       "      <th>FatJet1_Eta</th>\n",
       "      <th>FatJet1_Mass</th>\n",
       "      <th>FatJet1_PT</th>\n",
       "      <th>FatJet1_Phi</th>\n",
       "      <th>FatJet1_Tau1</th>\n",
       "      <th>FatJet1_Tau2</th>\n",
       "      <th>FatJet1_Tau3</th>\n",
       "      <th>FatJet1_Tau4</th>\n",
       "      <th>FatJet1_Tau5</th>\n",
       "      <th>...</th>\n",
       "      <th>gen_decay2</th>\n",
       "      <th>gen_decay_filter</th>\n",
       "      <th>gen_filter</th>\n",
       "      <th>gen_label</th>\n",
       "      <th>gen_n_btags</th>\n",
       "      <th>gen_sample</th>\n",
       "      <th>gen_sample_filter</th>\n",
       "      <th>gen_split</th>\n",
       "      <th>gen_weights</th>\n",
       "      <th>gen_xsec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1.408853</td>\n",
       "      <td>15.150869</td>\n",
       "      <td>339.182312</td>\n",
       "      <td>2.350262</td>\n",
       "      <td>1.396943</td>\n",
       "      <td>0.710451</td>\n",
       "      <td>0.109013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>PyDelphes</td>\n",
       "      <td>signal</td>\n",
       "      <td>1</td>\n",
       "      <td>tZFCNC</td>\n",
       "      <td>tZFCNC_PyDelphes</td>\n",
       "      <td>test</td>\n",
       "      <td>7.762202e-09</td>\n",
       "      <td>0.001285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-2.481838</td>\n",
       "      <td>7.208333</td>\n",
       "      <td>247.036240</td>\n",
       "      <td>-2.280740</td>\n",
       "      <td>0.428710</td>\n",
       "      <td>0.205213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>PyDelphes</td>\n",
       "      <td>signal</td>\n",
       "      <td>1</td>\n",
       "      <td>tZFCNC</td>\n",
       "      <td>tZFCNC_PyDelphes</td>\n",
       "      <td>val</td>\n",
       "      <td>7.762202e-09</td>\n",
       "      <td>0.001285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.476267</td>\n",
       "      <td>94.220718</td>\n",
       "      <td>238.014694</td>\n",
       "      <td>-1.788097</td>\n",
       "      <td>94.256210</td>\n",
       "      <td>2.418446</td>\n",
       "      <td>1.585315</td>\n",
       "      <td>1.127324</td>\n",
       "      <td>0.431098</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>PyDelphes</td>\n",
       "      <td>signal</td>\n",
       "      <td>1</td>\n",
       "      <td>tZFCNC</td>\n",
       "      <td>tZFCNC_PyDelphes</td>\n",
       "      <td>train</td>\n",
       "      <td>7.762249e-09</td>\n",
       "      <td>0.001285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.647480</td>\n",
       "      <td>13.459283</td>\n",
       "      <td>230.971832</td>\n",
       "      <td>-1.032663</td>\n",
       "      <td>1.227122</td>\n",
       "      <td>0.467150</td>\n",
       "      <td>0.164008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>PyDelphes</td>\n",
       "      <td>signal</td>\n",
       "      <td>1</td>\n",
       "      <td>tZFCNC</td>\n",
       "      <td>tZFCNC_PyDelphes</td>\n",
       "      <td>train</td>\n",
       "      <td>7.762249e-09</td>\n",
       "      <td>0.001285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2.106436</td>\n",
       "      <td>97.490242</td>\n",
       "      <td>698.399902</td>\n",
       "      <td>-3.059983</td>\n",
       "      <td>36.555862</td>\n",
       "      <td>2.937936</td>\n",
       "      <td>1.799140</td>\n",
       "      <td>1.093004</td>\n",
       "      <td>0.589724</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>PyDelphes</td>\n",
       "      <td>signal</td>\n",
       "      <td>1</td>\n",
       "      <td>tZFCNC</td>\n",
       "      <td>tZFCNC_PyDelphes</td>\n",
       "      <td>train</td>\n",
       "      <td>7.762249e-09</td>\n",
       "      <td>0.001285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Electron_Multi  FatJet1_Eta  FatJet1_Mass  FatJet1_PT  FatJet1_Phi  \\\n",
       "0               2     1.408853     15.150869  339.182312     2.350262   \n",
       "1               1    -2.481838      7.208333  247.036240    -2.280740   \n",
       "2               0     1.476267     94.220718  238.014694    -1.788097   \n",
       "3               1     0.647480     13.459283  230.971832    -1.032663   \n",
       "4               0     2.106436     97.490242  698.399902    -3.059983   \n",
       "\n",
       "   FatJet1_Tau1  FatJet1_Tau2  FatJet1_Tau3  FatJet1_Tau4  FatJet1_Tau5  ...  \\\n",
       "0      1.396943      0.710451      0.109013      0.000000      0.000000  ...   \n",
       "1      0.428710      0.205213      0.000000      0.000000      0.000000  ...   \n",
       "2     94.256210      2.418446      1.585315      1.127324      0.431098  ...   \n",
       "3      1.227122      0.467150      0.164008      0.000000      0.000000  ...   \n",
       "4     36.555862      2.937936      1.799140      1.093004      0.589724  ...   \n",
       "\n",
       "   gen_decay2  gen_decay_filter  gen_filter  gen_label  gen_n_btags  \\\n",
       "0           0              None   PyDelphes     signal            1   \n",
       "1           0              None   PyDelphes     signal            1   \n",
       "2           0              None   PyDelphes     signal            1   \n",
       "3           0              None   PyDelphes     signal            1   \n",
       "4           0              None   PyDelphes     signal            1   \n",
       "\n",
       "   gen_sample  gen_sample_filter  gen_split   gen_weights  gen_xsec  \n",
       "0      tZFCNC   tZFCNC_PyDelphes       test  7.762202e-09  0.001285  \n",
       "1      tZFCNC   tZFCNC_PyDelphes        val  7.762202e-09  0.001285  \n",
       "2      tZFCNC   tZFCNC_PyDelphes      train  7.762249e-09  0.001285  \n",
       "3      tZFCNC   tZFCNC_PyDelphes      train  7.762249e-09  0.001285  \n",
       "4      tZFCNC   tZFCNC_PyDelphes      train  7.762249e-09  0.001285  \n",
       "\n",
       "[5 rows x 83 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Read the HDF5 file using pandas\n",
    "data_frame_fcnc = pd.read_hdf('fcnc_pythia_sanitised_features.h5')\n",
    "\n",
    "# Get the number of rows\n",
    "num_rows = data_frame_fcnc.shape[0]\n",
    "\n",
    "print('Number of rows: {}'.format(num_rows))\n",
    "\n",
    "# Explore the data\n",
    "data_frame_fcnc.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1002490\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Electron_Multi</th>\n",
       "      <th>FatJet1_Eta</th>\n",
       "      <th>FatJet1_Mass</th>\n",
       "      <th>FatJet1_PT</th>\n",
       "      <th>FatJet1_Phi</th>\n",
       "      <th>FatJet1_Tau1</th>\n",
       "      <th>FatJet1_Tau2</th>\n",
       "      <th>FatJet1_Tau3</th>\n",
       "      <th>FatJet1_Tau4</th>\n",
       "      <th>FatJet1_Tau5</th>\n",
       "      <th>...</th>\n",
       "      <th>gen_decay2</th>\n",
       "      <th>gen_decay_filter</th>\n",
       "      <th>gen_filter</th>\n",
       "      <th>gen_label</th>\n",
       "      <th>gen_n_btags</th>\n",
       "      <th>gen_sample</th>\n",
       "      <th>gen_sample_filter</th>\n",
       "      <th>gen_split</th>\n",
       "      <th>gen_weights</th>\n",
       "      <th>gen_xsec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.988600</td>\n",
       "      <td>52.710262</td>\n",
       "      <td>229.350952</td>\n",
       "      <td>0.728242</td>\n",
       "      <td>36.148926</td>\n",
       "      <td>23.039709</td>\n",
       "      <td>16.949991</td>\n",
       "      <td>14.424411</td>\n",
       "      <td>12.000529</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2L</td>\n",
       "      <td>HT250to500</td>\n",
       "      <td>bkg</td>\n",
       "      <td>1</td>\n",
       "      <td>Zjj</td>\n",
       "      <td>Zjj_HT250to500</td>\n",
       "      <td>train</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>11.9635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.528382</td>\n",
       "      <td>61.115589</td>\n",
       "      <td>315.538910</td>\n",
       "      <td>-0.863614</td>\n",
       "      <td>32.592808</td>\n",
       "      <td>22.366640</td>\n",
       "      <td>16.285843</td>\n",
       "      <td>13.938633</td>\n",
       "      <td>11.180016</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2L</td>\n",
       "      <td>HT250to500</td>\n",
       "      <td>bkg</td>\n",
       "      <td>1</td>\n",
       "      <td>Zjj</td>\n",
       "      <td>Zjj_HT250to500</td>\n",
       "      <td>test</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>11.9635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.479911</td>\n",
       "      <td>98.012802</td>\n",
       "      <td>251.109573</td>\n",
       "      <td>-3.133624</td>\n",
       "      <td>90.252274</td>\n",
       "      <td>33.646885</td>\n",
       "      <td>30.612156</td>\n",
       "      <td>27.973904</td>\n",
       "      <td>23.729696</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2L</td>\n",
       "      <td>HT250to500</td>\n",
       "      <td>bkg</td>\n",
       "      <td>1</td>\n",
       "      <td>Zjj</td>\n",
       "      <td>Zjj_HT250to500</td>\n",
       "      <td>val</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>11.9635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.926899</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>240.909348</td>\n",
       "      <td>0.835656</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2L</td>\n",
       "      <td>HT250to500</td>\n",
       "      <td>bkg</td>\n",
       "      <td>1</td>\n",
       "      <td>Zjj</td>\n",
       "      <td>Zjj_HT250to500</td>\n",
       "      <td>val</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>11.9635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.781194</td>\n",
       "      <td>72.234299</td>\n",
       "      <td>206.020386</td>\n",
       "      <td>-0.320449</td>\n",
       "      <td>48.886372</td>\n",
       "      <td>20.743645</td>\n",
       "      <td>16.572512</td>\n",
       "      <td>13.070706</td>\n",
       "      <td>11.269534</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2L</td>\n",
       "      <td>HT250to500</td>\n",
       "      <td>bkg</td>\n",
       "      <td>1</td>\n",
       "      <td>Zjj</td>\n",
       "      <td>Zjj_HT250to500</td>\n",
       "      <td>val</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>11.9635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Electron_Multi  FatJet1_Eta  FatJet1_Mass  FatJet1_PT  FatJet1_Phi  \\\n",
       "0               2    -1.988600     52.710262  229.350952     0.728242   \n",
       "1               0     0.528382     61.115589  315.538910    -0.863614   \n",
       "2               0     1.479911     98.012802  251.109573    -3.133624   \n",
       "3               2     0.926899     -0.000007  240.909348     0.835656   \n",
       "4               0     0.781194     72.234299  206.020386    -0.320449   \n",
       "\n",
       "   FatJet1_Tau1  FatJet1_Tau2  FatJet1_Tau3  FatJet1_Tau4  FatJet1_Tau5  ...  \\\n",
       "0     36.148926     23.039709     16.949991     14.424411     12.000529  ...   \n",
       "1     32.592808     22.366640     16.285843     13.938633     11.180016  ...   \n",
       "2     90.252274     33.646885     30.612156     27.973904     23.729696  ...   \n",
       "3      0.000000      0.000000      0.000000      0.000000      0.000000  ...   \n",
       "4     48.886372     20.743645     16.572512     13.070706     11.269534  ...   \n",
       "\n",
       "   gen_decay2  gen_decay_filter  gen_filter  gen_label  gen_n_btags  \\\n",
       "0           0                2L  HT250to500        bkg            1   \n",
       "1           0                2L  HT250to500        bkg            1   \n",
       "2           0                2L  HT250to500        bkg            1   \n",
       "3           0                2L  HT250to500        bkg            1   \n",
       "4           0                2L  HT250to500        bkg            1   \n",
       "\n",
       "   gen_sample  gen_sample_filter  gen_split  gen_weights  gen_xsec  \n",
       "0         Zjj     Zjj_HT250to500      train     0.000018   11.9635  \n",
       "1         Zjj     Zjj_HT250to500       test     0.000018   11.9635  \n",
       "2         Zjj     Zjj_HT250to500        val     0.000018   11.9635  \n",
       "3         Zjj     Zjj_HT250to500        val     0.000018   11.9635  \n",
       "4         Zjj     Zjj_HT250to500        val     0.000018   11.9635  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the HDF5 file using pandas\n",
    "data_frame_bkg = pd.read_hdf('bkg_pythia_sanitised_features.h5')\n",
    "\n",
    "# Get the number of rows\n",
    "num_rows = data_frame_bkg.shape[0]\n",
    "\n",
    "print('Number of rows: {}'.format(num_rows))\n",
    "\n",
    "# Explore the data\n",
    "data_frame_bkg.head()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Normalize the data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVMs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_fcnc_pca = data_frame_fcnc.copy()\n",
    "data_frame_bkg_pca = data_frame_bkg.copy()\n",
    "\n",
    "# Drop the categorical features except label, weights and gen_split\n",
    "data_frame_fcnc_pca.drop(['gen_decay_filter', 'gen_filter', 'gen_n_btags', 'gen_sample', 'gen_sample_filter','gen_decay2','gen_decay1'], axis=1, inplace=True)\n",
    "data_frame_bkg_pca.drop(['gen_decay_filter', 'gen_filter', 'gen_n_btags', 'gen_sample', 'gen_sample_filter','gen_decay2','gen_decay1'], axis=1, inplace=True)\n",
    "\n",
    "# Drop the features that are not in both dataframes\n",
    "for feature in data_frame_fcnc_pca.columns.values:\n",
    "    if feature not in data_frame_bkg_pca.columns.values:\n",
    "        data_frame_fcnc_pca.drop([feature], axis=1, inplace=True)\n",
    "\n",
    "for feature in data_frame_bkg_pca.columns.values:\n",
    "    if feature not in data_frame_fcnc_pca.columns.values:\n",
    "        data_frame_bkg_pca.drop([feature], axis=1, inplace=True)\n",
    "        \n",
    "# Join the dataframes\n",
    "data = pd.concat([data_frame_fcnc_pca, data_frame_bkg_pca])\n",
    "\n",
    "# Substitute the labels \"signal\" and \"bkg\" by 1 and 0\n",
    "data = data.replace(['signal'], 1)\n",
    "data= data.replace(['bkg'], 0)\n",
    "\n",
    "#normalize the data except the categorical features and the weights\n",
    "DataFeatures = pd.Index(list(set(data.columns) - set([\"gen_label\", \"gen_xsec\", \"gen_split\"])))\n",
    "data [DataFeatures] = (data [DataFeatures] - data [DataFeatures].mean()) / data [DataFeatures].std()\n",
    "\n",
    "\n",
    "# Split the data into train, validation and test sets for each dataset\n",
    "\n",
    "# train, test and validation sets\n",
    "train = data.loc[data['gen_split'] == 'train']\n",
    "test = data.loc[data['gen_split'] == 'test']\n",
    "val = data.loc[data['gen_split'] == 'val']\n",
    "\n",
    "\n",
    "# get 5000 points of each dataset\n",
    "train_sgn = train.loc[train['gen_label'] == 1].sample(n=5000)\n",
    "train_bkg = train.loc[train['gen_label'] == 0].sample(n=5000)\n",
    "# get 10 batches of 500 points of each dataset and join them\n",
    "x_train_batches = [pd.concat([train_sgn[i*500:(i+1)*500], train_bkg[i*500:(i+1)*500]]) for i in range(10)]\n",
    "# shuffle each batch\n",
    "for i in range(10):\n",
    "    x_train_batches[i] = x_train_batches[i].sample(frac=1)\n",
    "\n",
    "# get 5000 points of each dataset\n",
    "test_sgn = test.loc[test['gen_label'] == 1].sample(n=5000)\n",
    "test_bkg = test.loc[test['gen_label'] == 0].sample(n=5000)\n",
    "# get 10 batches of 500 points of each dataset and join them\n",
    "x_test_batches = [pd.concat([test_sgn[i*500:(i+1)*500], test_bkg[i*500:(i+1)*500]]) for i in range(10)]\n",
    "# shuffle each batch\n",
    "for i in range(10):\n",
    "    x_test_batches[i] = x_test_batches[i].sample(frac=1)\n",
    "\n",
    "# get 5000 points of each dataset\n",
    "val_sgn = val.loc[val['gen_label'] == 1].sample(n=5000)\n",
    "val_bkg = val.loc[val['gen_label'] == 0].sample(n=5000)\n",
    "# get 10 batches of 500 points of each dataset and join them\n",
    "x_val_batches = [pd.concat([val_sgn[i*500:(i+1)*500], val_bkg[i*500:(i+1)*500]]) for i in range(10)]\n",
    "# shuffle each batch\n",
    "for i in range(10):\n",
    "    x_val_batches[i] = x_val_batches[i].sample(frac=1)\n",
    "    \n",
    "    \n",
    "# get an array with the labels for each set\n",
    "y_train_batches = [x_train_batches[i][['gen_label']].values.ravel() for i in range(10)]\n",
    "y_test_batches = [x_test_batches[i][['gen_label']].values.ravel() for i in range(10)]\n",
    "y_val_batches = [x_val_batches[i][['gen_label']].values.ravel() for i in range(10)]\n",
    "\n",
    "# get the weights for each dataset\n",
    "w_train_batches = [x_train_batches[i][['gen_xsec']].values.ravel() for i in range(10)]\n",
    "w_test_batches = [x_test_batches[i][['gen_xsec']].values.ravel() for i in range(10)]\n",
    "w_val_batches = [x_val_batches[i][['gen_xsec']].values.ravel() for i in range(10)]\n",
    "\n",
    "# get an array with the features for each set\n",
    "x_train_batches = [x_train_batches[i][['MissingET_MET', 'Jet1_BTag']].values for i in range(10)]\n",
    "x_test_batches = [x_test_batches[i][['MissingET_MET', 'Jet1_BTag']].values for i in range(10)]\n",
    "x_val_batches = [x_val_batches[i][['MissingET_MET', 'Jet1_BTag']].values for i in range(10)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Renormalize weights (for each batch)\n",
    "for i in range(10):\n",
    "    w_train_batches[i][y_train_batches[i]==1] = w_train_batches[i][y_train_batches[i]==1] / w_train_batches[i][y_train_batches[i]==1].sum() * w_train_batches[i].shape[0] / 2\n",
    "    w_train_batches[i][y_train_batches[i]==0] = w_train_batches[i][y_train_batches[i]==0] / w_train_batches[i][y_train_batches[i]==0].sum() * w_train_batches[i].shape[0] / 2\n",
    "    w_test_batches[i][y_test_batches[i]==1] = w_test_batches[i][y_test_batches[i]==1] / w_test_batches[i][y_test_batches[i]==1].sum() * w_test_batches[i].shape[0] / 2\n",
    "    w_test_batches[i][y_test_batches[i]==0] = w_test_batches[i][y_test_batches[i]==0] / w_test_batches[i][y_test_batches[i]==0].sum() * w_test_batches[i].shape[0] / 2\n",
    "    w_val_batches[i][y_val_batches[i]==1] = w_val_batches[i][y_val_batches[i]==1] / w_val_batches[i][y_val_batches[i]==1].sum() * w_val_batches[i].shape[0] / 2\n",
    "    w_val_batches[i][y_val_batches[i]==0] = w_val_batches[i][y_val_batches[i]==0] / w_val_batches[i][y_val_batches[i]==0].sum() * w_val_batches[i].shape[0] / 2\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train, Val, Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_svm (x_train,y_train, w_train):\n",
    "    clf = svm.SVC(kernel=\"rbf\", probability=True)   \n",
    "    clf.fit(x_train, y_train,sample_weight=w_train)\n",
    "    return clf\n",
    "\n",
    "def val_svm (clf, x_val, y_val, w_val):\n",
    "    y_pred = clf.predict(x_val)\n",
    "    y_pred_prob = clf.predict_proba(x_val)[:,1]\n",
    "    accuracy = accuracy_score(y_val, y_pred, sample_weight=w_val)\n",
    "    auc = roc_auc_score(y_val, y_pred_prob, sample_weight=w_val)\n",
    "    fpr, tpr, thresholds = roc_curve(y_val, y_pred_prob,sample_weight=w_val)\n",
    "    return accuracy, auc, fpr, tpr, thresholds, y_pred_prob\n",
    "\n",
    "def test_svm (clf, x_test, y_test, w_test):\n",
    "    y_pred = clf.predict(x_test)\n",
    "    y_pred_prob = clf.predict_proba(x_test)[:,1]\n",
    "    accuracy = accuracy_score(y_test, y_pred, sample_weight=w_test)\n",
    "    auc = roc_auc_score(y_test, y_pred_prob, sample_weight=w_test)\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob,sample_weight=w_test)\n",
    "    return accuracy, auc, fpr, tpr, thresholds, y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0\n",
      "Accuracy Val: 0.7269086211473512\n",
      "AUC Val: 0.7827763262805836\n",
      "\n",
      "Accuracy Test: 0.6884722654465394\n",
      "AUC Test: 0.7432180251198317\n",
      "\n",
      "Batch: 1\n",
      "Accuracy Val: 0.8167027157166123\n",
      "AUC Val: 0.8897980205762254\n",
      "\n",
      "Accuracy Test: 0.7011570190785421\n",
      "AUC Test: 0.7579255849024165\n",
      "\n",
      "Batch: 2\n",
      "Accuracy Val: 0.7681144570278312\n",
      "AUC Val: 0.8264791264237679\n",
      "\n",
      "Accuracy Test: 0.6726341560327428\n",
      "AUC Test: 0.7674380738318178\n",
      "\n",
      "Batch: 3\n",
      "Accuracy Val: 0.7382315079453188\n",
      "AUC Val: 0.7947835990196821\n",
      "\n",
      "Accuracy Test: 0.7183065552827815\n",
      "AUC Test: 0.7827380683937706\n",
      "\n",
      "Batch: 4\n",
      "Accuracy Val: 0.7347260692134768\n",
      "AUC Val: 0.8000101131352961\n",
      "\n",
      "Accuracy Test: 0.7375932272437296\n",
      "AUC Test: 0.7736530110167066\n",
      "\n",
      "Batch: 5\n",
      "Accuracy Val: 0.7561355854026515\n",
      "AUC Val: 0.8041731106347644\n",
      "\n",
      "Accuracy Test: 0.7008982935363218\n",
      "AUC Test: 0.782437553250195\n",
      "\n",
      "Batch: 6\n",
      "Accuracy Val: 0.7708000433109014\n",
      "AUC Val: 0.7810188124934087\n",
      "\n",
      "Accuracy Test: 0.837107347731421\n",
      "AUC Test: 0.8581953884886654\n",
      "\n",
      "Batch: 7\n",
      "Accuracy Val: 0.7840638569425741\n",
      "AUC Val: 0.8239456842193194\n",
      "\n",
      "Accuracy Test: 0.6663545679842366\n",
      "AUC Test: 0.7050138383820631\n",
      "\n",
      "Batch: 8\n",
      "Accuracy Val: 0.6898645767022943\n",
      "AUC Val: 0.7950868686648838\n",
      "\n",
      "Accuracy Test: 0.576032706513319\n",
      "AUC Test: 0.7466895924861175\n",
      "\n",
      "Batch: 9\n",
      "Accuracy Val: 0.6621128507191159\n",
      "AUC Val: 0.7365167081701353\n",
      "\n",
      "Accuracy Test: 0.7136946049799249\n",
      "AUC Test: 0.7657702749335432\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Get the accuracy and auc for each batch\n",
    "accuracy_val_batches = []\n",
    "auc_val_batches = []\n",
    "\n",
    "accuracy_test_batches = []\n",
    "auc_test_batches = []\n",
    "\n",
    "for i in range (10):\n",
    "    clf = train_svm(x_train_batches[i],y_train_batches[i], w_train_batches[i])\n",
    "    accuracy, auc, fpr, tpr, thresholds, y_pred_prob = val_svm(clf, x_val_batches[i], y_val_batches[i], w_val_batches[i])\n",
    "    print('Batch: {}'.format(i))\n",
    "    print('Accuracy Val: {}'.format(accuracy))\n",
    "    print('AUC Val: {}'.format(auc))\n",
    "    print('')\n",
    "    accuracy_val_batches.append(accuracy)\n",
    "    auc_val_batches.append(auc)\n",
    "    accuracy, auc, fpr, tpr, thresholds, y_pred_prob = test_svm(clf, x_test_batches[i], y_test_batches[i], w_test_batches[i])\n",
    "    print('Accuracy Test: {}'.format(accuracy))\n",
    "    print('AUC Test: {}'.format(auc))\n",
    "    print('')\n",
    "    accuracy_test_batches.append(accuracy)\n",
    "    auc_test_batches.append(auc) \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VQC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pennylane (V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the original data into train, validation and test sets for each dataset\n",
    "data_frame_fcnc_train = data_frame_fcnc.loc[data_frame_fcnc['gen_split'] == 'train']\n",
    "data_frame_bkg_train =  data_frame_bkg.loc[data_frame_bkg['gen_split'] == 'train']\n",
    "\n",
    "data_frame_fcnc_test = data_frame_fcnc.loc[data_frame_fcnc['gen_split'] == 'test']\n",
    "data_frame_bkg_test =  data_frame_bkg.loc[data_frame_bkg['gen_split'] == 'test']\n",
    "\n",
    "data_frame_fcnc_val = data_frame_fcnc.loc[data_frame_fcnc['gen_split'] == 'val']\n",
    "data_frame_bkg_val =  data_frame_bkg.loc[data_frame_bkg['gen_split'] == 'val']\n",
    "\n",
    "\n",
    "# get 500 points of each dataset and  join the datasets (randomly)\n",
    "train_fcnc = data_frame_fcnc_train.sample(n=250)\n",
    "train_bkg = data_frame_bkg_train.sample(n=250)\n",
    "train = pd.concat([train_fcnc, train_bkg])\n",
    "train = train.sample(frac=1, random_state=42)\n",
    "\n",
    "test_fcnc = data_frame_fcnc_test.sample(n=250)\n",
    "test_bkg = data_frame_bkg_test.sample(n=250)\n",
    "test= pd.concat([test_fcnc, test_bkg])\n",
    "test = train.sample(frac=1, random_state=42)\n",
    "\n",
    "val_fnc = data_frame_fcnc_val.sample(n=250)\n",
    "val_bkg = data_frame_bkg_val.sample(n=250)\n",
    "val = pd.concat([val_fnc, val_bkg])\n",
    "val = val.sample(frac=1, random_state=42)\n",
    "\n",
    "# get the weights for each dataset\n",
    "w_train = train[['gen_xsec']]\n",
    "w_test = test[['gen_xsec']]\n",
    "w_val = val[['gen_xsec']]\n",
    "\n",
    "\n",
    "# change the signal and bkg labels to 0 and 1 and get the labels for each dataset\n",
    "train = train.replace(['signal'], 1)\n",
    "train= train.replace(['bkg'], 0)\n",
    "y_train = train[['gen_label']]\n",
    "x_train = train[['MissingET_MET', 'Jet1_BTag']]\n",
    "\n",
    "test = test.replace(['signal'], 1)\n",
    "test= test.replace(['bkg'], 0)\n",
    "y_test = test[['gen_label']]\n",
    "x_test = test[['MissingET_MET', 'Jet1_BTag']]\n",
    "\n",
    "val = val.replace(['signal'], 1)\n",
    "val= val.replace(['bkg'], 0)\n",
    "y_val = val[['gen_label']]\n",
    "x_val = val[['MissingET_MET', 'Jet1_BTag']]\n",
    "\n",
    "y_train_arr = np.concatenate(y_train.values, axis=0 )\n",
    "y_val_arr = np.concatenate( y_val.values, axis=0 )\n",
    "y_test_arr = np.concatenate( y_test.values, axis=0 )\n",
    "\n",
    "# Renormalize data weights\n",
    "w_train[y_train_arr == 1] = (w_train[y_train_arr == 1] / w_train[y_train_arr == 1].sum()) * y_train_arr.shape[0] / 2\n",
    "w_train[y_train_arr == 0] = (w_train[y_train_arr == 0] / w_train[y_train_arr == 0].sum()) * y_train_arr.shape[0] / 2\n",
    "        \n",
    "w_test[y_test_arr == 1] = (w_test[y_test_arr == 1] / w_test[y_test_arr == 1].sum()) * w_test.shape[0] / 2\n",
    "w_test[y_test_arr == 0] = (w_test[y_test_arr == 0] / w_test[y_test_arr == 0].sum()) * w_test.shape[0] / 2\n",
    "        \n",
    "w_val[y_test_arr == 1] = (w_val[y_test_arr == 1] / w_val[y_test_arr == 1].sum()) * w_val.shape[0] / 2\n",
    "w_val[y_test_arr == 0] = (w_val[y_test_arr == 0] / w_val[y_test_arr == 0].sum()) * w_val.shape[0] / 2\n",
    "\n",
    "# Concatenate features\n",
    "X = np.concatenate([x_train, x_test,x_val])\n",
    "\n",
    "# Normalize the data for angle embeding  (Put the data between -pi and pi)\n",
    "X = (((X - X.min()) / (X.max() - X.min())) * 2 - 1) * (np.pi)\n",
    "\n",
    "# Split the features array into train, validation and test sets\n",
    "x_train = X[:500]\n",
    "x_test = X[500:1000]\n",
    "x_val = X[1000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy function\n",
    "def accuracy(labels, predictions):\n",
    "\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        if abs(l - p) < 1e-5:\n",
    "            loss = loss + 1\n",
    "    loss = loss / len(labels)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def square_loss(labels, predictions):\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        loss = loss + (l - p) ** 2\n",
    "\n",
    "    loss = loss / len(labels)\n",
    "    return loss\n",
    "\n",
    "# quantum circuit function\n",
    "def circuit(n_features, n_layers, weights, x):\n",
    "        # Embedding\n",
    "        \n",
    "        qml.AngleEmbedding(x,range (0, n_features),rotation=\"X\" )\n",
    "\n",
    "        # For every layer\n",
    "        for layer in range(n_layers):\n",
    "            W1 = weights[layer]\n",
    "\n",
    "            # Define Rotations\n",
    "            for i in range(0,n_features):\n",
    "                qml.Rot(W1[i, 0], W1[i, 1], W1[i, 2], wires=i)\n",
    "\n",
    "            # Entanglement\n",
    "            if n_features != 1:\n",
    "                if n_features > 2:\n",
    "                    for i in range(n_features):\n",
    "                        if i == n_features - 1:\n",
    "                            qml.CNOT(wires=[i, 0])\n",
    "                        else:\n",
    "                            qml.CNOT(wires=[i, i + 1])\n",
    "                else:\n",
    "                    qml.CNOT(wires=[1, 0])\n",
    "\n",
    "        return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "# classifier function    \n",
    "def classifier(n_features, n_layers, weights, x):\n",
    "        #c = circuit(n_features, n_layers, weights, x)\n",
    "        dev=qml.device(\"default.qubit\", wires=n_features)\n",
    "        return qml.QNode(circuit, dev)(n_features, n_layers, weights, x)\n",
    "    \n",
    "# cost function    \n",
    "def cost(n_features, n_layers,weights,X,Y,W):  \n",
    "        # Compute predictions\n",
    "        y_scores = [(classifier(n_features, n_layers,weights, x) + 1) / 2 for x in X]\n",
    "\n",
    "        loss = square_loss(Y, y_scores)\n",
    "        loss = loss * W\n",
    "        loss = loss.sum()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "# train step function    \n",
    "def train_step(n_features, n_layers,x_train,y_train, w_train, weights, opt,desc='Training'):\n",
    "        \n",
    "        # Only require grad if necessary\n",
    "        x_train = np.array(x_train, requires_grad=False)\n",
    "        y_train = np.array(y_train, requires_grad=True)\n",
    "        w_train = np.array(w_train, requires_grad=False)\n",
    "\n",
    "        # Compute cost and update weights\n",
    "        weights, loss = opt.step_and_cost(cost, n_features, n_layers,weights, X=x_train, Y=y_train, W=w_train)\n",
    "\n",
    "        return loss, weights\n",
    "    \n",
    "# validation step function\n",
    "def validation_step(n_features, n_layers, x_val, y_val, w_val, weights, best_score, epoch_number, best_score_epoch,best_weights,desc='Validation'):\n",
    "    X_val = np.array(x_val, requires_grad=False)\n",
    "    Y_val = np.array(y_val, requires_grad=False)\n",
    "    W_val = np.array(w_val, requires_grad=False)\n",
    "\n",
    "    y_scores = np.array([classifier(n_features, n_layers, weights, x) for x in X_val])\n",
    "    y_scores = (y_scores + 1) / 2\n",
    "\n",
    "    W_val[Y_val == 1] = (W_val[Y_val == 1] / W_val[Y_val == 1].sum()) * W_val.shape[0] / 2\n",
    "    W_val[Y_val == 0] = (W_val[Y_val == 0] / W_val[Y_val == 0].sum()) * W_val.shape[0] / 2\n",
    "\n",
    "    auc_score = roc_auc_score(y_true=Y_val, y_score=y_scores, sample_weight=W_val)\n",
    "    loss = cost(n_features, n_layers, weights, X_val, Y_val, W_val)\n",
    "\n",
    "\n",
    "    if best_score is None or auc_score > best_score:\n",
    "        best_score = auc_score\n",
    "        best_score_epoch = epoch_number\n",
    "        best_weights = weights\n",
    "\n",
    "    tqdm.write(f\"Epoch: {epoch_number}, Validation Loss: {loss:.4f}, AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "    return best_score, best_score_epoch, best_weights\n",
    "        \n",
    "        \n",
    "# train function\n",
    "def train(n_features, n_layers, x_train, y_train, learning_rate, weights, max_epochs, epoch_number):\n",
    "    opt = AdamOptimizer(learning_rate)\n",
    "    best_score = None\n",
    "    best_weights = None\n",
    "    best_score_epoch = None\n",
    "\n",
    "    with tqdm(total=max_epochs, desc='Epoch', unit='epoch') as pbar:\n",
    "        for epoch in range(epoch_number, max_epochs):\n",
    "            epoch_number = epoch\n",
    "\n",
    "            loss, nf_nl_weights = train_step(n_features, n_layers, x_train, y_train, w_train, weights, opt, desc='Training')\n",
    "            \n",
    "            # Log variable values using tqdm.write\n",
    "            tqdm.write(f\"Epoch: {epoch_number:}, Loss: {loss:.4f}\")\n",
    "            \n",
    "            \n",
    "            weights = nf_nl_weights[2:]\n",
    "            weights = weights[0]\n",
    "\n",
    "            if epoch_number == max_epochs - 1 or (epoch_number+1)%5==0:\n",
    "                best_score, best_score_epoch, best_weights = validation_step(n_features, n_layers, x_val, y_val, w_val, weights, best_score, epoch_number, best_score_epoch, best_weights,desc='Validation')\n",
    "                # early stopping\n",
    "                if epoch_number - best_score_epoch > 30 and epoch_number > 80:\n",
    "                    tqdm.write(f\"Early stopping at epoch {epoch_number}\")\n",
    "                    break\n",
    "\n",
    "            pbar.update(1)  # Update progress bar\n",
    "        tqdm.write(f\"Best Score: {best_score:.4f}\")            \n",
    "        \n",
    "    return best_score, best_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dcf460802ea49b1ae8cedff0a3b98f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1000 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 261.3483\n",
      "Epoch: 1, Loss: 261.0370\n",
      "Epoch: 2, Loss: 260.4762\n",
      "Epoch: 3, Loss: 259.6366\n",
      "Epoch: 4, Loss: 258.5122\n",
      "Epoch: 4, Validation Loss: 247.5088, AUC Score: 0.2079\n",
      "Epoch: 5, Loss: 257.1012\n",
      "Epoch: 6, Loss: 255.4059\n",
      "Epoch: 7, Loss: 253.4327\n",
      "Epoch: 8, Loss: 251.1930\n",
      "Epoch: 9, Loss: 248.7032\n",
      "Epoch: 9, Validation Loss: 236.9744, AUC Score: 0.2077\n",
      "Epoch: 10, Loss: 245.9842\n",
      "Epoch: 11, Loss: 243.0608\n",
      "Epoch: 12, Loss: 239.9576\n",
      "Epoch: 13, Loss: 236.6955\n",
      "Epoch: 14, Loss: 233.2885\n",
      "Epoch: 14, Validation Loss: 221.6034, AUC Score: 0.2078\n",
      "Epoch: 15, Loss: 229.7434\n",
      "Epoch: 16, Loss: 226.0629\n",
      "Epoch: 17, Loss: 222.2473\n",
      "Epoch: 18, Loss: 218.2970\n",
      "Epoch: 19, Loss: 214.2153\n",
      "Epoch: 19, Validation Loss: 202.9017, AUC Score: 0.2080\n",
      "Epoch: 20, Loss: 210.0096\n",
      "Epoch: 21, Loss: 205.6906\n",
      "Epoch: 22, Loss: 201.2709\n",
      "Epoch: 23, Loss: 196.7654\n",
      "Epoch: 24, Loss: 192.1916\n",
      "Epoch: 24, Validation Loss: 181.6469, AUC Score: 0.2080\n",
      "Epoch: 25, Loss: 187.5696\n",
      "Epoch: 26, Loss: 182.9225\n",
      "Epoch: 27, Loss: 178.2751\n",
      "Epoch: 28, Loss: 173.6542\n",
      "Epoch: 29, Loss: 169.0880\n",
      "Epoch: 29, Validation Loss: 160.0451, AUC Score: 0.2077\n",
      "Epoch: 30, Loss: 164.6052\n",
      "Epoch: 31, Loss: 160.2339\n",
      "Epoch: 32, Loss: 156.0010\n",
      "Epoch: 33, Loss: 151.9305\n",
      "Epoch: 34, Loss: 148.0436\n",
      "Epoch: 34, Validation Loss: 141.2581, AUC Score: 0.1917\n",
      "Epoch: 35, Loss: 144.3577\n",
      "Epoch: 36, Loss: 140.8856\n",
      "Epoch: 37, Loss: 137.6348\n",
      "Epoch: 38, Loss: 134.6069\n",
      "Epoch: 39, Loss: 131.7976\n",
      "Epoch: 39, Validation Loss: 127.5081, AUC Score: 0.5728\n",
      "Epoch: 40, Loss: 129.1978\n",
      "Epoch: 41, Loss: 126.7953\n",
      "Epoch: 42, Loss: 124.5765\n",
      "Epoch: 43, Loss: 122.5278\n",
      "Epoch: 44, Loss: 120.6371\n",
      "Epoch: 44, Validation Loss: 118.5633, AUC Score: 0.7315\n",
      "Epoch: 45, Loss: 118.8935\n",
      "Epoch: 46, Loss: 117.2877\n",
      "Epoch: 47, Loss: 115.8116\n",
      "Epoch: 48, Loss: 114.4577\n",
      "Epoch: 49, Loss: 113.2188\n",
      "Epoch: 49, Validation Loss: 113.0955, AUC Score: 0.7561\n",
      "Epoch: 50, Loss: 112.0874\n",
      "Epoch: 51, Loss: 111.0556\n",
      "Epoch: 52, Loss: 110.1152\n",
      "Epoch: 53, Loss: 109.2576\n",
      "Epoch: 54, Loss: 108.4747\n",
      "Epoch: 54, Validation Loss: 109.9349, AUC Score: 0.7806\n",
      "Epoch: 55, Loss: 107.7583\n",
      "Epoch: 56, Loss: 107.1014\n",
      "Epoch: 57, Loss: 106.4972\n",
      "Epoch: 58, Loss: 105.9403\n",
      "Epoch: 59, Loss: 105.4257\n",
      "Epoch: 59, Validation Loss: 108.0178, AUC Score: 0.7825\n",
      "Epoch: 60, Loss: 104.9495\n",
      "Epoch: 61, Loss: 104.5080\n",
      "Epoch: 62, Loss: 104.0985\n",
      "Epoch: 63, Loss: 103.7183\n",
      "Epoch: 64, Loss: 103.3649\n",
      "Epoch: 64, Validation Loss: 106.7550, AUC Score: 0.7834\n",
      "Epoch: 65, Loss: 103.0361\n",
      "Epoch: 66, Loss: 102.7298\n",
      "Epoch: 67, Loss: 102.4440\n",
      "Epoch: 68, Loss: 102.1765\n",
      "Epoch: 69, Loss: 101.9256\n",
      "Epoch: 69, Validation Loss: 105.9211, AUC Score: 0.7846\n",
      "Epoch: 70, Loss: 101.6896\n",
      "Epoch: 71, Loss: 101.4669\n",
      "Epoch: 72, Loss: 101.2561\n",
      "Epoch: 73, Loss: 101.0562\n",
      "Epoch: 74, Loss: 100.8659\n",
      "Epoch: 74, Validation Loss: 105.3633, AUC Score: 0.7865\n",
      "Epoch: 75, Loss: 100.6845\n",
      "Epoch: 76, Loss: 100.5111\n",
      "Epoch: 77, Loss: 100.3448\n",
      "Epoch: 78, Loss: 100.1848\n",
      "Epoch: 79, Loss: 100.0306\n",
      "Epoch: 79, Validation Loss: 104.9463, AUC Score: 0.7872\n",
      "Epoch: 80, Loss: 99.8814\n",
      "Epoch: 81, Loss: 99.7367\n",
      "Epoch: 82, Loss: 99.5959\n",
      "Epoch: 83, Loss: 99.4588\n",
      "Epoch: 84, Loss: 99.3249\n",
      "Epoch: 84, Validation Loss: 104.5756, AUC Score: 0.7877\n",
      "Epoch: 85, Loss: 99.1941\n",
      "Epoch: 86, Loss: 99.0662\n",
      "Epoch: 87, Loss: 98.9409\n",
      "Epoch: 88, Loss: 98.8182\n",
      "Epoch: 89, Loss: 98.6978\n",
      "Epoch: 89, Validation Loss: 104.2376, AUC Score: 0.7881\n",
      "Epoch: 90, Loss: 98.5797\n",
      "Epoch: 91, Loss: 98.4637\n",
      "Epoch: 92, Loss: 98.3496\n",
      "Epoch: 93, Loss: 98.2375\n",
      "Epoch: 94, Loss: 98.1272\n",
      "Epoch: 94, Validation Loss: 103.9529, AUC Score: 0.7883\n",
      "Epoch: 95, Loss: 98.0187\n",
      "Epoch: 96, Loss: 97.9121\n",
      "Epoch: 97, Loss: 97.8072\n",
      "Epoch: 98, Loss: 97.7040\n",
      "Epoch: 99, Loss: 97.6027\n",
      "Epoch: 99, Validation Loss: 103.7142, AUC Score: 0.7887\n",
      "Epoch: 100, Loss: 97.5030\n",
      "Epoch: 101, Loss: 97.4049\n",
      "Epoch: 102, Loss: 97.3085\n",
      "Epoch: 103, Loss: 97.2137\n",
      "Epoch: 104, Loss: 97.1206\n",
      "Epoch: 104, Validation Loss: 103.5009, AUC Score: 0.7886\n",
      "Epoch: 105, Loss: 97.0290\n",
      "Epoch: 106, Loss: 96.9391\n",
      "Epoch: 107, Loss: 96.8508\n",
      "Epoch: 108, Loss: 96.7641\n",
      "Epoch: 109, Loss: 96.6790\n",
      "Epoch: 109, Validation Loss: 103.3180, AUC Score: 0.7886\n",
      "Epoch: 110, Loss: 96.5955\n",
      "Epoch: 111, Loss: 96.5137\n",
      "Epoch: 112, Loss: 96.4335\n",
      "Epoch: 113, Loss: 96.3549\n",
      "Epoch: 114, Loss: 96.2779\n",
      "Epoch: 114, Validation Loss: 103.1794, AUC Score: 0.7886\n",
      "Epoch: 115, Loss: 96.2025\n",
      "Epoch: 116, Loss: 96.1288\n",
      "Epoch: 117, Loss: 96.0568\n",
      "Epoch: 118, Loss: 95.9863\n",
      "Epoch: 119, Loss: 95.9176\n",
      "Epoch: 119, Validation Loss: 103.0803, AUC Score: 0.7888\n",
      "Epoch: 120, Loss: 95.8505\n",
      "Epoch: 121, Loss: 95.7851\n",
      "Epoch: 122, Loss: 95.7214\n",
      "Epoch: 123, Loss: 95.6594\n",
      "Epoch: 124, Loss: 95.5991\n",
      "Epoch: 124, Validation Loss: 103.0137, AUC Score: 0.7891\n",
      "Epoch: 125, Loss: 95.5405\n",
      "Epoch: 126, Loss: 95.4837\n",
      "Epoch: 127, Loss: 95.4287\n",
      "Epoch: 128, Loss: 95.3754\n",
      "Epoch: 129, Loss: 95.3240\n",
      "Epoch: 129, Validation Loss: 102.9834, AUC Score: 0.7891\n",
      "Epoch: 130, Loss: 95.2744\n",
      "Epoch: 131, Loss: 95.2266\n",
      "Epoch: 132, Loss: 95.1806\n",
      "Epoch: 133, Loss: 95.1365\n",
      "Epoch: 134, Loss: 95.0942\n",
      "Epoch: 134, Validation Loss: 102.9898, AUC Score: 0.7891\n",
      "Epoch: 135, Loss: 95.0538\n",
      "Epoch: 136, Loss: 95.0152\n",
      "Epoch: 137, Loss: 94.9783\n",
      "Epoch: 138, Loss: 94.9433\n",
      "Epoch: 139, Loss: 94.9101\n",
      "Epoch: 139, Validation Loss: 103.0240, AUC Score: 0.7890\n",
      "Epoch: 140, Loss: 94.8786\n",
      "Epoch: 141, Loss: 94.8488\n",
      "Epoch: 142, Loss: 94.8206\n",
      "Epoch: 143, Loss: 94.7941\n",
      "Epoch: 144, Loss: 94.7692\n",
      "Epoch: 144, Validation Loss: 103.0765, AUC Score: 0.7890\n",
      "Epoch: 145, Loss: 94.7459\n",
      "Epoch: 146, Loss: 94.7240\n",
      "Epoch: 147, Loss: 94.7035\n",
      "Epoch: 148, Loss: 94.6845\n",
      "Epoch: 149, Loss: 94.6667\n",
      "Epoch: 149, Validation Loss: 103.1397, AUC Score: 0.7890\n",
      "Epoch: 150, Loss: 94.6501\n",
      "Epoch: 151, Loss: 94.6348\n",
      "Epoch: 152, Loss: 94.6205\n",
      "Epoch: 153, Loss: 94.6073\n",
      "Epoch: 154, Loss: 94.5950\n",
      "Epoch: 154, Validation Loss: 103.2049, AUC Score: 0.7889\n",
      "Epoch: 155, Loss: 94.5837\n",
      "Epoch: 156, Loss: 94.5732\n",
      "Epoch: 157, Loss: 94.5635\n",
      "Epoch: 158, Loss: 94.5545\n",
      "Epoch: 159, Loss: 94.5461\n",
      "Epoch: 159, Validation Loss: 103.2642, AUC Score: 0.7889\n",
      "Early stopping at epoch 159\n",
      "Best Score: 0.7891\n"
     ]
    }
   ],
   "source": [
    "# number of features and layers\n",
    "n_features = 2\n",
    "n_layers = 3\n",
    "\n",
    "# Random weight initialization\n",
    "weights = 0.01 * np.random.randn(n_layers, n_features, 3, requires_grad=True)\n",
    "\n",
    "# We create a quantum device with n_features \"wires\" (or qubits)\n",
    "dev = qml.device('default.qubit', wires=n_features)\n",
    "\n",
    "# train the model\n",
    "best_score, best_weights = train (n_features, n_layers,x_train,y_train_arr,0.01, weights, 1000, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7890931875857816\n"
     ]
    }
   ],
   "source": [
    "print (best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7790329921315001\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "\n",
    "def test(n_features, n_layers,x_test,y_test,w_test, weights):\n",
    "        # Remove grad\n",
    "        X_test = np.array(x_test, requires_grad=False)\n",
    "        Y_test = np.array(y_test, requires_grad=False)\n",
    "        W_test = np.array(w_test, requires_grad=False)\n",
    "\n",
    "        # This will be between -1 and 1, we need to convert to between 0 and 1\n",
    "        y_scores = np.array([classifier(n_features, n_layers,weights, x) for x in X_test])\n",
    "        y_scores = (y_scores + 1) / 2\n",
    "\n",
    "        # Renormalize weights\n",
    "        W_test[Y_test == 1] = (W_test[Y_test == 1] / W_test[Y_test == 1].sum()) * W_test.shape[0] / 2\n",
    "        W_test[Y_test == 0] = (W_test[Y_test == 0] / W_test[Y_test == 0].sum()) * W_test.shape[0] / 2\n",
    "\n",
    "        # Calculate ROC\n",
    "        auc_score = roc_auc_score(y_true=Y_test, y_score=y_scores, sample_weight=W_test)\n",
    "        \n",
    "        return auc_score\n",
    "    \n",
    "auc_score = test(n_features, n_layers,x_test,y_test,w_test, best_weights)\n",
    "print (auc_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
