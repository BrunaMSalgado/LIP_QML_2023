{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.optimize import NesterovMomentumOptimizer\n",
    "from pennylane.optimize import AdamOptimizer\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.templates.embeddings import AngleEmbedding\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from qiskit_ibm_provider import IBMProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 90548\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Electron_Multi</th>\n",
       "      <th>FatJet1_Eta</th>\n",
       "      <th>FatJet1_Mass</th>\n",
       "      <th>FatJet1_PT</th>\n",
       "      <th>FatJet1_Phi</th>\n",
       "      <th>FatJet1_Tau1</th>\n",
       "      <th>FatJet1_Tau2</th>\n",
       "      <th>FatJet1_Tau3</th>\n",
       "      <th>FatJet1_Tau4</th>\n",
       "      <th>FatJet1_Tau5</th>\n",
       "      <th>...</th>\n",
       "      <th>gen_decay2</th>\n",
       "      <th>gen_decay_filter</th>\n",
       "      <th>gen_filter</th>\n",
       "      <th>gen_label</th>\n",
       "      <th>gen_n_btags</th>\n",
       "      <th>gen_sample</th>\n",
       "      <th>gen_sample_filter</th>\n",
       "      <th>gen_split</th>\n",
       "      <th>gen_weights</th>\n",
       "      <th>gen_xsec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1.408853</td>\n",
       "      <td>15.150869</td>\n",
       "      <td>339.182312</td>\n",
       "      <td>2.350262</td>\n",
       "      <td>1.396943</td>\n",
       "      <td>0.710451</td>\n",
       "      <td>0.109013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>PyDelphes</td>\n",
       "      <td>signal</td>\n",
       "      <td>1</td>\n",
       "      <td>tZFCNC</td>\n",
       "      <td>tZFCNC_PyDelphes</td>\n",
       "      <td>test</td>\n",
       "      <td>7.762202e-09</td>\n",
       "      <td>0.001285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-2.481838</td>\n",
       "      <td>7.208333</td>\n",
       "      <td>247.036240</td>\n",
       "      <td>-2.280740</td>\n",
       "      <td>0.428710</td>\n",
       "      <td>0.205213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>PyDelphes</td>\n",
       "      <td>signal</td>\n",
       "      <td>1</td>\n",
       "      <td>tZFCNC</td>\n",
       "      <td>tZFCNC_PyDelphes</td>\n",
       "      <td>val</td>\n",
       "      <td>7.762202e-09</td>\n",
       "      <td>0.001285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.476267</td>\n",
       "      <td>94.220718</td>\n",
       "      <td>238.014694</td>\n",
       "      <td>-1.788097</td>\n",
       "      <td>94.256210</td>\n",
       "      <td>2.418446</td>\n",
       "      <td>1.585315</td>\n",
       "      <td>1.127324</td>\n",
       "      <td>0.431098</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>PyDelphes</td>\n",
       "      <td>signal</td>\n",
       "      <td>1</td>\n",
       "      <td>tZFCNC</td>\n",
       "      <td>tZFCNC_PyDelphes</td>\n",
       "      <td>train</td>\n",
       "      <td>7.762249e-09</td>\n",
       "      <td>0.001285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.647480</td>\n",
       "      <td>13.459283</td>\n",
       "      <td>230.971832</td>\n",
       "      <td>-1.032663</td>\n",
       "      <td>1.227122</td>\n",
       "      <td>0.467150</td>\n",
       "      <td>0.164008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>PyDelphes</td>\n",
       "      <td>signal</td>\n",
       "      <td>1</td>\n",
       "      <td>tZFCNC</td>\n",
       "      <td>tZFCNC_PyDelphes</td>\n",
       "      <td>train</td>\n",
       "      <td>7.762249e-09</td>\n",
       "      <td>0.001285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2.106436</td>\n",
       "      <td>97.490242</td>\n",
       "      <td>698.399902</td>\n",
       "      <td>-3.059983</td>\n",
       "      <td>36.555862</td>\n",
       "      <td>2.937936</td>\n",
       "      <td>1.799140</td>\n",
       "      <td>1.093004</td>\n",
       "      <td>0.589724</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>PyDelphes</td>\n",
       "      <td>signal</td>\n",
       "      <td>1</td>\n",
       "      <td>tZFCNC</td>\n",
       "      <td>tZFCNC_PyDelphes</td>\n",
       "      <td>train</td>\n",
       "      <td>7.762249e-09</td>\n",
       "      <td>0.001285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Electron_Multi  FatJet1_Eta  FatJet1_Mass  FatJet1_PT  FatJet1_Phi  \\\n",
       "0               2     1.408853     15.150869  339.182312     2.350262   \n",
       "1               1    -2.481838      7.208333  247.036240    -2.280740   \n",
       "2               0     1.476267     94.220718  238.014694    -1.788097   \n",
       "3               1     0.647480     13.459283  230.971832    -1.032663   \n",
       "4               0     2.106436     97.490242  698.399902    -3.059983   \n",
       "\n",
       "   FatJet1_Tau1  FatJet1_Tau2  FatJet1_Tau3  FatJet1_Tau4  FatJet1_Tau5  ...  \\\n",
       "0      1.396943      0.710451      0.109013      0.000000      0.000000  ...   \n",
       "1      0.428710      0.205213      0.000000      0.000000      0.000000  ...   \n",
       "2     94.256210      2.418446      1.585315      1.127324      0.431098  ...   \n",
       "3      1.227122      0.467150      0.164008      0.000000      0.000000  ...   \n",
       "4     36.555862      2.937936      1.799140      1.093004      0.589724  ...   \n",
       "\n",
       "   gen_decay2  gen_decay_filter  gen_filter  gen_label  gen_n_btags  \\\n",
       "0           0              None   PyDelphes     signal            1   \n",
       "1           0              None   PyDelphes     signal            1   \n",
       "2           0              None   PyDelphes     signal            1   \n",
       "3           0              None   PyDelphes     signal            1   \n",
       "4           0              None   PyDelphes     signal            1   \n",
       "\n",
       "   gen_sample  gen_sample_filter  gen_split   gen_weights  gen_xsec  \n",
       "0      tZFCNC   tZFCNC_PyDelphes       test  7.762202e-09  0.001285  \n",
       "1      tZFCNC   tZFCNC_PyDelphes        val  7.762202e-09  0.001285  \n",
       "2      tZFCNC   tZFCNC_PyDelphes      train  7.762249e-09  0.001285  \n",
       "3      tZFCNC   tZFCNC_PyDelphes      train  7.762249e-09  0.001285  \n",
       "4      tZFCNC   tZFCNC_PyDelphes      train  7.762249e-09  0.001285  \n",
       "\n",
       "[5 rows x 83 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the HDF5 file using pandas\n",
    "data_frame_fcnc = pd.read_hdf('fcnc_pythia_sanitised_features.h5')\n",
    "\n",
    "# Get the number of rows\n",
    "num_rows = data_frame_fcnc.shape[0]\n",
    "\n",
    "print('Number of rows: {}'.format(num_rows))\n",
    "\n",
    "# Explore the data\n",
    "data_frame_fcnc.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1002490\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Electron_Multi</th>\n",
       "      <th>FatJet1_Eta</th>\n",
       "      <th>FatJet1_Mass</th>\n",
       "      <th>FatJet1_PT</th>\n",
       "      <th>FatJet1_Phi</th>\n",
       "      <th>FatJet1_Tau1</th>\n",
       "      <th>FatJet1_Tau2</th>\n",
       "      <th>FatJet1_Tau3</th>\n",
       "      <th>FatJet1_Tau4</th>\n",
       "      <th>FatJet1_Tau5</th>\n",
       "      <th>...</th>\n",
       "      <th>gen_decay2</th>\n",
       "      <th>gen_decay_filter</th>\n",
       "      <th>gen_filter</th>\n",
       "      <th>gen_label</th>\n",
       "      <th>gen_n_btags</th>\n",
       "      <th>gen_sample</th>\n",
       "      <th>gen_sample_filter</th>\n",
       "      <th>gen_split</th>\n",
       "      <th>gen_weights</th>\n",
       "      <th>gen_xsec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.988600</td>\n",
       "      <td>52.710262</td>\n",
       "      <td>229.350952</td>\n",
       "      <td>0.728242</td>\n",
       "      <td>36.148926</td>\n",
       "      <td>23.039709</td>\n",
       "      <td>16.949991</td>\n",
       "      <td>14.424411</td>\n",
       "      <td>12.000529</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2L</td>\n",
       "      <td>HT250to500</td>\n",
       "      <td>bkg</td>\n",
       "      <td>1</td>\n",
       "      <td>Zjj</td>\n",
       "      <td>Zjj_HT250to500</td>\n",
       "      <td>train</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>11.9635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.528382</td>\n",
       "      <td>61.115589</td>\n",
       "      <td>315.538910</td>\n",
       "      <td>-0.863614</td>\n",
       "      <td>32.592808</td>\n",
       "      <td>22.366640</td>\n",
       "      <td>16.285843</td>\n",
       "      <td>13.938633</td>\n",
       "      <td>11.180016</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2L</td>\n",
       "      <td>HT250to500</td>\n",
       "      <td>bkg</td>\n",
       "      <td>1</td>\n",
       "      <td>Zjj</td>\n",
       "      <td>Zjj_HT250to500</td>\n",
       "      <td>test</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>11.9635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.479911</td>\n",
       "      <td>98.012802</td>\n",
       "      <td>251.109573</td>\n",
       "      <td>-3.133624</td>\n",
       "      <td>90.252274</td>\n",
       "      <td>33.646885</td>\n",
       "      <td>30.612156</td>\n",
       "      <td>27.973904</td>\n",
       "      <td>23.729696</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2L</td>\n",
       "      <td>HT250to500</td>\n",
       "      <td>bkg</td>\n",
       "      <td>1</td>\n",
       "      <td>Zjj</td>\n",
       "      <td>Zjj_HT250to500</td>\n",
       "      <td>val</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>11.9635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.926899</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>240.909348</td>\n",
       "      <td>0.835656</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2L</td>\n",
       "      <td>HT250to500</td>\n",
       "      <td>bkg</td>\n",
       "      <td>1</td>\n",
       "      <td>Zjj</td>\n",
       "      <td>Zjj_HT250to500</td>\n",
       "      <td>val</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>11.9635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.781194</td>\n",
       "      <td>72.234299</td>\n",
       "      <td>206.020386</td>\n",
       "      <td>-0.320449</td>\n",
       "      <td>48.886372</td>\n",
       "      <td>20.743645</td>\n",
       "      <td>16.572512</td>\n",
       "      <td>13.070706</td>\n",
       "      <td>11.269534</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2L</td>\n",
       "      <td>HT250to500</td>\n",
       "      <td>bkg</td>\n",
       "      <td>1</td>\n",
       "      <td>Zjj</td>\n",
       "      <td>Zjj_HT250to500</td>\n",
       "      <td>val</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>11.9635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Electron_Multi  FatJet1_Eta  FatJet1_Mass  FatJet1_PT  FatJet1_Phi  \\\n",
       "0               2    -1.988600     52.710262  229.350952     0.728242   \n",
       "1               0     0.528382     61.115589  315.538910    -0.863614   \n",
       "2               0     1.479911     98.012802  251.109573    -3.133624   \n",
       "3               2     0.926899     -0.000007  240.909348     0.835656   \n",
       "4               0     0.781194     72.234299  206.020386    -0.320449   \n",
       "\n",
       "   FatJet1_Tau1  FatJet1_Tau2  FatJet1_Tau3  FatJet1_Tau4  FatJet1_Tau5  ...  \\\n",
       "0     36.148926     23.039709     16.949991     14.424411     12.000529  ...   \n",
       "1     32.592808     22.366640     16.285843     13.938633     11.180016  ...   \n",
       "2     90.252274     33.646885     30.612156     27.973904     23.729696  ...   \n",
       "3      0.000000      0.000000      0.000000      0.000000      0.000000  ...   \n",
       "4     48.886372     20.743645     16.572512     13.070706     11.269534  ...   \n",
       "\n",
       "   gen_decay2  gen_decay_filter  gen_filter  gen_label  gen_n_btags  \\\n",
       "0           0                2L  HT250to500        bkg            1   \n",
       "1           0                2L  HT250to500        bkg            1   \n",
       "2           0                2L  HT250to500        bkg            1   \n",
       "3           0                2L  HT250to500        bkg            1   \n",
       "4           0                2L  HT250to500        bkg            1   \n",
       "\n",
       "   gen_sample  gen_sample_filter  gen_split  gen_weights  gen_xsec  \n",
       "0         Zjj     Zjj_HT250to500      train     0.000018   11.9635  \n",
       "1         Zjj     Zjj_HT250to500       test     0.000018   11.9635  \n",
       "2         Zjj     Zjj_HT250to500        val     0.000018   11.9635  \n",
       "3         Zjj     Zjj_HT250to500        val     0.000018   11.9635  \n",
       "4         Zjj     Zjj_HT250to500        val     0.000018   11.9635  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the HDF5 file using pandas\n",
    "data_frame_bkg = pd.read_hdf('bkg_pythia_sanitised_features.h5')\n",
    "\n",
    "# Get the number of rows\n",
    "num_rows = data_frame_bkg.shape[0]\n",
    "\n",
    "print('Number of rows: {}'.format(num_rows))\n",
    "\n",
    "# Explore the data\n",
    "data_frame_bkg.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VQC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pennylane (V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the original data into train, validation and test sets for each dataset\n",
    "data_frame_fcnc_train = data_frame_fcnc.loc[data_frame_fcnc['gen_split'] == 'train']\n",
    "data_frame_bkg_train =  data_frame_bkg.loc[data_frame_bkg['gen_split'] == 'train']\n",
    "\n",
    "data_frame_fcnc_test = data_frame_fcnc.loc[data_frame_fcnc['gen_split'] == 'test']\n",
    "data_frame_bkg_test =  data_frame_bkg.loc[data_frame_bkg['gen_split'] == 'test']\n",
    "\n",
    "data_frame_fcnc_val = data_frame_fcnc.loc[data_frame_fcnc['gen_split'] == 'val']\n",
    "data_frame_bkg_val =  data_frame_bkg.loc[data_frame_bkg['gen_split'] == 'val']\n",
    "\n",
    "\n",
    "# get 5'' points of each dataset and  join the datasets (randomly)\n",
    "train_fcnc = data_frame_fcnc_train.sample(n=500)\n",
    "train_bkg = data_frame_bkg_train.sample(n=500)\n",
    "train = pd.concat([train_fcnc, train_bkg])\n",
    "train = train.sample(frac=1)\n",
    "\n",
    "test_fcnc = data_frame_fcnc_test.sample(n=500)\n",
    "test_bkg = data_frame_bkg_test.sample(n=500)\n",
    "test= pd.concat([test_fcnc, test_bkg])\n",
    "test = test.sample(frac=1)\n",
    "\n",
    "val_fnc = data_frame_fcnc_val.sample(n=500)\n",
    "val_bkg = data_frame_bkg_val.sample(n=500)\n",
    "val = pd.concat([val_fnc, val_bkg])\n",
    "val = val.sample(frac=1)\n",
    "\n",
    "# get the weights for each dataset\n",
    "w_train = train[['gen_xsec']]\n",
    "w_test = test[['gen_xsec']]\n",
    "w_val = val[['gen_xsec']]\n",
    "\n",
    "\n",
    "# change the signal and bkg labels to 0 and 1 and get the labels for each dataset\n",
    "train = train.replace(['signal'], 1)\n",
    "train= train.replace(['bkg'], 0)\n",
    "y_train = train[['gen_label']]\n",
    "x_train = train[['MissingET_MET', 'Jet1_BTag']]\n",
    "\n",
    "test = test.replace(['signal'], 1)\n",
    "test= test.replace(['bkg'], 0)\n",
    "y_test = test[['gen_label']]\n",
    "x_test = test[['MissingET_MET', 'Jet1_BTag']]\n",
    "\n",
    "val = val.replace(['signal'], 1)\n",
    "val= val.replace(['bkg'], 0)\n",
    "y_val = val[['gen_label']]\n",
    "x_val = val[['MissingET_MET', 'Jet1_BTag']]\n",
    "\n",
    "y_train_arr = np.concatenate(y_train.values, axis=0 )\n",
    "y_val_arr = np.concatenate( y_val.values, axis=0 )\n",
    "y_test_arr = np.concatenate( y_test.values, axis=0 )\n",
    "\n",
    "# Renormalize data weights\n",
    "w_train[y_train_arr == 1] = (w_train[y_train_arr == 1] / w_train[y_train_arr == 1].sum()) * y_train_arr.shape[0] / 2\n",
    "w_train[y_train_arr == 0] = (w_train[y_train_arr == 0] / w_train[y_train_arr == 0].sum()) * y_train_arr.shape[0] / 2\n",
    "        \n",
    "w_test[y_test_arr == 1] = (w_test[y_test_arr == 1] / w_test[y_test_arr == 1].sum()) * w_test.shape[0] / 2\n",
    "w_test[y_test_arr == 0] = (w_test[y_test_arr == 0] / w_test[y_test_arr == 0].sum()) * w_test.shape[0] / 2\n",
    "        \n",
    "w_val[y_test_arr == 1] = (w_val[y_test_arr == 1] / w_val[y_test_arr == 1].sum()) * w_val.shape[0] / 2\n",
    "w_val[y_test_arr == 0] = (w_val[y_test_arr == 0] / w_val[y_test_arr == 0].sum()) * w_val.shape[0] / 2\n",
    "\n",
    "# Concatenate features\n",
    "X = np.concatenate([x_train, x_test,x_val])\n",
    "\n",
    "# Normalize the data for angle embeding  (Put the data between -pi and pi)\n",
    "X = (((X - X.min()) / (X.max() - X.min())) * 2 - 1) * (np.pi)\n",
    "\n",
    "# Split the features array into train, validation and test sets\n",
    "x_train = X[:1000]\n",
    "x_test = X[1000:2000]\n",
    "x_val = X[2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy function\n",
    "def accuracy(labels, predictions):\n",
    "\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        if abs(l - p) < 1e-5:\n",
    "            loss = loss + 1\n",
    "    loss = loss / len(labels)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loss function\n",
    "def square_loss(labels, predictions):\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        loss = loss + (l - p) ** 2\n",
    "\n",
    "    loss = loss / len(labels)\n",
    "    return loss\n",
    "\n",
    "# quantum circuit function\n",
    "def circuit(n_features, n_layers, weights, x):\n",
    "        # Embedding\n",
    "        \n",
    "        qml.AngleEmbedding(x,range (0, n_features),rotation=\"X\" )\n",
    "\n",
    "        # For every layer\n",
    "        for layer in range(n_layers):\n",
    "            W1 = weights[layer]\n",
    "\n",
    "            # Define Rotations\n",
    "            for i in range(0,n_features):\n",
    "                qml.Rot(W1[i, 0], W1[i, 1], W1[i, 2], wires=i)\n",
    "\n",
    "            # Entanglement\n",
    "            if n_features != 1:\n",
    "                if n_features > 2:\n",
    "                    for i in range(n_features):\n",
    "                        if i == n_features - 1:\n",
    "                            qml.CNOT(wires=[i, 0])\n",
    "                        else:\n",
    "                            qml.CNOT(wires=[i, i + 1])\n",
    "                else:\n",
    "                    qml.CNOT(wires=[1, 0])\n",
    "\n",
    "        return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "# classifier function    \n",
    "def classifier(n_features, n_layers, weights, x):\n",
    "        #c = circuit(n_features, n_layers, weights, x)\n",
    "        dev=qml.device(\"default.qubit\", wires=n_features)\n",
    "        return qml.QNode(circuit, dev)(n_features, n_layers, weights, x)\n",
    "    \n",
    "# cost function    \n",
    "def cost(n_features, n_layers,weights,X,Y,W):  \n",
    "        # Compute predictions\n",
    "        y_scores = [(classifier(n_features, n_layers,weights, x) + 1) / 2 for x in X]\n",
    "\n",
    "        loss = square_loss(Y, y_scores)\n",
    "        loss = loss * W\n",
    "        loss = loss.sum()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "# train step function    \n",
    "def train_step(n_features, n_layers,x_train,y_train, w_train, weights, opt,desc='Training'):\n",
    "        \n",
    "        # Only require grad if necessary\n",
    "        x_train = np.array(x_train, requires_grad=False)\n",
    "        y_train = np.array(y_train, requires_grad=True)\n",
    "        w_train = np.array(w_train, requires_grad=False)\n",
    "\n",
    "        # Compute cost and update weights\n",
    "        weights, loss = opt.step_and_cost(cost, n_features, n_layers,weights, X=x_train, Y=y_train, W=w_train)\n",
    "\n",
    "        return loss, weights\n",
    "    \n",
    "# validation step function\n",
    "def validation_step(n_features, n_layers, x_val, y_val, w_val, weights, best_score, epoch_number, best_score_epoch,best_weights,desc='Validation'):\n",
    "    X_val = np.array(x_val, requires_grad=False)\n",
    "    Y_val = np.array(y_val, requires_grad=False)\n",
    "    W_val = np.array(w_val, requires_grad=False)\n",
    "\n",
    "    y_scores = np.array([classifier(n_features, n_layers, weights, x) for x in X_val])\n",
    "    y_scores = (y_scores + 1) / 2\n",
    "\n",
    "    W_val[Y_val == 1] = (W_val[Y_val == 1] / W_val[Y_val == 1].sum()) * W_val.shape[0] / 2\n",
    "    W_val[Y_val == 0] = (W_val[Y_val == 0] / W_val[Y_val == 0].sum()) * W_val.shape[0] / 2\n",
    "\n",
    "    auc_score = roc_auc_score(y_true=Y_val, y_score=y_scores, sample_weight=W_val)\n",
    "    loss = cost(n_features, n_layers, weights, X_val, Y_val, W_val)\n",
    "\n",
    "\n",
    "    if best_score is None or auc_score > best_score:\n",
    "        best_score = auc_score\n",
    "        best_score_epoch = epoch_number\n",
    "        best_weights = weights\n",
    "\n",
    "    tqdm.write(f\"Epoch: {epoch_number}, Validation Loss: {loss:.4f}, AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "    return best_score, best_score_epoch, best_weights\n",
    "        \n",
    "        \n",
    "        \n",
    "# train function\n",
    "def train(n_features, n_layers, x_train, y_train, learning_rate, weights, max_epochs, epoch_number):\n",
    "    opt = AdamOptimizer(learning_rate)\n",
    "    best_score = None\n",
    "    best_weights = None\n",
    "    best_score_epoch = None\n",
    "\n",
    "    with tqdm(total=max_epochs, desc='Epoch', unit='epoch') as pbar:\n",
    "        for epoch in range(epoch_number, max_epochs):\n",
    "            epoch_number = epoch\n",
    "\n",
    "            loss, nf_nl_weights = train_step(n_features, n_layers, x_train, y_train, w_train, weights, opt, desc='Training')\n",
    "            \n",
    "            # Log variable values using tqdm.write\n",
    "            tqdm.write(f\"Epoch: {epoch_number:}, Loss: {loss:.4f}\")\n",
    "            \n",
    "            \n",
    "            weights = nf_nl_weights[2:]\n",
    "            weights = weights[0]\n",
    "\n",
    "            if epoch_number == max_epochs - 1 or (epoch_number+1)%5==0:\n",
    "                best_score, best_score_epoch, best_weights = validation_step(n_features, n_layers, x_val, y_val, w_val, weights, best_score, epoch_number, best_score_epoch, best_weights,desc='Validation')\n",
    "                # early stopping\n",
    "                if epoch_number - best_score_epoch > 30 and epoch_number > 80:\n",
    "                    tqdm.write(f\"Early stopping at epoch {epoch_number}\")\n",
    "                    break\n",
    "\n",
    "            pbar.update(1)  # Update progress bar\n",
    "        tqdm.write(f\"Best Score: {best_score:.4f}\")            \n",
    "        \n",
    "    return best_score, best_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa310174e7b44d38af0387a74f59622f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1000 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 510.0425\n",
      "Epoch: 1, Loss: 509.8386\n",
      "Epoch: 2, Loss: 509.2260\n",
      "Epoch: 3, Loss: 508.1480\n",
      "Epoch: 4, Loss: 506.5491\n",
      "Epoch: 4, Validation Loss: 510.0878, AUC Score: 0.2049\n",
      "Epoch: 5, Loss: 504.3978\n",
      "Epoch: 6, Loss: 501.6759\n",
      "Epoch: 7, Loss: 498.3757\n",
      "Epoch: 8, Loss: 494.4996\n",
      "Epoch: 9, Loss: 490.0604\n",
      "Epoch: 9, Validation Loss: 490.7805, AUC Score: 0.2045\n",
      "Epoch: 10, Loss: 485.0823\n",
      "Epoch: 11, Loss: 479.6010\n",
      "Epoch: 12, Loss: 473.6635\n",
      "Epoch: 13, Loss: 467.3264\n",
      "Epoch: 14, Loss: 460.6516\n",
      "Epoch: 14, Validation Loss: 459.4497, AUC Score: 0.2053\n",
      "Epoch: 15, Loss: 453.6963\n",
      "Epoch: 16, Loss: 446.5009\n",
      "Epoch: 17, Loss: 439.0881\n",
      "Epoch: 18, Loss: 431.4735\n",
      "Epoch: 19, Loss: 423.6696\n",
      "Epoch: 19, Validation Loss: 421.4074, AUC Score: 0.2063\n",
      "Epoch: 20, Loss: 415.6888\n",
      "Epoch: 21, Loss: 407.5482\n",
      "Epoch: 22, Loss: 399.2709\n",
      "Epoch: 23, Loss: 390.8836\n",
      "Epoch: 24, Loss: 382.4153\n",
      "Epoch: 24, Validation Loss: 379.1753, AUC Score: 0.2060\n",
      "Epoch: 25, Loss: 373.8958\n",
      "Epoch: 26, Loss: 365.3563\n",
      "Epoch: 27, Loss: 356.8299\n",
      "Epoch: 28, Loss: 348.3527\n",
      "Epoch: 29, Loss: 339.9630\n",
      "Epoch: 29, Validation Loss: 336.0212, AUC Score: 0.2061\n",
      "Epoch: 30, Loss: 331.7012\n",
      "Epoch: 31, Loss: 323.6084\n",
      "Epoch: 32, Loss: 315.7255\n",
      "Epoch: 33, Loss: 308.0925\n",
      "Epoch: 34, Loss: 300.7473\n",
      "Epoch: 34, Validation Loss: 296.7526, AUC Score: 0.2045\n",
      "Epoch: 35, Loss: 293.7244\n",
      "Epoch: 36, Loss: 287.0542\n",
      "Epoch: 37, Loss: 280.7613\n",
      "Epoch: 38, Loss: 274.8642\n",
      "Epoch: 39, Loss: 269.3746\n",
      "Epoch: 39, Validation Loss: 266.1235, AUC Score: 0.3542\n",
      "Epoch: 40, Loss: 264.2965\n",
      "Epoch: 41, Loss: 259.6260\n",
      "Epoch: 42, Loss: 255.3509\n",
      "Epoch: 43, Loss: 251.4513\n",
      "Epoch: 44, Loss: 247.9006\n",
      "Epoch: 44, Validation Loss: 245.6110, AUC Score: 0.7037\n",
      "Epoch: 45, Loss: 244.6686\n",
      "Epoch: 46, Loss: 241.7237\n",
      "Epoch: 47, Loss: 239.0356\n",
      "Epoch: 48, Loss: 236.5774\n",
      "Epoch: 49, Loss: 234.3259\n",
      "Epoch: 49, Validation Loss: 232.4415, AUC Score: 0.7560\n",
      "Epoch: 50, Loss: 232.2615\n",
      "Epoch: 51, Loss: 230.3675\n",
      "Epoch: 52, Loss: 228.6299\n",
      "Epoch: 53, Loss: 227.0366\n",
      "Epoch: 54, Loss: 225.5769\n",
      "Epoch: 54, Validation Loss: 223.6401, AUC Score: 0.7716\n",
      "Epoch: 55, Loss: 224.2405\n",
      "Epoch: 56, Loss: 223.0176\n",
      "Epoch: 57, Loss: 221.8982\n",
      "Epoch: 58, Loss: 220.8727\n",
      "Epoch: 59, Loss: 219.9316\n",
      "Epoch: 59, Validation Loss: 217.7994, AUC Score: 0.7790\n",
      "Epoch: 60, Loss: 219.0659\n",
      "Epoch: 61, Loss: 218.2672\n",
      "Epoch: 62, Loss: 217.5282\n",
      "Epoch: 63, Loss: 216.8422\n",
      "Epoch: 64, Loss: 216.2038\n",
      "Epoch: 64, Validation Loss: 213.9083, AUC Score: 0.7827\n",
      "Epoch: 65, Loss: 215.6081\n",
      "Epoch: 66, Loss: 215.0515\n",
      "Epoch: 67, Loss: 214.5308\n",
      "Epoch: 68, Loss: 214.0431\n",
      "Epoch: 69, Loss: 213.5863\n",
      "Epoch: 69, Validation Loss: 211.2371, AUC Score: 0.7840\n",
      "Epoch: 70, Loss: 213.1579\n",
      "Epoch: 71, Loss: 212.7559\n",
      "Epoch: 72, Loss: 212.3781\n",
      "Epoch: 73, Loss: 212.0223\n",
      "Epoch: 74, Loss: 211.6864\n",
      "Epoch: 74, Validation Loss: 209.3173, AUC Score: 0.7850\n",
      "Epoch: 75, Loss: 211.3683\n",
      "Epoch: 76, Loss: 211.0662\n",
      "Epoch: 77, Loss: 210.7786\n",
      "Epoch: 78, Loss: 210.5041\n",
      "Epoch: 79, Loss: 210.2415\n",
      "Epoch: 79, Validation Loss: 207.8007, AUC Score: 0.7868\n",
      "Epoch: 80, Loss: 209.9898\n",
      "Epoch: 81, Loss: 209.7483\n",
      "Epoch: 82, Loss: 209.5161\n",
      "Epoch: 83, Loss: 209.2924\n",
      "Epoch: 84, Loss: 209.0764\n",
      "Epoch: 84, Validation Loss: 206.5439, AUC Score: 0.7871\n",
      "Epoch: 85, Loss: 208.8674\n",
      "Epoch: 86, Loss: 208.6646\n",
      "Epoch: 87, Loss: 208.4674\n",
      "Epoch: 88, Loss: 208.2754\n",
      "Epoch: 89, Loss: 208.0880\n",
      "Epoch: 89, Validation Loss: 205.5060, AUC Score: 0.7882\n",
      "Epoch: 90, Loss: 207.9050\n",
      "Epoch: 91, Loss: 207.7263\n",
      "Epoch: 92, Loss: 207.5516\n",
      "Epoch: 93, Loss: 207.3807\n",
      "Epoch: 94, Loss: 207.2136\n",
      "Epoch: 94, Validation Loss: 204.6185, AUC Score: 0.7883\n",
      "Epoch: 95, Loss: 207.0499\n",
      "Epoch: 96, Loss: 206.8894\n",
      "Epoch: 97, Loss: 206.7321\n",
      "Epoch: 98, Loss: 206.5777\n",
      "Epoch: 99, Loss: 206.4261\n",
      "Epoch: 99, Validation Loss: 203.7982, AUC Score: 0.7882\n",
      "Epoch: 100, Loss: 206.2773\n",
      "Epoch: 101, Loss: 206.1313\n",
      "Epoch: 102, Loss: 205.9879\n",
      "Epoch: 103, Loss: 205.8472\n",
      "Epoch: 104, Loss: 205.7091\n",
      "Epoch: 104, Validation Loss: 203.0313, AUC Score: 0.7884\n",
      "Epoch: 105, Loss: 205.5735\n",
      "Epoch: 106, Loss: 205.4404\n",
      "Epoch: 107, Loss: 205.3096\n",
      "Epoch: 108, Loss: 205.1812\n",
      "Epoch: 109, Loss: 205.0551\n",
      "Epoch: 109, Validation Loss: 202.3490, AUC Score: 0.7884\n",
      "Epoch: 110, Loss: 204.9313\n",
      "Epoch: 111, Loss: 204.8098\n",
      "Epoch: 112, Loss: 204.6905\n",
      "Epoch: 113, Loss: 204.5735\n",
      "Epoch: 114, Loss: 204.4588\n",
      "Epoch: 114, Validation Loss: 201.7390, AUC Score: 0.7885\n",
      "Epoch: 115, Loss: 204.3462\n",
      "Epoch: 116, Loss: 204.2358\n",
      "Epoch: 117, Loss: 204.1275\n",
      "Epoch: 118, Loss: 204.0214\n",
      "Epoch: 119, Loss: 203.9174\n",
      "Epoch: 119, Validation Loss: 201.1721, AUC Score: 0.7885\n",
      "Epoch: 120, Loss: 203.8155\n",
      "Epoch: 121, Loss: 203.7157\n",
      "Epoch: 122, Loss: 203.6180\n",
      "Epoch: 123, Loss: 203.5223\n",
      "Epoch: 124, Loss: 203.4287\n",
      "Epoch: 124, Validation Loss: 200.6571, AUC Score: 0.7890\n",
      "Epoch: 125, Loss: 203.3371\n",
      "Epoch: 126, Loss: 203.2475\n",
      "Epoch: 127, Loss: 203.1598\n",
      "Epoch: 128, Loss: 203.0742\n",
      "Epoch: 129, Loss: 202.9906\n",
      "Epoch: 129, Validation Loss: 200.2049, AUC Score: 0.7890\n",
      "Epoch: 130, Loss: 202.9089\n",
      "Epoch: 131, Loss: 202.8292\n",
      "Epoch: 132, Loss: 202.7515\n",
      "Epoch: 133, Loss: 202.6757\n",
      "Epoch: 134, Loss: 202.6019\n",
      "Epoch: 134, Validation Loss: 199.8020, AUC Score: 0.7890\n",
      "Epoch: 135, Loss: 202.5301\n",
      "Epoch: 136, Loss: 202.4602\n",
      "Epoch: 137, Loss: 202.3923\n",
      "Epoch: 138, Loss: 202.3263\n",
      "Epoch: 139, Loss: 202.2623\n",
      "Epoch: 139, Validation Loss: 199.4439, AUC Score: 0.7891\n",
      "Epoch: 140, Loss: 202.2002\n",
      "Epoch: 141, Loss: 202.1401\n",
      "Epoch: 142, Loss: 202.0819\n",
      "Epoch: 143, Loss: 202.0256\n",
      "Epoch: 144, Loss: 201.9713\n",
      "Epoch: 144, Validation Loss: 199.1391, AUC Score: 0.7891\n",
      "Epoch: 145, Loss: 201.9189\n",
      "Epoch: 146, Loss: 201.8684\n",
      "Epoch: 147, Loss: 201.8198\n",
      "Epoch: 148, Loss: 201.7730\n",
      "Epoch: 149, Loss: 201.7282\n",
      "Epoch: 149, Validation Loss: 198.8855, AUC Score: 0.7883\n",
      "Epoch: 150, Loss: 201.6852\n",
      "Epoch: 151, Loss: 201.6441\n",
      "Epoch: 152, Loss: 201.6048\n",
      "Epoch: 153, Loss: 201.5672\n",
      "Epoch: 154, Loss: 201.5315\n",
      "Epoch: 154, Validation Loss: 198.6767, AUC Score: 0.7880\n",
      "Epoch: 155, Loss: 201.4975\n",
      "Epoch: 156, Loss: 201.4651\n",
      "Epoch: 157, Loss: 201.4345\n",
      "Epoch: 158, Loss: 201.4054\n",
      "Epoch: 159, Loss: 201.3780\n",
      "Epoch: 159, Validation Loss: 198.5125, AUC Score: 0.7879\n",
      "Epoch: 160, Loss: 201.3521\n",
      "Epoch: 161, Loss: 201.3276\n",
      "Epoch: 162, Loss: 201.3046\n",
      "Epoch: 163, Loss: 201.2830\n",
      "Epoch: 164, Loss: 201.2628\n",
      "Epoch: 164, Validation Loss: 198.3898, AUC Score: 0.7875\n",
      "Epoch: 165, Loss: 201.2438\n",
      "Epoch: 166, Loss: 201.2260\n",
      "Epoch: 167, Loss: 201.2094\n",
      "Epoch: 168, Loss: 201.1939\n",
      "Epoch: 169, Loss: 201.1795\n",
      "Epoch: 169, Validation Loss: 198.2998, AUC Score: 0.7874\n",
      "Epoch: 170, Loss: 201.1660\n",
      "Epoch: 171, Loss: 201.1535\n",
      "Epoch: 172, Loss: 201.1419\n",
      "Epoch: 173, Loss: 201.1311\n",
      "Epoch: 174, Loss: 201.1211\n",
      "Epoch: 174, Validation Loss: 198.2354, AUC Score: 0.7874\n",
      "Early stopping at epoch 174\n",
      "Best Score: 0.7891\n"
     ]
    }
   ],
   "source": [
    "# number of features and layers\n",
    "n_features = 2\n",
    "n_layers = 3\n",
    "\n",
    "# Random weight initialization\n",
    "weights = 0.01 * np.random.randn(n_layers, n_features, 3, requires_grad=True)\n",
    "\n",
    "# We create a quantum device with n_features \"wires\" (or qubits)\n",
    "dev = qml.device('default.qubit', wires=n_features)\n",
    "\n",
    "# train the model\n",
    "best_score, best_weights = train (n_features, n_layers,x_train,y_train_arr,0.01, weights, 1000, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7890943370681786\n"
     ]
    }
   ],
   "source": [
    "print (best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568898558963438\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "\n",
    "def test(n_features, n_layers,x_test,y_test,w_test, weights):\n",
    "        # Remove grad\n",
    "        X_test = np.array(x_test, requires_grad=False)\n",
    "        Y_test = np.array(y_test, requires_grad=False)\n",
    "        W_test = np.array(w_test, requires_grad=False)\n",
    "\n",
    "        # This will be between -1 and 1, we need to convert to between 0 and 1\n",
    "        y_scores = np.array([classifier(n_features, n_layers,weights, x) for x in X_test])\n",
    "        y_scores = (y_scores + 1) / 2\n",
    "\n",
    "        # Renormalize weights\n",
    "        W_test[Y_test == 1] = (W_test[Y_test == 1] / W_test[Y_test == 1].sum()) * W_test.shape[0] / 2\n",
    "        W_test[Y_test == 0] = (W_test[Y_test == 0] / W_test[Y_test == 0].sum()) * W_test.shape[0] / 2\n",
    "\n",
    "        # Calculate ROC\n",
    "        auc_score = roc_auc_score(y_true=Y_test, y_score=y_scores, sample_weight=W_test)\n",
    "        \n",
    "        return auc_score\n",
    "    \n",
    "auc_score = test(n_features, n_layers,x_test,y_test,w_test, best_weights)\n",
    "print (auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### default.qubit.tf Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-12 10:29:57.323912: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-12 10:29:57.325997: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-12 10:29:57.362884: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-12 10:29:57.363899: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-12 10:29:57.836434: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-07-12 10:29:58.330266: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658e2d1d47434ea786b7cd64bd69856f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1000 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 509.9651\n",
      "Epoch: 1, Loss: 509.3178\n",
      "Epoch: 2, Loss: 508.1373\n",
      "Epoch: 3, Loss: 506.3903\n",
      "Epoch: 4, Loss: 504.0652\n",
      "Epoch: 4, Validation Loss: 506.8460, AUC Score: 0.2050\n",
      "Epoch: 5, Loss: 501.1568\n",
      "Epoch: 6, Loss: 497.6660\n",
      "Epoch: 7, Loss: 493.6026\n",
      "Epoch: 8, Loss: 488.9864\n",
      "Epoch: 9, Loss: 483.8470\n",
      "Epoch: 9, Validation Loss: 483.9308, AUC Score: 0.2040\n",
      "Epoch: 10, Loss: 478.2245\n",
      "Epoch: 11, Loss: 472.1687\n",
      "Epoch: 12, Loss: 465.7373\n",
      "Epoch: 13, Loss: 458.9918\n",
      "Epoch: 14, Loss: 451.9898\n",
      "Epoch: 14, Validation Loss: 450.5407, AUC Score: 0.2048\n",
      "Epoch: 15, Loss: 444.7758\n",
      "Epoch: 16, Loss: 437.3762\n",
      "Epoch: 17, Loss: 429.7998\n",
      "Epoch: 18, Loss: 422.0462\n",
      "Epoch: 19, Loss: 414.1195\n",
      "Epoch: 19, Validation Loss: 411.7187, AUC Score: 0.2063\n",
      "Epoch: 20, Loss: 406.0341\n",
      "Epoch: 21, Loss: 397.8136\n",
      "Epoch: 22, Loss: 389.4869\n",
      "Epoch: 23, Loss: 381.0853\n",
      "Epoch: 24, Loss: 372.6400\n",
      "Epoch: 24, Validation Loss: 369.3196, AUC Score: 0.2059\n",
      "Epoch: 25, Loss: 364.1818\n",
      "Epoch: 26, Loss: 355.7426\n",
      "Epoch: 27, Loss: 347.3559\n",
      "Epoch: 28, Loss: 339.0588\n",
      "Epoch: 29, Loss: 330.8909\n",
      "Epoch: 29, Validation Loss: 326.9885, AUC Score: 0.2056\n",
      "Epoch: 30, Loss: 322.8936\n",
      "Epoch: 31, Loss: 315.1081\n",
      "Epoch: 32, Loss: 307.5742\n",
      "Epoch: 33, Loss: 300.3290\n",
      "Epoch: 34, Loss: 293.4059\n",
      "Epoch: 34, Validation Loss: 289.6465, AUC Score: 0.2035\n",
      "Epoch: 35, Loss: 286.8336\n",
      "Epoch: 36, Loss: 280.6353\n",
      "Epoch: 37, Loss: 274.8278\n",
      "Epoch: 38, Loss: 269.4209\n",
      "Epoch: 39, Loss: 264.4169\n",
      "Epoch: 39, Validation Loss: 261.4963, AUC Score: 0.4968\n",
      "Epoch: 40, Loss: 259.8099\n",
      "Epoch: 41, Loss: 255.5867\n"
     ]
    }
   ],
   "source": [
    "# number of features and layers\n",
    "n_features = 2\n",
    "n_layers = 3\n",
    "\n",
    "# Random weight initialization\n",
    "weights = 0.01 * np.random.randn(n_layers, n_features, 3, requires_grad=True)\n",
    "\n",
    "# We create a quantum device with n_features \"wires\" (or qubits) \n",
    "dev =qml.device('default.qubit.tf', wires=n_features) \n",
    " \n",
    "# train the model\n",
    "best_score, best_weights = train (n_features, n_layers,x_train,y_train_arr,0.01, weights, 1000, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7221942085766418\n"
     ]
    }
   ],
   "source": [
    "print (best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8027792189265726\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "\n",
    "def test(n_features, n_layers,x_test,y_test,w_test, weights):\n",
    "        # Remove grad\n",
    "        X_test = np.array(x_test, requires_grad=False)\n",
    "        Y_test = np.array(y_test, requires_grad=False)\n",
    "        W_test = np.array(w_test, requires_grad=False)\n",
    "\n",
    "        # This will be between -1 and 1, we need to convert to between 0 and 1\n",
    "        y_scores = np.array([classifier(n_features, n_layers,weights, x) for x in X_test])\n",
    "        y_scores = (y_scores + 1) / 2\n",
    "\n",
    "        # Renormalize weights\n",
    "        W_test[Y_test == 1] = (W_test[Y_test == 1] / W_test[Y_test == 1].sum()) * W_test.shape[0] / 2\n",
    "        W_test[Y_test == 0] = (W_test[Y_test == 0] / W_test[Y_test == 0].sum()) * W_test.shape[0] / 2\n",
    "\n",
    "        # Calculate ROC\n",
    "        auc_score = roc_auc_score(y_true=Y_test, y_score=y_scores, sample_weight=W_test)\n",
    "        \n",
    "        return auc_score\n",
    "    \n",
    "auc_score = test(n_features, n_layers,x_test,y_test,w_test, best_weights)\n",
    "print (auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### default.qubit.autograd Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e2281da81e4978b6fe9b487f65a81e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1000 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 260.7758\n",
      "Epoch: 1, Loss: 260.4985\n",
      "Epoch: 2, Loss: 259.9815\n",
      "Epoch: 3, Loss: 259.1910\n",
      "Epoch: 4, Loss: 258.1202\n",
      "Epoch: 5, Loss: 256.7669\n",
      "Epoch: 6, Loss: 255.1331\n",
      "Epoch: 7, Loss: 253.2255\n",
      "Epoch: 8, Loss: 251.0544\n",
      "Epoch: 9, Loss: 248.6347\n",
      "Epoch: 9, Validation Loss: 253.2934, AUC Score: 0.1791\n",
      "Epoch: 10, Loss: 245.9862\n",
      "Epoch: 11, Loss: 243.1317\n",
      "Epoch: 12, Loss: 240.0952\n",
      "Epoch: 13, Loss: 236.8960\n",
      "Epoch: 14, Loss: 233.5446\n",
      "Epoch: 15, Loss: 230.0417\n",
      "Epoch: 16, Loss: 226.3849\n",
      "Epoch: 17, Loss: 222.5731\n",
      "Epoch: 18, Loss: 218.6095\n",
      "Epoch: 19, Loss: 214.5019\n",
      "Epoch: 19, Validation Loss: 215.1435, AUC Score: 0.1793\n",
      "Epoch: 20, Loss: 210.2610\n",
      "Epoch: 21, Loss: 205.8995\n",
      "Epoch: 22, Loss: 201.4313\n",
      "Epoch: 23, Loss: 196.8725\n",
      "Epoch: 24, Loss: 192.2420\n",
      "Epoch: 25, Loss: 187.5622\n",
      "Epoch: 26, Loss: 182.8589\n",
      "Epoch: 27, Loss: 178.1607\n",
      "Epoch: 28, Loss: 173.4973\n",
      "Epoch: 29, Loss: 168.8985\n",
      "Epoch: 29, Validation Loss: 166.6355, AUC Score: 0.1789\n",
      "Epoch: 30, Loss: 164.3935\n",
      "Epoch: 31, Loss: 160.0106\n",
      "Epoch: 32, Loss: 155.7763\n",
      "Epoch: 33, Loss: 151.7150\n",
      "Epoch: 34, Loss: 147.8478\n",
      "Epoch: 35, Loss: 144.1909\n",
      "Epoch: 36, Loss: 140.7553\n",
      "Epoch: 37, Loss: 137.5456\n",
      "Epoch: 38, Loss: 134.5609\n",
      "Epoch: 39, Loss: 131.7950\n",
      "Epoch: 39, Validation Loss: 129.7142, AUC Score: 0.5488\n",
      "Epoch: 40, Loss: 129.2383\n",
      "Epoch: 41, Loss: 126.8788\n",
      "Epoch: 42, Loss: 124.7039\n",
      "Epoch: 43, Loss: 122.7010\n",
      "Epoch: 44, Loss: 120.8587\n",
      "Epoch: 45, Loss: 119.1665\n",
      "Epoch: 46, Loss: 117.6152\n",
      "Epoch: 47, Loss: 116.1962\n",
      "Epoch: 48, Loss: 114.9014\n",
      "Epoch: 49, Loss: 113.7222\n",
      "Epoch: 49, Validation Loss: 112.0800, AUC Score: 0.7712\n",
      "Epoch: 50, Loss: 112.6500\n",
      "Epoch: 51, Loss: 111.6756\n",
      "Epoch: 52, Loss: 110.7899\n",
      "Epoch: 53, Loss: 109.9839\n",
      "Epoch: 54, Loss: 109.2490\n",
      "Epoch: 55, Loss: 108.5773\n",
      "Epoch: 56, Loss: 107.9620\n",
      "Epoch: 57, Loss: 107.3969\n",
      "Epoch: 58, Loss: 106.8768\n",
      "Epoch: 59, Loss: 106.3973\n",
      "Epoch: 59, Validation Loss: 104.7168, AUC Score: 0.7942\n",
      "Epoch: 60, Loss: 105.9548\n",
      "Epoch: 61, Loss: 105.5459\n",
      "Epoch: 62, Loss: 105.1678\n",
      "Epoch: 63, Loss: 104.8179\n",
      "Epoch: 64, Loss: 104.4938\n",
      "Epoch: 65, Loss: 104.1932\n",
      "Epoch: 66, Loss: 103.9138\n",
      "Epoch: 67, Loss: 103.6536\n",
      "Epoch: 68, Loss: 103.4107\n",
      "Epoch: 69, Loss: 103.1833\n",
      "Epoch: 69, Validation Loss: 101.2030, AUC Score: 0.7994\n",
      "Epoch: 70, Loss: 102.9698\n",
      "Epoch: 71, Loss: 102.7690\n",
      "Epoch: 72, Loss: 102.5795\n",
      "Epoch: 73, Loss: 102.4004\n",
      "Epoch: 74, Loss: 102.2306\n",
      "Epoch: 75, Loss: 102.0693\n",
      "Epoch: 76, Loss: 101.9157\n",
      "Epoch: 77, Loss: 101.7689\n",
      "Epoch: 78, Loss: 101.6281\n",
      "Epoch: 79, Loss: 101.4929\n",
      "Epoch: 79, Validation Loss: 99.2722, AUC Score: 0.8060\n",
      "Epoch: 80, Loss: 101.3624\n",
      "Epoch: 81, Loss: 101.2362\n",
      "Epoch: 82, Loss: 101.1139\n",
      "Epoch: 83, Loss: 100.9952\n",
      "Epoch: 84, Loss: 100.8797\n",
      "Epoch: 85, Loss: 100.7673\n",
      "Epoch: 86, Loss: 100.6578\n",
      "Epoch: 87, Loss: 100.5509\n",
      "Epoch: 88, Loss: 100.4466\n",
      "Epoch: 89, Loss: 100.3446\n",
      "Epoch: 89, Validation Loss: 97.8596, AUC Score: 0.8087\n",
      "Epoch: 90, Loss: 100.2447\n",
      "Epoch: 91, Loss: 100.1470\n",
      "Epoch: 92, Loss: 100.0511\n",
      "Epoch: 93, Loss: 99.9571\n",
      "Epoch: 94, Loss: 99.8650\n",
      "Epoch: 95, Loss: 99.7746\n",
      "Epoch: 96, Loss: 99.6859\n",
      "Epoch: 97, Loss: 99.5990\n",
      "Epoch: 98, Loss: 99.5138\n",
      "Epoch: 99, Loss: 99.4302\n",
      "Epoch: 99, Validation Loss: 96.7261, AUC Score: 0.8099\n",
      "Epoch: 100, Loss: 99.3482\n",
      "Epoch: 101, Loss: 99.2678\n",
      "Epoch: 102, Loss: 99.1889\n",
      "Epoch: 103, Loss: 99.1115\n",
      "Epoch: 104, Loss: 99.0356\n",
      "Epoch: 105, Loss: 98.9612\n",
      "Epoch: 106, Loss: 98.8884\n",
      "Epoch: 107, Loss: 98.8170\n",
      "Epoch: 108, Loss: 98.7470\n",
      "Epoch: 109, Loss: 98.6786\n",
      "Epoch: 109, Validation Loss: 95.7468, AUC Score: 0.8101\n",
      "Epoch: 110, Loss: 98.6115\n",
      "Epoch: 111, Loss: 98.5459\n",
      "Epoch: 112, Loss: 98.4817\n",
      "Epoch: 113, Loss: 98.4190\n",
      "Epoch: 114, Loss: 98.3576\n",
      "Epoch: 115, Loss: 98.2976\n",
      "Epoch: 116, Loss: 98.2390\n",
      "Epoch: 117, Loss: 98.1818\n",
      "Epoch: 118, Loss: 98.1260\n",
      "Epoch: 119, Loss: 98.0715\n",
      "Epoch: 119, Validation Loss: 94.9347, AUC Score: 0.8106\n",
      "Epoch: 120, Loss: 98.0184\n",
      "Epoch: 121, Loss: 97.9665\n",
      "Epoch: 122, Loss: 97.9161\n",
      "Epoch: 123, Loss: 97.8669\n",
      "Epoch: 124, Loss: 97.8191\n",
      "Epoch: 125, Loss: 97.7726\n",
      "Epoch: 126, Loss: 97.7275\n",
      "Epoch: 127, Loss: 97.6836\n",
      "Epoch: 128, Loss: 97.6411\n",
      "Epoch: 129, Loss: 97.5999\n",
      "Epoch: 129, Validation Loss: 94.2605, AUC Score: 0.8106\n",
      "Epoch: 130, Loss: 97.5600\n",
      "Epoch: 131, Loss: 97.5215\n",
      "Epoch: 132, Loss: 97.4843\n",
      "Epoch: 133, Loss: 97.4483\n",
      "Epoch: 134, Loss: 97.4137\n",
      "Epoch: 135, Loss: 97.3804\n",
      "Epoch: 136, Loss: 97.3484\n",
      "Epoch: 137, Loss: 97.3176\n",
      "Epoch: 138, Loss: 97.2882\n",
      "Epoch: 139, Loss: 97.2600\n",
      "Epoch: 139, Validation Loss: 93.7432, AUC Score: 0.8102\n",
      "Epoch: 140, Loss: 97.2330\n",
      "Epoch: 141, Loss: 97.2073\n",
      "Epoch: 142, Loss: 97.1829\n",
      "Epoch: 143, Loss: 97.1596\n",
      "Epoch: 144, Loss: 97.1375\n",
      "Epoch: 145, Loss: 97.1166\n",
      "Epoch: 146, Loss: 97.0968\n",
      "Epoch: 147, Loss: 97.0781\n",
      "Epoch: 148, Loss: 97.0605\n",
      "Epoch: 149, Loss: 97.0440\n",
      "Epoch: 149, Validation Loss: 93.3736, AUC Score: 0.8101\n",
      "Epoch: 150, Loss: 97.0284\n",
      "Epoch: 151, Loss: 97.0139\n",
      "Epoch: 152, Loss: 97.0003\n",
      "Epoch: 153, Loss: 96.9876\n",
      "Epoch: 154, Loss: 96.9758\n",
      "Epoch: 155, Loss: 96.9648\n",
      "Epoch: 156, Loss: 96.9546\n",
      "Epoch: 157, Loss: 96.9451\n",
      "Epoch: 158, Loss: 96.9364\n",
      "Epoch: 159, Loss: 96.9283\n",
      "Epoch: 159, Validation Loss: 93.1389, AUC Score: 0.8100\n",
      "Epoch: 160, Loss: 96.9209\n",
      "Epoch: 161, Loss: 96.9140\n",
      "Epoch: 162, Loss: 96.9077\n",
      "Epoch: 163, Loss: 96.9019\n",
      "Epoch: 164, Loss: 96.8966\n",
      "Epoch: 165, Loss: 96.8917\n",
      "Epoch: 166, Loss: 96.8872\n",
      "Epoch: 167, Loss: 96.8831\n",
      "Epoch: 168, Loss: 96.8794\n",
      "Epoch: 169, Loss: 96.8759\n",
      "Epoch: 169, Validation Loss: 93.0008, AUC Score: 0.8099\n",
      "Epoch: 170, Loss: 96.8728\n",
      "Epoch: 171, Loss: 96.8699\n",
      "Epoch: 172, Loss: 96.8673\n",
      "Epoch: 173, Loss: 96.8649\n",
      "Epoch: 174, Loss: 96.8626\n",
      "Epoch: 175, Loss: 96.8606\n",
      "Epoch: 176, Loss: 96.8587\n",
      "Epoch: 177, Loss: 96.8570\n",
      "Epoch: 178, Loss: 96.8553\n",
      "Epoch: 179, Loss: 96.8538\n",
      "Epoch: 179, Validation Loss: 92.9226, AUC Score: 0.8099\n",
      "Epoch: 180, Loss: 96.8524\n",
      "Epoch: 181, Loss: 96.8511\n",
      "Epoch: 182, Loss: 96.8499\n",
      "Epoch: 183, Loss: 96.8487\n",
      "Epoch: 184, Loss: 96.8477\n",
      "Epoch: 185, Loss: 96.8466\n",
      "Epoch: 186, Loss: 96.8456\n",
      "Epoch: 187, Loss: 96.8447\n",
      "Epoch: 188, Loss: 96.8438\n",
      "Epoch: 189, Loss: 96.8429\n",
      "Epoch: 189, Validation Loss: 92.8773, AUC Score: 0.8099\n",
      "Epoch: 190, Loss: 96.8420\n",
      "Epoch: 191, Loss: 96.8412\n",
      "Epoch: 192, Loss: 96.8404\n",
      "Epoch: 193, Loss: 96.8396\n",
      "Epoch: 194, Loss: 96.8389\n",
      "Epoch: 195, Loss: 96.8381\n",
      "Epoch: 196, Loss: 96.8374\n",
      "Epoch: 197, Loss: 96.8367\n",
      "Epoch: 198, Loss: 96.8359\n",
      "Epoch: 199, Loss: 96.8352\n",
      "Epoch: 199, Validation Loss: 92.8497, AUC Score: 0.8099\n",
      "Epoch: 200, Loss: 96.8345\n",
      "Epoch: 201, Loss: 96.8339\n",
      "Epoch: 202, Loss: 96.8332\n",
      "Epoch: 203, Loss: 96.8325\n",
      "Epoch: 204, Loss: 96.8318\n",
      "Epoch: 205, Loss: 96.8312\n",
      "Epoch: 206, Loss: 96.8305\n",
      "Epoch: 207, Loss: 96.8299\n",
      "Epoch: 208, Loss: 96.8292\n",
      "Epoch: 209, Loss: 96.8286\n",
      "Epoch: 209, Validation Loss: 92.8322, AUC Score: 0.8100\n",
      "Epoch: 210, Loss: 96.8280\n",
      "Epoch: 211, Loss: 96.8273\n",
      "Epoch: 212, Loss: 96.8267\n",
      "Epoch: 213, Loss: 96.8261\n",
      "Epoch: 214, Loss: 96.8255\n",
      "Epoch: 215, Loss: 96.8249\n",
      "Epoch: 216, Loss: 96.8243\n",
      "Epoch: 217, Loss: 96.8237\n",
      "Epoch: 218, Loss: 96.8232\n",
      "Epoch: 219, Loss: 96.8226\n",
      "Epoch: 219, Validation Loss: 92.8200, AUC Score: 0.8101\n",
      "Epoch: 220, Loss: 96.8220\n",
      "Epoch: 221, Loss: 96.8215\n",
      "Epoch: 222, Loss: 96.8209\n",
      "Epoch: 223, Loss: 96.8204\n",
      "Epoch: 224, Loss: 96.8198\n",
      "Epoch: 225, Loss: 96.8193\n",
      "Epoch: 226, Loss: 96.8187\n",
      "Epoch: 227, Loss: 96.8182\n",
      "Epoch: 228, Loss: 96.8177\n",
      "Epoch: 229, Loss: 96.8172\n",
      "Epoch: 229, Validation Loss: 92.8105, AUC Score: 0.8102\n",
      "Epoch: 230, Loss: 96.8167\n",
      "Epoch: 231, Loss: 96.8162\n",
      "Epoch: 232, Loss: 96.8157\n",
      "Epoch: 233, Loss: 96.8152\n",
      "Epoch: 234, Loss: 96.8147\n",
      "Epoch: 235, Loss: 96.8142\n",
      "Epoch: 236, Loss: 96.8138\n",
      "Epoch: 237, Loss: 96.8133\n",
      "Epoch: 238, Loss: 96.8128\n",
      "Epoch: 239, Loss: 96.8124\n",
      "Epoch: 239, Validation Loss: 92.8016, AUC Score: 0.8106\n",
      "Epoch: 240, Loss: 96.8119\n",
      "Epoch: 241, Loss: 96.8115\n",
      "Epoch: 242, Loss: 96.8110\n",
      "Epoch: 243, Loss: 96.8106\n",
      "Epoch: 244, Loss: 96.8102\n",
      "Epoch: 245, Loss: 96.8097\n",
      "Epoch: 246, Loss: 96.8093\n",
      "Epoch: 247, Loss: 96.8089\n",
      "Epoch: 248, Loss: 96.8084\n",
      "Epoch: 249, Loss: 96.8080\n",
      "Epoch: 249, Validation Loss: 92.7924, AUC Score: 0.8106\n",
      "Epoch: 250, Loss: 96.8076\n",
      "Epoch: 251, Loss: 96.8072\n",
      "Epoch: 252, Loss: 96.8068\n",
      "Epoch: 253, Loss: 96.8064\n",
      "Epoch: 254, Loss: 96.8060\n",
      "Epoch: 255, Loss: 96.8056\n",
      "Epoch: 256, Loss: 96.8052\n",
      "Epoch: 257, Loss: 96.8048\n",
      "Epoch: 258, Loss: 96.8044\n",
      "Epoch: 259, Loss: 96.8040\n",
      "Epoch: 259, Validation Loss: 92.7828, AUC Score: 0.8106\n",
      "Epoch: 260, Loss: 96.8036\n",
      "Epoch: 261, Loss: 96.8032\n",
      "Epoch: 262, Loss: 96.8029\n",
      "Epoch: 263, Loss: 96.8025\n",
      "Epoch: 264, Loss: 96.8021\n",
      "Epoch: 265, Loss: 96.8018\n",
      "Epoch: 266, Loss: 96.8014\n",
      "Epoch: 267, Loss: 96.8010\n",
      "Epoch: 268, Loss: 96.8007\n",
      "Epoch: 269, Loss: 96.8003\n",
      "Epoch: 269, Validation Loss: 92.7731, AUC Score: 0.8108\n",
      "Epoch: 270, Loss: 96.8000\n",
      "Epoch: 271, Loss: 96.7996\n",
      "Epoch: 272, Loss: 96.7992\n",
      "Epoch: 273, Loss: 96.7989\n",
      "Epoch: 274, Loss: 96.7986\n",
      "Epoch: 275, Loss: 96.7982\n",
      "Epoch: 276, Loss: 96.7979\n",
      "Epoch: 277, Loss: 96.7975\n",
      "Epoch: 278, Loss: 96.7972\n",
      "Epoch: 279, Loss: 96.7969\n",
      "Epoch: 279, Validation Loss: 92.7638, AUC Score: 0.8108\n",
      "Epoch: 280, Loss: 96.7965\n",
      "Epoch: 281, Loss: 96.7962\n",
      "Epoch: 282, Loss: 96.7959\n",
      "Epoch: 283, Loss: 96.7956\n",
      "Epoch: 284, Loss: 96.7953\n",
      "Epoch: 285, Loss: 96.7949\n",
      "Epoch: 286, Loss: 96.7946\n",
      "Epoch: 287, Loss: 96.7943\n",
      "Epoch: 288, Loss: 96.7940\n",
      "Epoch: 289, Loss: 96.7937\n",
      "Epoch: 289, Validation Loss: 92.7551, AUC Score: 0.8109\n",
      "Epoch: 290, Loss: 96.7934\n",
      "Epoch: 291, Loss: 96.7931\n",
      "Epoch: 292, Loss: 96.7928\n",
      "Epoch: 293, Loss: 96.7925\n",
      "Epoch: 294, Loss: 96.7922\n",
      "Epoch: 295, Loss: 96.7919\n",
      "Epoch: 296, Loss: 96.7916\n",
      "Epoch: 297, Loss: 96.7913\n",
      "Epoch: 298, Loss: 96.7910\n",
      "Epoch: 299, Loss: 96.7907\n",
      "Epoch: 299, Validation Loss: 92.7471, AUC Score: 0.8109\n",
      "Epoch: 300, Loss: 96.7905\n",
      "Epoch: 301, Loss: 96.7902\n",
      "Epoch: 302, Loss: 96.7899\n",
      "Epoch: 303, Loss: 96.7896\n",
      "Epoch: 304, Loss: 96.7894\n",
      "Epoch: 305, Loss: 96.7891\n",
      "Epoch: 306, Loss: 96.7888\n",
      "Epoch: 307, Loss: 96.7885\n",
      "Epoch: 308, Loss: 96.7883\n",
      "Epoch: 309, Loss: 96.7880\n",
      "Epoch: 309, Validation Loss: 92.7398, AUC Score: 0.8109\n",
      "Epoch: 310, Loss: 96.7878\n",
      "Epoch: 311, Loss: 96.7875\n",
      "Epoch: 312, Loss: 96.7872\n",
      "Epoch: 313, Loss: 96.7870\n",
      "Epoch: 314, Loss: 96.7867\n",
      "Epoch: 315, Loss: 96.7865\n",
      "Epoch: 316, Loss: 96.7862\n",
      "Epoch: 317, Loss: 96.7860\n",
      "Epoch: 318, Loss: 96.7857\n",
      "Epoch: 319, Loss: 96.7855\n",
      "Epoch: 319, Validation Loss: 92.7328, AUC Score: 0.8112\n",
      "Epoch: 320, Loss: 96.7853\n",
      "Epoch: 321, Loss: 96.7850\n",
      "Epoch: 322, Loss: 96.7848\n",
      "Epoch: 323, Loss: 96.7845\n",
      "Epoch: 324, Loss: 96.7843\n",
      "Epoch: 325, Loss: 96.7841\n",
      "Epoch: 326, Loss: 96.7838\n",
      "Epoch: 327, Loss: 96.7836\n",
      "Epoch: 328, Loss: 96.7834\n",
      "Epoch: 329, Loss: 96.7832\n",
      "Epoch: 329, Validation Loss: 92.7263, AUC Score: 0.8115\n",
      "Epoch: 330, Loss: 96.7829\n",
      "Epoch: 331, Loss: 96.7827\n",
      "Epoch: 332, Loss: 96.7825\n",
      "Epoch: 333, Loss: 96.7823\n",
      "Epoch: 334, Loss: 96.7821\n",
      "Epoch: 335, Loss: 96.7819\n",
      "Epoch: 336, Loss: 96.7816\n",
      "Epoch: 337, Loss: 96.7814\n",
      "Epoch: 338, Loss: 96.7812\n",
      "Epoch: 339, Loss: 96.7810\n",
      "Epoch: 339, Validation Loss: 92.7201, AUC Score: 0.8115\n",
      "Epoch: 340, Loss: 96.7808\n",
      "Epoch: 341, Loss: 96.7806\n",
      "Epoch: 342, Loss: 96.7804\n",
      "Epoch: 343, Loss: 96.7802\n",
      "Epoch: 344, Loss: 96.7800\n",
      "Epoch: 345, Loss: 96.7798\n",
      "Epoch: 346, Loss: 96.7796\n",
      "Epoch: 347, Loss: 96.7794\n",
      "Epoch: 348, Loss: 96.7792\n",
      "Epoch: 349, Loss: 96.7790\n",
      "Epoch: 349, Validation Loss: 92.7143, AUC Score: 0.8115\n",
      "Epoch: 350, Loss: 96.7788\n",
      "Epoch: 351, Loss: 96.7787\n",
      "Epoch: 352, Loss: 96.7785\n",
      "Epoch: 353, Loss: 96.7783\n",
      "Epoch: 354, Loss: 96.7781\n",
      "Epoch: 355, Loss: 96.7779\n",
      "Epoch: 356, Loss: 96.7778\n",
      "Epoch: 357, Loss: 96.7776\n",
      "Epoch: 358, Loss: 96.7774\n",
      "Epoch: 359, Loss: 96.7772\n",
      "Epoch: 359, Validation Loss: 92.7089, AUC Score: 0.8117\n",
      "Epoch: 360, Loss: 96.7771\n",
      "Epoch: 361, Loss: 96.7769\n",
      "Epoch: 362, Loss: 96.7767\n",
      "Epoch: 363, Loss: 96.7765\n",
      "Epoch: 364, Loss: 96.7764\n",
      "Epoch: 365, Loss: 96.7762\n",
      "Epoch: 366, Loss: 96.7761\n",
      "Epoch: 367, Loss: 96.7759\n",
      "Epoch: 368, Loss: 96.7757\n",
      "Epoch: 369, Loss: 96.7756\n",
      "Epoch: 369, Validation Loss: 92.7039, AUC Score: 0.8119\n",
      "Epoch: 370, Loss: 96.7754\n",
      "Epoch: 371, Loss: 96.7753\n",
      "Epoch: 372, Loss: 96.7751\n",
      "Epoch: 373, Loss: 96.7750\n",
      "Epoch: 374, Loss: 96.7748\n",
      "Epoch: 375, Loss: 96.7747\n",
      "Epoch: 376, Loss: 96.7745\n",
      "Epoch: 377, Loss: 96.7744\n",
      "Epoch: 378, Loss: 96.7742\n",
      "Epoch: 379, Loss: 96.7741\n",
      "Epoch: 379, Validation Loss: 92.6992, AUC Score: 0.8119\n",
      "Epoch: 380, Loss: 96.7739\n",
      "Epoch: 381, Loss: 96.7738\n",
      "Epoch: 382, Loss: 96.7737\n",
      "Epoch: 383, Loss: 96.7735\n",
      "Epoch: 384, Loss: 96.7734\n",
      "Epoch: 385, Loss: 96.7732\n",
      "Epoch: 386, Loss: 96.7731\n",
      "Epoch: 387, Loss: 96.7730\n",
      "Epoch: 388, Loss: 96.7728\n",
      "Epoch: 389, Loss: 96.7727\n",
      "Epoch: 389, Validation Loss: 92.6949, AUC Score: 0.8119\n",
      "Epoch: 390, Loss: 96.7726\n",
      "Epoch: 391, Loss: 96.7725\n",
      "Epoch: 392, Loss: 96.7723\n",
      "Epoch: 393, Loss: 96.7722\n",
      "Epoch: 394, Loss: 96.7721\n",
      "Epoch: 395, Loss: 96.7720\n",
      "Epoch: 396, Loss: 96.7718\n",
      "Epoch: 397, Loss: 96.7717\n",
      "Epoch: 398, Loss: 96.7716\n",
      "Epoch: 399, Loss: 96.7715\n",
      "Epoch: 399, Validation Loss: 92.6910, AUC Score: 0.8120\n",
      "Epoch: 400, Loss: 96.7714\n",
      "Epoch: 401, Loss: 96.7713\n",
      "Epoch: 402, Loss: 96.7711\n",
      "Epoch: 403, Loss: 96.7710\n",
      "Epoch: 404, Loss: 96.7709\n",
      "Epoch: 405, Loss: 96.7708\n",
      "Epoch: 406, Loss: 96.7707\n",
      "Epoch: 407, Loss: 96.7706\n",
      "Epoch: 408, Loss: 96.7705\n",
      "Epoch: 409, Loss: 96.7704\n",
      "Epoch: 409, Validation Loss: 92.6874, AUC Score: 0.8120\n",
      "Epoch: 410, Loss: 96.7703\n",
      "Epoch: 411, Loss: 96.7702\n",
      "Epoch: 412, Loss: 96.7701\n",
      "Epoch: 413, Loss: 96.7700\n",
      "Epoch: 414, Loss: 96.7699\n",
      "Epoch: 415, Loss: 96.7698\n",
      "Epoch: 416, Loss: 96.7697\n",
      "Epoch: 417, Loss: 96.7696\n",
      "Epoch: 418, Loss: 96.7695\n",
      "Epoch: 419, Loss: 96.7694\n",
      "Epoch: 419, Validation Loss: 92.6841, AUC Score: 0.8120\n",
      "Epoch: 420, Loss: 96.7693\n",
      "Epoch: 421, Loss: 96.7692\n",
      "Epoch: 422, Loss: 96.7691\n",
      "Epoch: 423, Loss: 96.7690\n",
      "Epoch: 424, Loss: 96.7690\n",
      "Epoch: 425, Loss: 96.7689\n",
      "Epoch: 426, Loss: 96.7688\n",
      "Epoch: 427, Loss: 96.7687\n",
      "Epoch: 428, Loss: 96.7686\n",
      "Epoch: 429, Loss: 96.7685\n",
      "Epoch: 429, Validation Loss: 92.6812, AUC Score: 0.8120\n",
      "Epoch: 430, Loss: 96.7684\n",
      "Epoch: 431, Loss: 96.7684\n",
      "Epoch: 432, Loss: 96.7683\n",
      "Epoch: 433, Loss: 96.7682\n",
      "Epoch: 434, Loss: 96.7681\n",
      "Epoch: 435, Loss: 96.7681\n",
      "Epoch: 436, Loss: 96.7680\n",
      "Epoch: 437, Loss: 96.7679\n",
      "Epoch: 438, Loss: 96.7678\n",
      "Epoch: 439, Loss: 96.7678\n",
      "Epoch: 439, Validation Loss: 92.6786, AUC Score: 0.8120\n",
      "Epoch: 440, Loss: 96.7677\n",
      "Epoch: 441, Loss: 96.7676\n",
      "Epoch: 442, Loss: 96.7675\n",
      "Epoch: 443, Loss: 96.7675\n",
      "Epoch: 444, Loss: 96.7674\n",
      "Epoch: 445, Loss: 96.7673\n",
      "Epoch: 446, Loss: 96.7673\n",
      "Epoch: 447, Loss: 96.7672\n",
      "Epoch: 448, Loss: 96.7671\n",
      "Epoch: 449, Loss: 96.7671\n",
      "Epoch: 449, Validation Loss: 92.6762, AUC Score: 0.8120\n",
      "Epoch: 450, Loss: 96.7670\n",
      "Epoch: 451, Loss: 96.7669\n",
      "Epoch: 452, Loss: 96.7669\n",
      "Epoch: 453, Loss: 96.7668\n",
      "Epoch: 454, Loss: 96.7668\n",
      "Epoch: 455, Loss: 96.7667\n",
      "Epoch: 456, Loss: 96.7666\n",
      "Epoch: 457, Loss: 96.7666\n",
      "Epoch: 458, Loss: 96.7665\n",
      "Epoch: 459, Loss: 96.7665\n",
      "Epoch: 459, Validation Loss: 92.6742, AUC Score: 0.8121\n",
      "Epoch: 460, Loss: 96.7664\n",
      "Epoch: 461, Loss: 96.7663\n",
      "Epoch: 462, Loss: 96.7663\n",
      "Epoch: 463, Loss: 96.7662\n",
      "Epoch: 464, Loss: 96.7662\n",
      "Epoch: 465, Loss: 96.7661\n",
      "Epoch: 466, Loss: 96.7661\n",
      "Epoch: 467, Loss: 96.7660\n",
      "Epoch: 468, Loss: 96.7660\n",
      "Epoch: 469, Loss: 96.7659\n",
      "Epoch: 469, Validation Loss: 92.6724, AUC Score: 0.8121\n",
      "Epoch: 470, Loss: 96.7659\n",
      "Epoch: 471, Loss: 96.7658\n",
      "Epoch: 472, Loss: 96.7658\n",
      "Epoch: 473, Loss: 96.7657\n",
      "Epoch: 474, Loss: 96.7657\n",
      "Epoch: 475, Loss: 96.7656\n",
      "Epoch: 476, Loss: 96.7656\n",
      "Epoch: 477, Loss: 96.7656\n",
      "Epoch: 478, Loss: 96.7655\n",
      "Epoch: 479, Loss: 96.7655\n",
      "Epoch: 479, Validation Loss: 92.6708, AUC Score: 0.8121\n",
      "Epoch: 480, Loss: 96.7654\n",
      "Epoch: 481, Loss: 96.7654\n",
      "Epoch: 482, Loss: 96.7653\n",
      "Epoch: 483, Loss: 96.7653\n",
      "Epoch: 484, Loss: 96.7653\n",
      "Epoch: 485, Loss: 96.7652\n",
      "Epoch: 486, Loss: 96.7652\n",
      "Epoch: 487, Loss: 96.7651\n",
      "Epoch: 488, Loss: 96.7651\n",
      "Epoch: 489, Loss: 96.7651\n",
      "Epoch: 489, Validation Loss: 92.6695, AUC Score: 0.8123\n",
      "Epoch: 490, Loss: 96.7650\n",
      "Epoch: 491, Loss: 96.7650\n",
      "Epoch: 492, Loss: 96.7650\n",
      "Epoch: 493, Loss: 96.7649\n",
      "Epoch: 494, Loss: 96.7649\n",
      "Epoch: 495, Loss: 96.7649\n",
      "Epoch: 496, Loss: 96.7648\n",
      "Epoch: 497, Loss: 96.7648\n",
      "Epoch: 498, Loss: 96.7647\n",
      "Epoch: 499, Loss: 96.7647\n",
      "Epoch: 499, Validation Loss: 92.6684, AUC Score: 0.8124\n",
      "Epoch: 500, Loss: 96.7647\n",
      "Epoch: 501, Loss: 96.7647\n",
      "Epoch: 502, Loss: 96.7646\n",
      "Epoch: 503, Loss: 96.7646\n",
      "Epoch: 504, Loss: 96.7646\n",
      "Epoch: 505, Loss: 96.7645\n",
      "Epoch: 506, Loss: 96.7645\n",
      "Epoch: 507, Loss: 96.7645\n",
      "Epoch: 508, Loss: 96.7644\n",
      "Epoch: 509, Loss: 96.7644\n",
      "Epoch: 509, Validation Loss: 92.6675, AUC Score: 0.8124\n",
      "Epoch: 510, Loss: 96.7644\n",
      "Epoch: 511, Loss: 96.7644\n",
      "Epoch: 512, Loss: 96.7643\n",
      "Epoch: 513, Loss: 96.7643\n",
      "Epoch: 514, Loss: 96.7643\n",
      "Epoch: 515, Loss: 96.7643\n",
      "Epoch: 516, Loss: 96.7642\n",
      "Epoch: 517, Loss: 96.7642\n",
      "Epoch: 518, Loss: 96.7642\n",
      "Epoch: 519, Loss: 96.7642\n",
      "Epoch: 519, Validation Loss: 92.6668, AUC Score: 0.8124\n",
      "Epoch: 520, Loss: 96.7641\n",
      "Epoch: 521, Loss: 96.7641\n",
      "Epoch: 522, Loss: 96.7641\n",
      "Epoch: 523, Loss: 96.7641\n",
      "Epoch: 524, Loss: 96.7641\n",
      "Epoch: 525, Loss: 96.7640\n",
      "Epoch: 526, Loss: 96.7640\n",
      "Epoch: 527, Loss: 96.7640\n",
      "Epoch: 528, Loss: 96.7640\n",
      "Epoch: 529, Loss: 96.7639\n",
      "Epoch: 529, Validation Loss: 92.6662, AUC Score: 0.8124\n",
      "Epoch: 530, Loss: 96.7639\n",
      "Epoch: 531, Loss: 96.7639\n",
      "Epoch: 532, Loss: 96.7639\n",
      "Epoch: 533, Loss: 96.7639\n",
      "Epoch: 534, Loss: 96.7639\n",
      "Epoch: 535, Loss: 96.7638\n",
      "Epoch: 536, Loss: 96.7638\n",
      "Epoch: 537, Loss: 96.7638\n",
      "Epoch: 538, Loss: 96.7638\n",
      "Epoch: 539, Loss: 96.7638\n",
      "Epoch: 539, Validation Loss: 92.6658, AUC Score: 0.8124\n",
      "Epoch: 540, Loss: 96.7638\n",
      "Epoch: 541, Loss: 96.7637\n",
      "Epoch: 542, Loss: 96.7637\n",
      "Epoch: 543, Loss: 96.7637\n",
      "Epoch: 544, Loss: 96.7637\n",
      "Epoch: 545, Loss: 96.7637\n",
      "Epoch: 546, Loss: 96.7637\n",
      "Epoch: 547, Loss: 96.7636\n",
      "Epoch: 548, Loss: 96.7636\n",
      "Epoch: 549, Loss: 96.7636\n",
      "Epoch: 549, Validation Loss: 92.6655, AUC Score: 0.8124\n",
      "Epoch: 550, Loss: 96.7636\n",
      "Epoch: 551, Loss: 96.7636\n",
      "Epoch: 552, Loss: 96.7636\n",
      "Epoch: 553, Loss: 96.7636\n",
      "Epoch: 554, Loss: 96.7635\n",
      "Epoch: 555, Loss: 96.7635\n",
      "Epoch: 556, Loss: 96.7635\n",
      "Epoch: 557, Loss: 96.7635\n",
      "Epoch: 558, Loss: 96.7635\n",
      "Epoch: 559, Loss: 96.7635\n",
      "Epoch: 559, Validation Loss: 92.6653, AUC Score: 0.8125\n",
      "Epoch: 560, Loss: 96.7635\n",
      "Epoch: 561, Loss: 96.7635\n",
      "Epoch: 562, Loss: 96.7635\n",
      "Epoch: 563, Loss: 96.7634\n",
      "Epoch: 564, Loss: 96.7634\n",
      "Epoch: 565, Loss: 96.7634\n",
      "Epoch: 566, Loss: 96.7634\n",
      "Epoch: 567, Loss: 96.7634\n",
      "Epoch: 568, Loss: 96.7634\n",
      "Epoch: 569, Loss: 96.7634\n",
      "Epoch: 569, Validation Loss: 92.6652, AUC Score: 0.8125\n",
      "Epoch: 570, Loss: 96.7634\n",
      "Epoch: 571, Loss: 96.7634\n",
      "Epoch: 572, Loss: 96.7634\n",
      "Epoch: 573, Loss: 96.7634\n",
      "Epoch: 574, Loss: 96.7633\n",
      "Epoch: 575, Loss: 96.7633\n",
      "Epoch: 576, Loss: 96.7633\n",
      "Epoch: 577, Loss: 96.7633\n",
      "Epoch: 578, Loss: 96.7633\n",
      "Epoch: 579, Loss: 96.7633\n",
      "Epoch: 579, Validation Loss: 92.6652, AUC Score: 0.8125\n",
      "Epoch: 580, Loss: 96.7633\n",
      "Epoch: 581, Loss: 96.7633\n",
      "Epoch: 582, Loss: 96.7633\n",
      "Epoch: 583, Loss: 96.7633\n",
      "Epoch: 584, Loss: 96.7633\n",
      "Epoch: 585, Loss: 96.7633\n",
      "Epoch: 586, Loss: 96.7633\n",
      "Epoch: 587, Loss: 96.7633\n",
      "Epoch: 588, Loss: 96.7632\n",
      "Epoch: 589, Loss: 96.7632\n",
      "Epoch: 589, Validation Loss: 92.6652, AUC Score: 0.8125\n",
      "Epoch: 590, Loss: 96.7632\n",
      "Epoch: 591, Loss: 96.7632\n",
      "Epoch: 592, Loss: 96.7632\n",
      "Epoch: 593, Loss: 96.7632\n",
      "Epoch: 594, Loss: 96.7632\n",
      "Epoch: 595, Loss: 96.7632\n",
      "Epoch: 596, Loss: 96.7632\n",
      "Epoch: 597, Loss: 96.7632\n",
      "Epoch: 598, Loss: 96.7632\n",
      "Epoch: 599, Loss: 96.7632\n",
      "Epoch: 599, Validation Loss: 92.6653, AUC Score: 0.8125\n",
      "Epoch: 600, Loss: 96.7632\n",
      "Epoch: 601, Loss: 96.7632\n",
      "Epoch: 602, Loss: 96.7632\n",
      "Epoch: 603, Loss: 96.7632\n",
      "Epoch: 604, Loss: 96.7632\n",
      "Epoch: 605, Loss: 96.7632\n",
      "Epoch: 606, Loss: 96.7632\n",
      "Epoch: 607, Loss: 96.7632\n",
      "Epoch: 608, Loss: 96.7631\n",
      "Epoch: 609, Loss: 96.7631\n",
      "Epoch: 609, Validation Loss: 92.6654, AUC Score: 0.8125\n",
      "Epoch: 610, Loss: 96.7631\n",
      "Epoch: 611, Loss: 96.7631\n",
      "Epoch: 612, Loss: 96.7631\n",
      "Epoch: 613, Loss: 96.7631\n",
      "Epoch: 614, Loss: 96.7631\n",
      "Epoch: 615, Loss: 96.7631\n",
      "Epoch: 616, Loss: 96.7631\n",
      "Epoch: 617, Loss: 96.7631\n",
      "Epoch: 618, Loss: 96.7631\n",
      "Epoch: 619, Loss: 96.7631\n",
      "Epoch: 619, Validation Loss: 92.6656, AUC Score: 0.8125\n",
      "Epoch: 620, Loss: 96.7631\n",
      "Epoch: 621, Loss: 96.7631\n",
      "Epoch: 622, Loss: 96.7631\n",
      "Epoch: 623, Loss: 96.7631\n",
      "Epoch: 624, Loss: 96.7631\n",
      "Epoch: 625, Loss: 96.7631\n",
      "Epoch: 626, Loss: 96.7631\n",
      "Epoch: 627, Loss: 96.7631\n",
      "Epoch: 628, Loss: 96.7631\n",
      "Epoch: 629, Loss: 96.7631\n",
      "Epoch: 629, Validation Loss: 92.6657, AUC Score: 0.8125\n",
      "Epoch: 630, Loss: 96.7631\n",
      "Epoch: 631, Loss: 96.7631\n",
      "Epoch: 632, Loss: 96.7631\n",
      "Epoch: 633, Loss: 96.7631\n",
      "Epoch: 634, Loss: 96.7631\n",
      "Epoch: 635, Loss: 96.7631\n",
      "Epoch: 636, Loss: 96.7631\n",
      "Epoch: 637, Loss: 96.7631\n",
      "Epoch: 638, Loss: 96.7631\n",
      "Epoch: 639, Loss: 96.7631\n",
      "Epoch: 639, Validation Loss: 92.6659, AUC Score: 0.8125\n",
      "Epoch: 640, Loss: 96.7631\n",
      "Epoch: 641, Loss: 96.7631\n",
      "Epoch: 642, Loss: 96.7631\n",
      "Epoch: 643, Loss: 96.7631\n",
      "Epoch: 644, Loss: 96.7631\n",
      "Epoch: 645, Loss: 96.7631\n",
      "Epoch: 646, Loss: 96.7631\n",
      "Epoch: 647, Loss: 96.7631\n",
      "Epoch: 648, Loss: 96.7631\n",
      "Epoch: 649, Loss: 96.7631\n",
      "Epoch: 649, Validation Loss: 92.6660, AUC Score: 0.8125\n",
      "Epoch: 650, Loss: 96.7631\n",
      "Epoch: 651, Loss: 96.7631\n",
      "Epoch: 652, Loss: 96.7631\n",
      "Epoch: 653, Loss: 96.7631\n",
      "Epoch: 654, Loss: 96.7631\n",
      "Epoch: 655, Loss: 96.7631\n",
      "Epoch: 656, Loss: 96.7631\n",
      "Epoch: 657, Loss: 96.7631\n",
      "Epoch: 658, Loss: 96.7631\n",
      "Epoch: 659, Loss: 96.7631\n",
      "Epoch: 659, Validation Loss: 92.6662, AUC Score: 0.8126\n",
      "Epoch: 660, Loss: 96.7631\n",
      "Epoch: 661, Loss: 96.7631\n",
      "Epoch: 662, Loss: 96.7631\n",
      "Epoch: 663, Loss: 96.7631\n",
      "Epoch: 664, Loss: 96.7631\n",
      "Epoch: 665, Loss: 96.7631\n",
      "Epoch: 666, Loss: 96.7630\n",
      "Epoch: 667, Loss: 96.7630\n",
      "Epoch: 668, Loss: 96.7630\n",
      "Epoch: 669, Loss: 96.7630\n",
      "Epoch: 669, Validation Loss: 92.6663, AUC Score: 0.8127\n",
      "Epoch: 670, Loss: 96.7630\n",
      "Epoch: 671, Loss: 96.7630\n",
      "Epoch: 672, Loss: 96.7630\n",
      "Epoch: 673, Loss: 96.7630\n",
      "Epoch: 674, Loss: 96.7630\n",
      "Epoch: 675, Loss: 96.7630\n",
      "Epoch: 676, Loss: 96.7630\n",
      "Epoch: 677, Loss: 96.7630\n",
      "Epoch: 678, Loss: 96.7630\n",
      "Epoch: 679, Loss: 96.7630\n",
      "Epoch: 679, Validation Loss: 92.6664, AUC Score: 0.8127\n",
      "Epoch: 680, Loss: 96.7630\n",
      "Epoch: 681, Loss: 96.7630\n",
      "Epoch: 682, Loss: 96.7630\n",
      "Epoch: 683, Loss: 96.7630\n",
      "Epoch: 684, Loss: 96.7630\n",
      "Epoch: 685, Loss: 96.7630\n",
      "Epoch: 686, Loss: 96.7630\n",
      "Epoch: 687, Loss: 96.7630\n",
      "Epoch: 688, Loss: 96.7630\n",
      "Epoch: 689, Loss: 96.7630\n",
      "Epoch: 689, Validation Loss: 92.6665, AUC Score: 0.8127\n",
      "Epoch: 690, Loss: 96.7630\n",
      "Epoch: 691, Loss: 96.7630\n",
      "Epoch: 692, Loss: 96.7630\n",
      "Epoch: 693, Loss: 96.7630\n",
      "Epoch: 694, Loss: 96.7630\n",
      "Epoch: 695, Loss: 96.7630\n",
      "Epoch: 696, Loss: 96.7630\n",
      "Epoch: 697, Loss: 96.7630\n",
      "Epoch: 698, Loss: 96.7630\n",
      "Epoch: 699, Loss: 96.7630\n",
      "Epoch: 699, Validation Loss: 92.6666, AUC Score: 0.8127\n",
      "Epoch: 700, Loss: 96.7630\n",
      "Epoch: 701, Loss: 96.7630\n",
      "Epoch: 702, Loss: 96.7630\n",
      "Epoch: 703, Loss: 96.7630\n",
      "Epoch: 704, Loss: 96.7630\n",
      "Epoch: 705, Loss: 96.7630\n",
      "Epoch: 706, Loss: 96.7630\n",
      "Epoch: 707, Loss: 96.7630\n",
      "Epoch: 708, Loss: 96.7630\n",
      "Epoch: 709, Loss: 96.7630\n",
      "Epoch: 709, Validation Loss: 92.6667, AUC Score: 0.8127\n",
      "Epoch: 710, Loss: 96.7630\n",
      "Epoch: 711, Loss: 96.7630\n",
      "Epoch: 712, Loss: 96.7630\n",
      "Epoch: 713, Loss: 96.7630\n",
      "Epoch: 714, Loss: 96.7630\n",
      "Epoch: 715, Loss: 96.7630\n",
      "Epoch: 716, Loss: 96.7630\n",
      "Epoch: 717, Loss: 96.7630\n",
      "Epoch: 718, Loss: 96.7630\n",
      "Epoch: 719, Loss: 96.7630\n",
      "Epoch: 719, Validation Loss: 92.6668, AUC Score: 0.8127\n",
      "Epoch: 720, Loss: 96.7630\n",
      "Epoch: 721, Loss: 96.7630\n",
      "Epoch: 722, Loss: 96.7630\n",
      "Epoch: 723, Loss: 96.7630\n",
      "Epoch: 724, Loss: 96.7630\n",
      "Epoch: 725, Loss: 96.7630\n",
      "Epoch: 726, Loss: 96.7630\n",
      "Epoch: 727, Loss: 96.7630\n",
      "Epoch: 728, Loss: 96.7630\n",
      "Epoch: 729, Loss: 96.7630\n",
      "Epoch: 729, Validation Loss: 92.6668, AUC Score: 0.8127\n",
      "Epoch: 730, Loss: 96.7630\n",
      "Epoch: 731, Loss: 96.7630\n",
      "Epoch: 732, Loss: 96.7630\n",
      "Epoch: 733, Loss: 96.7630\n",
      "Epoch: 734, Loss: 96.7630\n",
      "Epoch: 735, Loss: 96.7630\n",
      "Epoch: 736, Loss: 96.7630\n",
      "Epoch: 737, Loss: 96.7630\n",
      "Epoch: 738, Loss: 96.7630\n",
      "Epoch: 739, Loss: 96.7630\n",
      "Epoch: 739, Validation Loss: 92.6669, AUC Score: 0.8127\n",
      "Epoch: 740, Loss: 96.7630\n",
      "Epoch: 741, Loss: 96.7630\n",
      "Epoch: 742, Loss: 96.7630\n",
      "Epoch: 743, Loss: 96.7630\n",
      "Epoch: 744, Loss: 96.7630\n",
      "Epoch: 745, Loss: 96.7630\n",
      "Epoch: 746, Loss: 96.7630\n",
      "Epoch: 747, Loss: 96.7630\n",
      "Epoch: 748, Loss: 96.7630\n",
      "Epoch: 749, Loss: 96.7630\n",
      "Epoch: 749, Validation Loss: 92.6670, AUC Score: 0.8127\n",
      "Epoch: 750, Loss: 96.7630\n",
      "Epoch: 751, Loss: 96.7630\n",
      "Epoch: 752, Loss: 96.7630\n",
      "Epoch: 753, Loss: 96.7630\n",
      "Epoch: 754, Loss: 96.7630\n",
      "Epoch: 755, Loss: 96.7630\n",
      "Epoch: 756, Loss: 96.7630\n",
      "Epoch: 757, Loss: 96.7630\n",
      "Epoch: 758, Loss: 96.7630\n",
      "Epoch: 759, Loss: 96.7630\n",
      "Epoch: 759, Validation Loss: 92.6670, AUC Score: 0.8127\n",
      "Epoch: 760, Loss: 96.7630\n",
      "Epoch: 761, Loss: 96.7630\n",
      "Epoch: 762, Loss: 96.7630\n",
      "Epoch: 763, Loss: 96.7630\n",
      "Epoch: 764, Loss: 96.7630\n",
      "Epoch: 765, Loss: 96.7630\n",
      "Epoch: 766, Loss: 96.7630\n",
      "Epoch: 767, Loss: 96.7630\n",
      "Epoch: 768, Loss: 96.7630\n",
      "Epoch: 769, Loss: 96.7630\n",
      "Epoch: 769, Validation Loss: 92.6670, AUC Score: 0.8127\n",
      "Epoch: 770, Loss: 96.7630\n",
      "Epoch: 771, Loss: 96.7630\n",
      "Epoch: 772, Loss: 96.7630\n",
      "Epoch: 773, Loss: 96.7630\n",
      "Epoch: 774, Loss: 96.7630\n",
      "Epoch: 775, Loss: 96.7630\n",
      "Epoch: 776, Loss: 96.7630\n",
      "Epoch: 777, Loss: 96.7630\n",
      "Epoch: 778, Loss: 96.7630\n",
      "Epoch: 779, Loss: 96.7630\n",
      "Epoch: 779, Validation Loss: 92.6671, AUC Score: 0.8127\n",
      "Epoch: 780, Loss: 96.7630\n",
      "Epoch: 781, Loss: 96.7630\n",
      "Epoch: 782, Loss: 96.7630\n",
      "Epoch: 783, Loss: 96.7630\n",
      "Epoch: 784, Loss: 96.7630\n",
      "Epoch: 785, Loss: 96.7630\n",
      "Epoch: 786, Loss: 96.7630\n",
      "Epoch: 787, Loss: 96.7630\n",
      "Epoch: 788, Loss: 96.7630\n",
      "Epoch: 789, Loss: 96.7630\n",
      "Epoch: 789, Validation Loss: 92.6671, AUC Score: 0.8127\n",
      "Epoch: 790, Loss: 96.7630\n",
      "Epoch: 791, Loss: 96.7630\n",
      "Epoch: 792, Loss: 96.7630\n",
      "Epoch: 793, Loss: 96.7630\n",
      "Epoch: 794, Loss: 96.7630\n",
      "Epoch: 795, Loss: 96.7630\n",
      "Epoch: 796, Loss: 96.7630\n",
      "Epoch: 797, Loss: 96.7630\n",
      "Epoch: 798, Loss: 96.7630\n",
      "Epoch: 799, Loss: 96.7630\n",
      "Epoch: 799, Validation Loss: 92.6672, AUC Score: 0.8127\n",
      "Epoch: 800, Loss: 96.7630\n",
      "Epoch: 801, Loss: 96.7630\n",
      "Epoch: 802, Loss: 96.7630\n",
      "Epoch: 803, Loss: 96.7630\n",
      "Epoch: 804, Loss: 96.7630\n",
      "Epoch: 805, Loss: 96.7630\n",
      "Epoch: 806, Loss: 96.7630\n",
      "Epoch: 807, Loss: 96.7630\n",
      "Epoch: 808, Loss: 96.7630\n",
      "Epoch: 809, Loss: 96.7630\n",
      "Epoch: 809, Validation Loss: 92.6672, AUC Score: 0.8127\n",
      "Epoch: 810, Loss: 96.7630\n",
      "Epoch: 811, Loss: 96.7630\n",
      "Epoch: 812, Loss: 96.7630\n",
      "Epoch: 813, Loss: 96.7630\n",
      "Epoch: 814, Loss: 96.7630\n",
      "Epoch: 815, Loss: 96.7630\n",
      "Epoch: 816, Loss: 96.7630\n",
      "Epoch: 817, Loss: 96.7630\n",
      "Epoch: 818, Loss: 96.7630\n",
      "Epoch: 819, Loss: 96.7630\n",
      "Epoch: 819, Validation Loss: 92.6672, AUC Score: 0.8127\n",
      "Epoch: 820, Loss: 96.7630\n",
      "Epoch: 821, Loss: 96.7630\n",
      "Epoch: 822, Loss: 96.7630\n",
      "Epoch: 823, Loss: 96.7630\n",
      "Epoch: 824, Loss: 96.7630\n",
      "Epoch: 825, Loss: 96.7630\n",
      "Epoch: 826, Loss: 96.7630\n",
      "Epoch: 827, Loss: 96.7630\n",
      "Epoch: 828, Loss: 96.7630\n",
      "Epoch: 829, Loss: 96.7630\n",
      "Epoch: 829, Validation Loss: 92.6673, AUC Score: 0.8127\n",
      "Epoch: 830, Loss: 96.7630\n",
      "Epoch: 831, Loss: 96.7630\n",
      "Epoch: 832, Loss: 96.7630\n",
      "Epoch: 833, Loss: 96.7630\n",
      "Epoch: 834, Loss: 96.7630\n",
      "Epoch: 835, Loss: 96.7630\n",
      "Epoch: 836, Loss: 96.7630\n",
      "Epoch: 837, Loss: 96.7630\n",
      "Epoch: 838, Loss: 96.7630\n",
      "Epoch: 839, Loss: 96.7630\n",
      "Epoch: 839, Validation Loss: 92.6673, AUC Score: 0.8127\n",
      "Epoch: 840, Loss: 96.7630\n",
      "Epoch: 841, Loss: 96.7630\n",
      "Epoch: 842, Loss: 96.7630\n",
      "Epoch: 843, Loss: 96.7630\n",
      "Epoch: 844, Loss: 96.7630\n",
      "Epoch: 845, Loss: 96.7630\n",
      "Epoch: 846, Loss: 96.7630\n",
      "Epoch: 847, Loss: 96.7630\n",
      "Epoch: 848, Loss: 96.7630\n",
      "Epoch: 849, Loss: 96.7630\n",
      "Epoch: 849, Validation Loss: 92.6674, AUC Score: 0.8127\n",
      "Epoch: 850, Loss: 96.7630\n",
      "Epoch: 851, Loss: 96.7630\n",
      "Epoch: 852, Loss: 96.7630\n",
      "Epoch: 853, Loss: 96.7630\n",
      "Epoch: 854, Loss: 96.7630\n",
      "Epoch: 855, Loss: 96.7630\n",
      "Epoch: 856, Loss: 96.7630\n",
      "Epoch: 857, Loss: 96.7630\n",
      "Epoch: 858, Loss: 96.7630\n",
      "Epoch: 859, Loss: 96.7630\n",
      "Epoch: 859, Validation Loss: 92.6674, AUC Score: 0.8127\n",
      "Epoch: 860, Loss: 96.7630\n",
      "Epoch: 861, Loss: 96.7630\n",
      "Epoch: 862, Loss: 96.7630\n",
      "Epoch: 863, Loss: 96.7630\n",
      "Epoch: 864, Loss: 96.7630\n",
      "Epoch: 865, Loss: 96.7630\n",
      "Epoch: 866, Loss: 96.7630\n",
      "Epoch: 867, Loss: 96.7630\n",
      "Epoch: 868, Loss: 96.7630\n",
      "Epoch: 869, Loss: 96.7630\n",
      "Epoch: 869, Validation Loss: 92.6675, AUC Score: 0.8127\n",
      "Epoch: 870, Loss: 96.7630\n",
      "Epoch: 871, Loss: 96.7630\n",
      "Epoch: 872, Loss: 96.7630\n",
      "Epoch: 873, Loss: 96.7630\n",
      "Epoch: 874, Loss: 96.7630\n",
      "Epoch: 875, Loss: 96.7630\n",
      "Epoch: 876, Loss: 96.7630\n",
      "Epoch: 877, Loss: 96.7630\n",
      "Epoch: 878, Loss: 96.7630\n",
      "Epoch: 879, Loss: 96.7630\n",
      "Epoch: 879, Validation Loss: 92.6675, AUC Score: 0.8127\n",
      "Epoch: 880, Loss: 96.7630\n",
      "Epoch: 881, Loss: 96.7630\n",
      "Epoch: 882, Loss: 96.7630\n",
      "Epoch: 883, Loss: 96.7630\n",
      "Epoch: 884, Loss: 96.7630\n",
      "Epoch: 885, Loss: 96.7630\n",
      "Epoch: 886, Loss: 96.7630\n",
      "Epoch: 887, Loss: 96.7630\n",
      "Epoch: 888, Loss: 96.7630\n",
      "Epoch: 889, Loss: 96.7630\n",
      "Epoch: 889, Validation Loss: 92.6675, AUC Score: 0.8127\n",
      "Epoch: 890, Loss: 96.7630\n",
      "Epoch: 891, Loss: 96.7630\n",
      "Epoch: 892, Loss: 96.7630\n",
      "Epoch: 893, Loss: 96.7630\n",
      "Epoch: 894, Loss: 96.7630\n",
      "Epoch: 895, Loss: 96.7630\n",
      "Epoch: 896, Loss: 96.7630\n",
      "Epoch: 897, Loss: 96.7630\n",
      "Epoch: 898, Loss: 96.7630\n",
      "Epoch: 899, Loss: 96.7630\n",
      "Epoch: 899, Validation Loss: 92.6679, AUC Score: 0.8127\n",
      "Epoch: 900, Loss: 96.7630\n",
      "Epoch: 901, Loss: 96.7630\n",
      "Epoch: 902, Loss: 96.7630\n",
      "Epoch: 903, Loss: 96.7631\n",
      "Epoch: 904, Loss: 96.7634\n",
      "Epoch: 905, Loss: 96.7649\n",
      "Epoch: 906, Loss: 96.7671\n",
      "Epoch: 907, Loss: 96.7641\n",
      "Epoch: 908, Loss: 96.7639\n",
      "Epoch: 909, Loss: 96.7654\n",
      "Epoch: 909, Validation Loss: 92.6688, AUC Score: 0.8127\n",
      "Epoch: 910, Loss: 96.7630\n",
      "Epoch: 911, Loss: 96.7650\n",
      "Epoch: 912, Loss: 96.7632\n",
      "Epoch: 913, Loss: 96.7643\n",
      "Epoch: 914, Loss: 96.7635\n",
      "Epoch: 915, Loss: 96.7637\n",
      "Epoch: 916, Loss: 96.7637\n",
      "Epoch: 917, Loss: 96.7634\n",
      "Epoch: 918, Loss: 96.7637\n",
      "Epoch: 919, Loss: 96.7632\n",
      "Epoch: 919, Validation Loss: 92.6848, AUC Score: 0.8127\n",
      "Epoch: 920, Loss: 96.7637\n",
      "Epoch: 921, Loss: 96.7631\n",
      "Epoch: 922, Loss: 96.7636\n",
      "Epoch: 923, Loss: 96.7630\n",
      "Epoch: 924, Loss: 96.7635\n",
      "Epoch: 925, Loss: 96.7630\n",
      "Epoch: 926, Loss: 96.7634\n",
      "Epoch: 927, Loss: 96.7630\n",
      "Epoch: 928, Loss: 96.7633\n",
      "Epoch: 929, Loss: 96.7630\n",
      "Epoch: 929, Validation Loss: 92.6582, AUC Score: 0.8126\n",
      "Epoch: 930, Loss: 96.7632\n",
      "Epoch: 931, Loss: 96.7631\n",
      "Epoch: 932, Loss: 96.7632\n",
      "Epoch: 933, Loss: 96.7631\n",
      "Epoch: 934, Loss: 96.7631\n",
      "Epoch: 935, Loss: 96.7631\n",
      "Epoch: 936, Loss: 96.7631\n",
      "Epoch: 937, Loss: 96.7631\n",
      "Epoch: 938, Loss: 96.7630\n",
      "Epoch: 939, Loss: 96.7631\n",
      "Epoch: 939, Validation Loss: 92.6690, AUC Score: 0.8127\n",
      "Epoch: 940, Loss: 96.7630\n",
      "Epoch: 941, Loss: 96.7631\n",
      "Epoch: 942, Loss: 96.7630\n",
      "Epoch: 943, Loss: 96.7631\n",
      "Epoch: 944, Loss: 96.7630\n",
      "Epoch: 945, Loss: 96.7630\n",
      "Epoch: 946, Loss: 96.7630\n",
      "Epoch: 947, Loss: 96.7630\n",
      "Epoch: 948, Loss: 96.7630\n",
      "Epoch: 949, Loss: 96.7630\n",
      "Epoch: 949, Validation Loss: 92.6714, AUC Score: 0.8127\n",
      "Epoch: 950, Loss: 96.7630\n",
      "Epoch: 951, Loss: 96.7630\n",
      "Epoch: 952, Loss: 96.7630\n",
      "Epoch: 953, Loss: 96.7630\n",
      "Epoch: 954, Loss: 96.7630\n",
      "Epoch: 955, Loss: 96.7630\n",
      "Epoch: 956, Loss: 96.7630\n",
      "Epoch: 957, Loss: 96.7630\n",
      "Epoch: 958, Loss: 96.7630\n",
      "Epoch: 959, Loss: 96.7630\n",
      "Epoch: 959, Validation Loss: 92.6677, AUC Score: 0.8127\n",
      "Epoch: 960, Loss: 96.7630\n",
      "Epoch: 961, Loss: 96.7630\n",
      "Epoch: 962, Loss: 96.7630\n",
      "Epoch: 963, Loss: 96.7630\n",
      "Epoch: 964, Loss: 96.7630\n",
      "Epoch: 965, Loss: 96.7630\n",
      "Epoch: 966, Loss: 96.7630\n",
      "Epoch: 967, Loss: 96.7630\n",
      "Epoch: 968, Loss: 96.7630\n",
      "Epoch: 969, Loss: 96.7630\n",
      "Epoch: 969, Validation Loss: 92.6670, AUC Score: 0.8127\n",
      "Epoch: 970, Loss: 96.7630\n",
      "Epoch: 971, Loss: 96.7630\n",
      "Epoch: 972, Loss: 96.7630\n",
      "Epoch: 973, Loss: 96.7630\n",
      "Epoch: 974, Loss: 96.7630\n",
      "Epoch: 975, Loss: 96.7630\n",
      "Epoch: 976, Loss: 96.7630\n",
      "Epoch: 977, Loss: 96.7630\n",
      "Epoch: 978, Loss: 96.7630\n",
      "Epoch: 979, Loss: 96.7630\n",
      "Epoch: 979, Validation Loss: 92.6674, AUC Score: 0.8127\n",
      "Epoch: 980, Loss: 96.7630\n",
      "Epoch: 981, Loss: 96.7630\n",
      "Epoch: 982, Loss: 96.7630\n",
      "Epoch: 983, Loss: 96.7630\n",
      "Epoch: 984, Loss: 96.7630\n",
      "Epoch: 985, Loss: 96.7630\n",
      "Epoch: 986, Loss: 96.7630\n",
      "Epoch: 987, Loss: 96.7630\n",
      "Epoch: 988, Loss: 96.7630\n",
      "Epoch: 989, Loss: 96.7630\n",
      "Epoch: 989, Validation Loss: 92.6680, AUC Score: 0.8127\n",
      "Epoch: 990, Loss: 96.7630\n",
      "Epoch: 991, Loss: 96.7630\n",
      "Epoch: 992, Loss: 96.7630\n",
      "Epoch: 993, Loss: 96.7630\n",
      "Epoch: 994, Loss: 96.7630\n",
      "Epoch: 995, Loss: 96.7630\n",
      "Epoch: 996, Loss: 96.7630\n",
      "Epoch: 997, Loss: 96.7630\n",
      "Epoch: 998, Loss: 96.7630\n",
      "Epoch: 999, Loss: 96.7630\n",
      "Epoch: 999, Validation Loss: 92.6682, AUC Score: 0.8127\n",
      "Best Score: 0.8127\n"
     ]
    }
   ],
   "source": [
    "# number of features and layers\n",
    "n_features = 2\n",
    "n_layers = 3\n",
    "\n",
    "# Random weight initialization\n",
    "weights = 0.01 * np.random.randn(n_layers, n_features, 3, requires_grad=True)\n",
    "\n",
    "# We create a quantum device with n_features \"wires\" (or qubits) \n",
    "dev =qml.device('default.qubit.autograd', wires=n_features) \n",
    " \n",
    "# train the model\n",
    "best_score, best_weights = train (n_features, n_layers,x_train,y_train_arr,0.01, weights, 1000, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8127164212937147\n"
     ]
    }
   ],
   "source": [
    "print (best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7030216574917096\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "\n",
    "def test(n_features, n_layers,x_test,y_test,w_test, weights):\n",
    "        # Remove grad\n",
    "        X_test = np.array(x_test, requires_grad=False)\n",
    "        Y_test = np.array(y_test, requires_grad=False)\n",
    "        W_test = np.array(w_test, requires_grad=False)\n",
    "\n",
    "        # This will be between -1 and 1, we need to convert to between 0 and 1\n",
    "        y_scores = np.array([classifier(n_features, n_layers,weights, x) for x in X_test])\n",
    "        y_scores = (y_scores + 1) / 2\n",
    "\n",
    "        # Renormalize weights\n",
    "        W_test[Y_test == 1] = (W_test[Y_test == 1] / W_test[Y_test == 1].sum()) * W_test.shape[0] / 2\n",
    "        W_test[Y_test == 0] = (W_test[Y_test == 0] / W_test[Y_test == 0].sum()) * W_test.shape[0] / 2\n",
    "\n",
    "        # Calculate ROC\n",
    "        auc_score = roc_auc_score(y_true=Y_test, y_score=y_scores, sample_weight=W_test)\n",
    "        \n",
    "        return auc_score\n",
    "    \n",
    "auc_score = test(n_features, n_layers,x_test,y_test,w_test, best_weights)\n",
    "print (auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### qiskit.ibmq Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dd6dfccd9b94159af8ab77d73974874",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1000 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 241.2400\n",
      "Epoch: 1, Loss: 241.0099\n",
      "Epoch: 2, Loss: 240.5597\n",
      "Epoch: 3, Loss: 239.8305\n",
      "Epoch: 4, Loss: 238.8123\n",
      "Epoch: 5, Loss: 237.4988\n",
      "Epoch: 6, Loss: 235.8874\n",
      "Epoch: 7, Loss: 233.9794\n",
      "Epoch: 8, Loss: 231.7807\n",
      "Epoch: 9, Loss: 229.3020\n",
      "Epoch: 9, Validation Loss: 232.6091, AUC Score: 0.1642\n",
      "Epoch: 10, Loss: 226.5588\n",
      "Epoch: 11, Loss: 223.5714\n",
      "Epoch: 12, Loss: 220.3651\n",
      "Epoch: 13, Loss: 216.9686\n",
      "Epoch: 14, Loss: 213.4123\n",
      "Epoch: 15, Loss: 209.7246\n",
      "Epoch: 16, Loss: 205.9293\n",
      "Epoch: 17, Loss: 202.0455\n",
      "Epoch: 18, Loss: 198.0899\n",
      "Epoch: 19, Loss: 194.0783\n",
      "Epoch: 19, Validation Loss: 195.4333, AUC Score: 0.1642\n",
      "Epoch: 20, Loss: 190.0276\n",
      "Epoch: 21, Loss: 185.9571\n",
      "Epoch: 22, Loss: 181.8898\n",
      "Epoch: 23, Loss: 177.8518\n",
      "Epoch: 24, Loss: 173.8700\n",
      "Epoch: 25, Loss: 169.9698\n",
      "Epoch: 26, Loss: 166.1737\n",
      "Epoch: 27, Loss: 162.5018\n",
      "Epoch: 28, Loss: 158.9704\n",
      "Epoch: 29, Loss: 155.5921\n",
      "Epoch: 29, Validation Loss: 155.8933, AUC Score: 0.1639\n",
      "Epoch: 30, Loss: 152.3754\n",
      "Epoch: 31, Loss: 149.3251\n",
      "Epoch: 32, Loss: 146.4431\n",
      "Epoch: 33, Loss: 143.7284\n",
      "Epoch: 34, Loss: 141.1777\n",
      "Epoch: 35, Loss: 138.7856\n",
      "Epoch: 36, Loss: 136.5446\n",
      "Epoch: 37, Loss: 134.4459\n",
      "Epoch: 38, Loss: 132.4800\n",
      "Epoch: 39, Loss: 130.6375\n",
      "Epoch: 39, Validation Loss: 130.2633, AUC Score: 0.1627\n",
      "Epoch: 40, Loss: 128.9098\n",
      "Epoch: 41, Loss: 127.2894\n",
      "Epoch: 42, Loss: 125.7711\n",
      "Epoch: 43, Loss: 124.3513\n",
      "Epoch: 44, Loss: 123.0287\n",
      "Epoch: 45, Loss: 121.8034\n",
      "Epoch: 46, Loss: 120.6760\n",
      "Epoch: 47, Loss: 119.6459\n",
      "Epoch: 48, Loss: 118.7102\n",
      "Epoch: 49, Loss: 117.8637\n",
      "Epoch: 49, Validation Loss: 117.5937, AUC Score: 0.8326\n",
      "Epoch: 50, Loss: 117.1003\n",
      "Epoch: 51, Loss: 116.4131\n",
      "Epoch: 52, Loss: 115.7946\n",
      "Epoch: 53, Loss: 115.2372\n",
      "Epoch: 54, Loss: 114.7332\n",
      "Epoch: 55, Loss: 114.2759\n",
      "Epoch: 56, Loss: 113.8593\n",
      "Epoch: 57, Loss: 113.4785\n",
      "Epoch: 58, Loss: 113.1295\n",
      "Epoch: 59, Loss: 112.8095\n",
      "Epoch: 59, Validation Loss: 112.8371, AUC Score: 0.8336\n",
      "Epoch: 60, Loss: 112.5159\n",
      "Epoch: 61, Loss: 112.2467\n",
      "Epoch: 62, Loss: 111.9999\n",
      "Epoch: 63, Loss: 111.7736\n",
      "Epoch: 64, Loss: 111.5658\n",
      "Epoch: 65, Loss: 111.3743\n",
      "Epoch: 66, Loss: 111.1971\n",
      "Epoch: 67, Loss: 111.0321\n",
      "Epoch: 68, Loss: 110.8777\n",
      "Epoch: 69, Loss: 110.7322\n",
      "Epoch: 69, Validation Loss: 110.7230, AUC Score: 0.8337\n",
      "Epoch: 70, Loss: 110.5943\n",
      "Epoch: 71, Loss: 110.4628\n",
      "Epoch: 72, Loss: 110.3370\n",
      "Epoch: 73, Loss: 110.2162\n",
      "Epoch: 74, Loss: 110.0997\n",
      "Epoch: 75, Loss: 109.9871\n",
      "Epoch: 76, Loss: 109.8779\n",
      "Epoch: 77, Loss: 109.7718\n",
      "Epoch: 78, Loss: 109.6684\n",
      "Epoch: 79, Loss: 109.5672\n",
      "Epoch: 79, Validation Loss: 109.4744, AUC Score: 0.8337\n",
      "Epoch: 80, Loss: 109.4677\n",
      "Epoch: 81, Loss: 109.3697\n",
      "Epoch: 82, Loss: 109.2727\n",
      "Epoch: 83, Loss: 109.1764\n",
      "Epoch: 84, Loss: 109.0805\n",
      "Epoch: 85, Loss: 108.9849\n",
      "Epoch: 86, Loss: 108.8893\n",
      "Epoch: 87, Loss: 108.7937\n",
      "Epoch: 88, Loss: 108.6981\n",
      "Epoch: 89, Loss: 108.6023\n",
      "Epoch: 89, Validation Loss: 108.3523, AUC Score: 0.8341\n",
      "Epoch: 90, Loss: 108.5066\n",
      "Epoch: 91, Loss: 108.4108\n",
      "Epoch: 92, Loss: 108.3151\n",
      "Epoch: 93, Loss: 108.2197\n",
      "Epoch: 94, Loss: 108.1246\n",
      "Epoch: 95, Loss: 108.0301\n",
      "Epoch: 96, Loss: 107.9363\n",
      "Epoch: 97, Loss: 107.8432\n",
      "Epoch: 98, Loss: 107.7510\n",
      "Epoch: 99, Loss: 107.6599\n",
      "Epoch: 99, Validation Loss: 107.2035, AUC Score: 0.8344\n",
      "Epoch: 100, Loss: 107.5697\n",
      "Epoch: 101, Loss: 107.4806\n",
      "Epoch: 102, Loss: 107.3924\n",
      "Epoch: 103, Loss: 107.3052\n",
      "Epoch: 104, Loss: 107.2190\n",
      "Epoch: 105, Loss: 107.1337\n",
      "Epoch: 106, Loss: 107.0495\n",
      "Epoch: 107, Loss: 106.9662\n",
      "Epoch: 108, Loss: 106.8841\n",
      "Epoch: 109, Loss: 106.8032\n",
      "Epoch: 109, Validation Loss: 106.0897, AUC Score: 0.8347\n",
      "Epoch: 110, Loss: 106.7235\n",
      "Epoch: 111, Loss: 106.6450\n",
      "Epoch: 112, Loss: 106.5678\n",
      "Epoch: 113, Loss: 106.4919\n",
      "Epoch: 114, Loss: 106.4172\n",
      "Epoch: 115, Loss: 106.3437\n",
      "Epoch: 116, Loss: 106.2714\n",
      "Epoch: 117, Loss: 106.2005\n",
      "Epoch: 118, Loss: 106.1309\n",
      "Epoch: 119, Loss: 106.0626\n",
      "Epoch: 119, Validation Loss: 105.1256, AUC Score: 0.8349\n",
      "Epoch: 120, Loss: 105.9957\n",
      "Epoch: 121, Loss: 105.9303\n",
      "Epoch: 122, Loss: 105.8664\n",
      "Epoch: 123, Loss: 105.8039\n",
      "Epoch: 124, Loss: 105.7430\n",
      "Epoch: 125, Loss: 105.6835\n",
      "Epoch: 126, Loss: 105.6255\n",
      "Epoch: 127, Loss: 105.5690\n",
      "Epoch: 128, Loss: 105.5141\n",
      "Epoch: 129, Loss: 105.4607\n",
      "Epoch: 129, Validation Loss: 104.3517, AUC Score: 0.8348\n",
      "Epoch: 130, Loss: 105.4089\n",
      "Epoch: 131, Loss: 105.3586\n",
      "Epoch: 132, Loss: 105.3099\n",
      "Epoch: 133, Loss: 105.2626\n",
      "Epoch: 134, Loss: 105.2168\n",
      "Epoch: 135, Loss: 105.1725\n",
      "Epoch: 136, Loss: 105.1296\n",
      "Epoch: 137, Loss: 105.0881\n",
      "Epoch: 138, Loss: 105.0480\n",
      "Epoch: 139, Loss: 105.0093\n",
      "Epoch: 139, Validation Loss: 103.7672, AUC Score: 0.8349\n",
      "Epoch: 140, Loss: 104.9720\n",
      "Epoch: 141, Loss: 104.9359\n",
      "Epoch: 142, Loss: 104.9011\n",
      "Epoch: 143, Loss: 104.8676\n",
      "Epoch: 144, Loss: 104.8352\n",
      "Epoch: 145, Loss: 104.8041\n",
      "Epoch: 146, Loss: 104.7740\n",
      "Epoch: 147, Loss: 104.7451\n",
      "Epoch: 148, Loss: 104.7172\n",
      "Epoch: 149, Loss: 104.6904\n",
      "Epoch: 149, Validation Loss: 103.3465, AUC Score: 0.8350\n",
      "Epoch: 150, Loss: 104.6646\n",
      "Epoch: 151, Loss: 104.6398\n",
      "Epoch: 152, Loss: 104.6159\n",
      "Epoch: 153, Loss: 104.5928\n",
      "Epoch: 154, Loss: 104.5707\n",
      "Epoch: 155, Loss: 104.5493\n",
      "Epoch: 156, Loss: 104.5288\n",
      "Epoch: 157, Loss: 104.5091\n",
      "Epoch: 158, Loss: 104.4900\n",
      "Epoch: 159, Loss: 104.4717\n",
      "Epoch: 159, Validation Loss: 103.0569, AUC Score: 0.8350\n",
      "Epoch: 160, Loss: 104.4541\n",
      "Epoch: 161, Loss: 104.4371\n",
      "Epoch: 162, Loss: 104.4208\n",
      "Epoch: 163, Loss: 104.4050\n",
      "Epoch: 164, Loss: 104.3899\n",
      "Epoch: 165, Loss: 104.3753\n",
      "Epoch: 166, Loss: 104.3612\n",
      "Epoch: 167, Loss: 104.3476\n",
      "Epoch: 168, Loss: 104.3345\n",
      "Epoch: 169, Loss: 104.3219\n",
      "Epoch: 169, Validation Loss: 102.8599, AUC Score: 0.8349\n",
      "Epoch: 170, Loss: 104.3097\n",
      "Epoch: 171, Loss: 104.2979\n",
      "Epoch: 172, Loss: 104.2866\n",
      "Epoch: 173, Loss: 104.2756\n",
      "Epoch: 174, Loss: 104.2650\n",
      "Epoch: 175, Loss: 104.2548\n",
      "Epoch: 176, Loss: 104.2450\n",
      "Epoch: 177, Loss: 104.2354\n",
      "Epoch: 178, Loss: 104.2262\n",
      "Epoch: 179, Loss: 104.2174\n",
      "Epoch: 179, Validation Loss: 102.7278, AUC Score: 0.8350\n",
      "Epoch: 180, Loss: 104.2088\n",
      "Epoch: 181, Loss: 104.2005\n",
      "Epoch: 182, Loss: 104.1925\n",
      "Epoch: 183, Loss: 104.1847\n",
      "Epoch: 184, Loss: 104.1772\n",
      "Epoch: 185, Loss: 104.1700\n",
      "Epoch: 186, Loss: 104.1630\n",
      "Epoch: 187, Loss: 104.1563\n",
      "Epoch: 188, Loss: 104.1498\n",
      "Epoch: 189, Loss: 104.1435\n",
      "Epoch: 189, Validation Loss: 102.6362, AUC Score: 0.8350\n",
      "Epoch: 190, Loss: 104.1374\n",
      "Epoch: 191, Loss: 104.1315\n",
      "Epoch: 192, Loss: 104.1258\n",
      "Epoch: 193, Loss: 104.1203\n",
      "Epoch: 194, Loss: 104.1150\n",
      "Epoch: 195, Loss: 104.1099\n",
      "Epoch: 196, Loss: 104.1050\n",
      "Epoch: 197, Loss: 104.1002\n",
      "Epoch: 198, Loss: 104.0956\n",
      "Epoch: 199, Loss: 104.0911\n",
      "Epoch: 199, Validation Loss: 102.5719, AUC Score: 0.8353\n",
      "Epoch: 200, Loss: 104.0868\n",
      "Epoch: 201, Loss: 104.0827\n",
      "Epoch: 202, Loss: 104.0787\n",
      "Epoch: 203, Loss: 104.0748\n",
      "Epoch: 204, Loss: 104.0710\n",
      "Epoch: 205, Loss: 104.0674\n",
      "Epoch: 206, Loss: 104.0640\n",
      "Epoch: 207, Loss: 104.0606\n",
      "Epoch: 208, Loss: 104.0574\n",
      "Epoch: 209, Loss: 104.0543\n",
      "Epoch: 209, Validation Loss: 102.5259, AUC Score: 0.8353\n",
      "Epoch: 210, Loss: 104.0513\n",
      "Epoch: 211, Loss: 104.0484\n",
      "Epoch: 212, Loss: 104.0456\n",
      "Epoch: 213, Loss: 104.0429\n",
      "Epoch: 214, Loss: 104.0403\n",
      "Epoch: 215, Loss: 104.0378\n",
      "Epoch: 216, Loss: 104.0354\n",
      "Epoch: 217, Loss: 104.0331\n",
      "Epoch: 218, Loss: 104.0308\n",
      "Epoch: 219, Loss: 104.0287\n",
      "Epoch: 219, Validation Loss: 102.4940, AUC Score: 0.8353\n",
      "Epoch: 220, Loss: 104.0266\n",
      "Epoch: 221, Loss: 104.0246\n",
      "Epoch: 222, Loss: 104.0227\n",
      "Epoch: 223, Loss: 104.0209\n",
      "Epoch: 224, Loss: 104.0191\n",
      "Epoch: 225, Loss: 104.0174\n",
      "Epoch: 226, Loss: 104.0158\n",
      "Epoch: 227, Loss: 104.0142\n",
      "Epoch: 228, Loss: 104.0127\n",
      "Epoch: 229, Loss: 104.0112\n",
      "Epoch: 229, Validation Loss: 102.4722, AUC Score: 0.8353\n",
      "Epoch: 230, Loss: 104.0099\n",
      "Epoch: 231, Loss: 104.0085\n",
      "Epoch: 232, Loss: 104.0072\n",
      "Epoch: 233, Loss: 104.0060\n",
      "Epoch: 234, Loss: 104.0048\n",
      "Epoch: 235, Loss: 104.0037\n",
      "Epoch: 236, Loss: 104.0026\n",
      "Epoch: 237, Loss: 104.0016\n",
      "Epoch: 238, Loss: 104.0006\n",
      "Epoch: 239, Loss: 103.9996\n",
      "Epoch: 239, Validation Loss: 102.4576, AUC Score: 0.8354\n",
      "Epoch: 240, Loss: 103.9987\n",
      "Epoch: 241, Loss: 103.9978\n",
      "Epoch: 242, Loss: 103.9970\n",
      "Epoch: 243, Loss: 103.9962\n",
      "Epoch: 244, Loss: 103.9954\n",
      "Epoch: 245, Loss: 103.9946\n",
      "Epoch: 246, Loss: 103.9939\n",
      "Epoch: 247, Loss: 103.9933\n",
      "Epoch: 248, Loss: 103.9926\n",
      "Epoch: 249, Loss: 103.9920\n",
      "Epoch: 249, Validation Loss: 102.4480, AUC Score: 0.8354\n",
      "Epoch: 250, Loss: 103.9914\n",
      "Epoch: 251, Loss: 103.9909\n",
      "Epoch: 252, Loss: 103.9903\n",
      "Epoch: 253, Loss: 103.9898\n",
      "Epoch: 254, Loss: 103.9893\n",
      "Epoch: 255, Loss: 103.9889\n",
      "Epoch: 256, Loss: 103.9884\n",
      "Epoch: 257, Loss: 103.9880\n",
      "Epoch: 258, Loss: 103.9876\n",
      "Epoch: 259, Loss: 103.9872\n",
      "Epoch: 259, Validation Loss: 102.4418, AUC Score: 0.8354\n",
      "Epoch: 260, Loss: 103.9868\n",
      "Epoch: 261, Loss: 103.9865\n",
      "Epoch: 262, Loss: 103.9862\n",
      "Epoch: 263, Loss: 103.9858\n",
      "Epoch: 264, Loss: 103.9855\n",
      "Epoch: 265, Loss: 103.9853\n",
      "Epoch: 266, Loss: 103.9850\n",
      "Epoch: 267, Loss: 103.9847\n",
      "Epoch: 268, Loss: 103.9845\n",
      "Epoch: 269, Loss: 103.9843\n",
      "Epoch: 269, Validation Loss: 102.4381, AUC Score: 0.8354\n",
      "Epoch: 270, Loss: 103.9840\n",
      "Epoch: 271, Loss: 103.9838\n",
      "Epoch: 272, Loss: 103.9836\n",
      "Epoch: 273, Loss: 103.9834\n",
      "Epoch: 274, Loss: 103.9833\n",
      "Epoch: 275, Loss: 103.9831\n",
      "Epoch: 276, Loss: 103.9829\n",
      "Epoch: 277, Loss: 103.9828\n",
      "Epoch: 278, Loss: 103.9826\n",
      "Epoch: 279, Loss: 103.9825\n",
      "Epoch: 279, Validation Loss: 102.4359, AUC Score: 0.8354\n",
      "Epoch: 280, Loss: 103.9824\n",
      "Epoch: 281, Loss: 103.9823\n",
      "Epoch: 282, Loss: 103.9821\n",
      "Epoch: 283, Loss: 103.9820\n",
      "Epoch: 284, Loss: 103.9819\n",
      "Epoch: 285, Loss: 103.9818\n",
      "Epoch: 286, Loss: 103.9817\n",
      "Epoch: 287, Loss: 103.9817\n",
      "Epoch: 288, Loss: 103.9816\n",
      "Epoch: 289, Loss: 103.9815\n",
      "Epoch: 289, Validation Loss: 102.4346, AUC Score: 0.8354\n",
      "Epoch: 290, Loss: 103.9814\n",
      "Epoch: 291, Loss: 103.9814\n",
      "Epoch: 292, Loss: 103.9813\n",
      "Epoch: 293, Loss: 103.9812\n",
      "Epoch: 294, Loss: 103.9812\n",
      "Epoch: 295, Loss: 103.9811\n",
      "Epoch: 296, Loss: 103.9811\n",
      "Epoch: 297, Loss: 103.9810\n",
      "Epoch: 298, Loss: 103.9810\n",
      "Epoch: 299, Loss: 103.9809\n",
      "Epoch: 299, Validation Loss: 102.4339, AUC Score: 0.8354\n",
      "Epoch: 300, Loss: 103.9809\n",
      "Epoch: 301, Loss: 103.9809\n",
      "Epoch: 302, Loss: 103.9808\n",
      "Epoch: 303, Loss: 103.9808\n",
      "Epoch: 304, Loss: 103.9808\n",
      "Epoch: 305, Loss: 103.9807\n",
      "Epoch: 306, Loss: 103.9807\n",
      "Epoch: 307, Loss: 103.9807\n",
      "Epoch: 308, Loss: 103.9806\n",
      "Epoch: 309, Loss: 103.9806\n",
      "Epoch: 309, Validation Loss: 102.4335, AUC Score: 0.8354\n",
      "Epoch: 310, Loss: 103.9806\n",
      "Epoch: 311, Loss: 103.9806\n",
      "Epoch: 312, Loss: 103.9805\n",
      "Epoch: 313, Loss: 103.9805\n",
      "Epoch: 314, Loss: 103.9805\n",
      "Epoch: 315, Loss: 103.9805\n",
      "Epoch: 316, Loss: 103.9805\n",
      "Epoch: 317, Loss: 103.9805\n",
      "Epoch: 318, Loss: 103.9804\n",
      "Epoch: 319, Loss: 103.9804\n",
      "Epoch: 319, Validation Loss: 102.4332, AUC Score: 0.8354\n",
      "Epoch: 320, Loss: 103.9804\n",
      "Epoch: 321, Loss: 103.9804\n",
      "Epoch: 322, Loss: 103.9804\n",
      "Epoch: 323, Loss: 103.9804\n",
      "Epoch: 324, Loss: 103.9804\n",
      "Epoch: 325, Loss: 103.9804\n",
      "Epoch: 326, Loss: 103.9803\n",
      "Epoch: 327, Loss: 103.9803\n",
      "Epoch: 328, Loss: 103.9803\n",
      "Epoch: 329, Loss: 103.9803\n",
      "Epoch: 329, Validation Loss: 102.4331, AUC Score: 0.8354\n",
      "Epoch: 330, Loss: 103.9803\n",
      "Epoch: 331, Loss: 103.9803\n",
      "Epoch: 332, Loss: 103.9803\n",
      "Epoch: 333, Loss: 103.9803\n",
      "Epoch: 334, Loss: 103.9803\n",
      "Epoch: 335, Loss: 103.9803\n",
      "Epoch: 336, Loss: 103.9802\n",
      "Epoch: 337, Loss: 103.9802\n",
      "Epoch: 338, Loss: 103.9802\n",
      "Epoch: 339, Loss: 103.9802\n",
      "Epoch: 339, Validation Loss: 102.4330, AUC Score: 0.8354\n",
      "Epoch: 340, Loss: 103.9802\n",
      "Epoch: 341, Loss: 103.9802\n",
      "Epoch: 342, Loss: 103.9802\n",
      "Epoch: 343, Loss: 103.9802\n",
      "Epoch: 344, Loss: 103.9802\n",
      "Epoch: 345, Loss: 103.9802\n",
      "Epoch: 346, Loss: 103.9802\n",
      "Epoch: 347, Loss: 103.9802\n",
      "Epoch: 348, Loss: 103.9802\n",
      "Epoch: 349, Loss: 103.9801\n",
      "Epoch: 349, Validation Loss: 102.4329, AUC Score: 0.8354\n",
      "Epoch: 350, Loss: 103.9801\n",
      "Epoch: 351, Loss: 103.9801\n",
      "Epoch: 352, Loss: 103.9801\n",
      "Epoch: 353, Loss: 103.9801\n",
      "Epoch: 354, Loss: 103.9801\n",
      "Epoch: 355, Loss: 103.9801\n",
      "Epoch: 356, Loss: 103.9801\n",
      "Epoch: 357, Loss: 103.9801\n",
      "Epoch: 358, Loss: 103.9801\n",
      "Epoch: 359, Loss: 103.9801\n",
      "Epoch: 359, Validation Loss: 102.4328, AUC Score: 0.8354\n",
      "Epoch: 360, Loss: 103.9801\n",
      "Epoch: 361, Loss: 103.9801\n",
      "Epoch: 362, Loss: 103.9801\n",
      "Epoch: 363, Loss: 103.9801\n",
      "Epoch: 364, Loss: 103.9801\n",
      "Epoch: 365, Loss: 103.9800\n",
      "Epoch: 366, Loss: 103.9800\n",
      "Epoch: 367, Loss: 103.9800\n",
      "Epoch: 368, Loss: 103.9800\n",
      "Epoch: 369, Loss: 103.9800\n",
      "Epoch: 369, Validation Loss: 102.4328, AUC Score: 0.8354\n",
      "Epoch: 370, Loss: 103.9800\n",
      "Epoch: 371, Loss: 103.9800\n",
      "Epoch: 372, Loss: 103.9800\n",
      "Epoch: 373, Loss: 103.9800\n",
      "Epoch: 374, Loss: 103.9800\n",
      "Epoch: 375, Loss: 103.9800\n",
      "Epoch: 376, Loss: 103.9800\n",
      "Epoch: 377, Loss: 103.9800\n",
      "Epoch: 378, Loss: 103.9800\n",
      "Epoch: 379, Loss: 103.9800\n",
      "Epoch: 379, Validation Loss: 102.4327, AUC Score: 0.8354\n",
      "Epoch: 380, Loss: 103.9800\n",
      "Epoch: 381, Loss: 103.9800\n",
      "Epoch: 382, Loss: 103.9800\n",
      "Epoch: 383, Loss: 103.9799\n",
      "Epoch: 384, Loss: 103.9799\n",
      "Epoch: 385, Loss: 103.9799\n",
      "Epoch: 386, Loss: 103.9799\n",
      "Epoch: 387, Loss: 103.9799\n",
      "Epoch: 388, Loss: 103.9799\n",
      "Epoch: 389, Loss: 103.9799\n",
      "Epoch: 389, Validation Loss: 102.4326, AUC Score: 0.8354\n",
      "Epoch: 390, Loss: 103.9799\n",
      "Epoch: 391, Loss: 103.9799\n",
      "Epoch: 392, Loss: 103.9799\n",
      "Epoch: 393, Loss: 103.9799\n",
      "Epoch: 394, Loss: 103.9799\n",
      "Epoch: 395, Loss: 103.9799\n",
      "Epoch: 396, Loss: 103.9799\n",
      "Epoch: 397, Loss: 103.9799\n",
      "Epoch: 398, Loss: 103.9799\n",
      "Epoch: 399, Loss: 103.9799\n",
      "Epoch: 399, Validation Loss: 102.4326, AUC Score: 0.8354\n",
      "Epoch: 400, Loss: 103.9799\n",
      "Epoch: 401, Loss: 103.9799\n",
      "Epoch: 402, Loss: 103.9799\n",
      "Epoch: 403, Loss: 103.9798\n",
      "Epoch: 404, Loss: 103.9798\n",
      "Epoch: 405, Loss: 103.9798\n",
      "Epoch: 406, Loss: 103.9798\n",
      "Epoch: 407, Loss: 103.9798\n",
      "Epoch: 408, Loss: 103.9798\n",
      "Epoch: 409, Loss: 103.9798\n",
      "Epoch: 409, Validation Loss: 102.4325, AUC Score: 0.8354\n",
      "Epoch: 410, Loss: 103.9798\n",
      "Epoch: 411, Loss: 103.9798\n",
      "Epoch: 412, Loss: 103.9798\n",
      "Epoch: 413, Loss: 103.9798\n",
      "Epoch: 414, Loss: 103.9798\n",
      "Epoch: 415, Loss: 103.9798\n",
      "Epoch: 416, Loss: 103.9798\n",
      "Epoch: 417, Loss: 103.9798\n",
      "Epoch: 418, Loss: 103.9798\n",
      "Epoch: 419, Loss: 103.9798\n",
      "Epoch: 419, Validation Loss: 102.4325, AUC Score: 0.8354\n",
      "Epoch: 420, Loss: 103.9798\n",
      "Epoch: 421, Loss: 103.9798\n",
      "Epoch: 422, Loss: 103.9798\n",
      "Epoch: 423, Loss: 103.9798\n",
      "Epoch: 424, Loss: 103.9798\n",
      "Epoch: 425, Loss: 103.9798\n",
      "Epoch: 426, Loss: 103.9798\n",
      "Epoch: 427, Loss: 103.9798\n",
      "Epoch: 428, Loss: 103.9797\n",
      "Epoch: 429, Loss: 103.9797\n",
      "Epoch: 429, Validation Loss: 102.4325, AUC Score: 0.8354\n",
      "Epoch: 430, Loss: 103.9797\n",
      "Epoch: 431, Loss: 103.9797\n",
      "Epoch: 432, Loss: 103.9797\n",
      "Epoch: 433, Loss: 103.9797\n",
      "Epoch: 434, Loss: 103.9797\n",
      "Epoch: 435, Loss: 103.9797\n",
      "Epoch: 436, Loss: 103.9797\n",
      "Epoch: 437, Loss: 103.9797\n",
      "Epoch: 438, Loss: 103.9797\n",
      "Epoch: 439, Loss: 103.9797\n",
      "Epoch: 439, Validation Loss: 102.4324, AUC Score: 0.8354\n",
      "Epoch: 440, Loss: 103.9797\n",
      "Epoch: 441, Loss: 103.9797\n",
      "Epoch: 442, Loss: 103.9797\n",
      "Epoch: 443, Loss: 103.9797\n",
      "Epoch: 444, Loss: 103.9797\n",
      "Epoch: 445, Loss: 103.9797\n",
      "Epoch: 446, Loss: 103.9797\n",
      "Epoch: 447, Loss: 103.9797\n",
      "Epoch: 448, Loss: 103.9797\n",
      "Epoch: 449, Loss: 103.9797\n",
      "Epoch: 449, Validation Loss: 102.4324, AUC Score: 0.8354\n",
      "Epoch: 450, Loss: 103.9797\n",
      "Epoch: 451, Loss: 103.9797\n",
      "Epoch: 452, Loss: 103.9797\n",
      "Epoch: 453, Loss: 103.9797\n",
      "Epoch: 454, Loss: 103.9797\n",
      "Epoch: 455, Loss: 103.9797\n",
      "Epoch: 456, Loss: 103.9797\n",
      "Epoch: 457, Loss: 103.9797\n",
      "Epoch: 458, Loss: 103.9797\n",
      "Epoch: 459, Loss: 103.9797\n",
      "Epoch: 459, Validation Loss: 102.4324, AUC Score: 0.8354\n",
      "Epoch: 460, Loss: 103.9796\n",
      "Epoch: 461, Loss: 103.9796\n",
      "Epoch: 462, Loss: 103.9796\n",
      "Epoch: 463, Loss: 103.9796\n",
      "Epoch: 464, Loss: 103.9796\n",
      "Epoch: 465, Loss: 103.9796\n",
      "Epoch: 466, Loss: 103.9796\n",
      "Epoch: 467, Loss: 103.9796\n",
      "Epoch: 468, Loss: 103.9796\n",
      "Epoch: 469, Loss: 103.9796\n",
      "Epoch: 469, Validation Loss: 102.4323, AUC Score: 0.8354\n",
      "Epoch: 470, Loss: 103.9796\n",
      "Epoch: 471, Loss: 103.9796\n",
      "Epoch: 472, Loss: 103.9796\n",
      "Epoch: 473, Loss: 103.9796\n",
      "Epoch: 474, Loss: 103.9796\n",
      "Epoch: 475, Loss: 103.9796\n",
      "Epoch: 476, Loss: 103.9796\n",
      "Epoch: 477, Loss: 103.9796\n",
      "Epoch: 478, Loss: 103.9796\n",
      "Epoch: 479, Loss: 103.9796\n",
      "Epoch: 479, Validation Loss: 102.4323, AUC Score: 0.8354\n",
      "Epoch: 480, Loss: 103.9796\n",
      "Epoch: 481, Loss: 103.9796\n",
      "Epoch: 482, Loss: 103.9796\n",
      "Epoch: 483, Loss: 103.9796\n",
      "Epoch: 484, Loss: 103.9796\n",
      "Epoch: 485, Loss: 103.9796\n",
      "Epoch: 486, Loss: 103.9796\n",
      "Epoch: 487, Loss: 103.9796\n",
      "Epoch: 488, Loss: 103.9796\n",
      "Epoch: 489, Loss: 103.9796\n",
      "Epoch: 489, Validation Loss: 102.4323, AUC Score: 0.8354\n",
      "Epoch: 490, Loss: 103.9796\n",
      "Epoch: 491, Loss: 103.9796\n",
      "Epoch: 492, Loss: 103.9796\n",
      "Epoch: 493, Loss: 103.9796\n",
      "Epoch: 494, Loss: 103.9796\n",
      "Epoch: 495, Loss: 103.9796\n",
      "Epoch: 496, Loss: 103.9796\n",
      "Epoch: 497, Loss: 103.9796\n",
      "Epoch: 498, Loss: 103.9796\n",
      "Epoch: 499, Loss: 103.9796\n",
      "Epoch: 499, Validation Loss: 102.4323, AUC Score: 0.8354\n",
      "Epoch: 500, Loss: 103.9796\n",
      "Epoch: 501, Loss: 103.9796\n",
      "Epoch: 502, Loss: 103.9796\n",
      "Epoch: 503, Loss: 103.9796\n",
      "Epoch: 504, Loss: 103.9796\n",
      "Epoch: 505, Loss: 103.9796\n",
      "Epoch: 506, Loss: 103.9796\n",
      "Epoch: 507, Loss: 103.9796\n",
      "Epoch: 508, Loss: 103.9796\n",
      "Epoch: 509, Loss: 103.9796\n",
      "Epoch: 509, Validation Loss: 102.4323, AUC Score: 0.8354\n",
      "Epoch: 510, Loss: 103.9796\n",
      "Epoch: 511, Loss: 103.9796\n",
      "Epoch: 512, Loss: 103.9796\n",
      "Epoch: 513, Loss: 103.9796\n",
      "Epoch: 514, Loss: 103.9796\n",
      "Epoch: 515, Loss: 103.9795\n",
      "Epoch: 516, Loss: 103.9795\n",
      "Epoch: 517, Loss: 103.9795\n",
      "Epoch: 518, Loss: 103.9795\n",
      "Epoch: 519, Loss: 103.9795\n",
      "Epoch: 519, Validation Loss: 102.4323, AUC Score: 0.8354\n",
      "Epoch: 520, Loss: 103.9795\n",
      "Epoch: 521, Loss: 103.9795\n",
      "Epoch: 522, Loss: 103.9795\n",
      "Epoch: 523, Loss: 103.9795\n",
      "Epoch: 524, Loss: 103.9795\n",
      "Epoch: 525, Loss: 103.9795\n",
      "Epoch: 526, Loss: 103.9795\n",
      "Epoch: 527, Loss: 103.9795\n",
      "Epoch: 528, Loss: 103.9795\n",
      "Epoch: 529, Loss: 103.9795\n",
      "Epoch: 529, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 530, Loss: 103.9795\n",
      "Epoch: 531, Loss: 103.9795\n",
      "Epoch: 532, Loss: 103.9795\n",
      "Epoch: 533, Loss: 103.9795\n",
      "Epoch: 534, Loss: 103.9795\n",
      "Epoch: 535, Loss: 103.9795\n",
      "Epoch: 536, Loss: 103.9795\n",
      "Epoch: 537, Loss: 103.9795\n",
      "Epoch: 538, Loss: 103.9795\n",
      "Epoch: 539, Loss: 103.9795\n",
      "Epoch: 539, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 540, Loss: 103.9795\n",
      "Epoch: 541, Loss: 103.9795\n",
      "Epoch: 542, Loss: 103.9795\n",
      "Epoch: 543, Loss: 103.9795\n",
      "Epoch: 544, Loss: 103.9795\n",
      "Epoch: 545, Loss: 103.9795\n",
      "Epoch: 546, Loss: 103.9795\n",
      "Epoch: 547, Loss: 103.9795\n",
      "Epoch: 548, Loss: 103.9795\n",
      "Epoch: 549, Loss: 103.9795\n",
      "Epoch: 549, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 550, Loss: 103.9795\n",
      "Epoch: 551, Loss: 103.9795\n",
      "Epoch: 552, Loss: 103.9795\n",
      "Epoch: 553, Loss: 103.9795\n",
      "Epoch: 554, Loss: 103.9795\n",
      "Epoch: 555, Loss: 103.9795\n",
      "Epoch: 556, Loss: 103.9795\n",
      "Epoch: 557, Loss: 103.9795\n",
      "Epoch: 558, Loss: 103.9795\n",
      "Epoch: 559, Loss: 103.9795\n",
      "Epoch: 559, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 560, Loss: 103.9795\n",
      "Epoch: 561, Loss: 103.9795\n",
      "Epoch: 562, Loss: 103.9795\n",
      "Epoch: 563, Loss: 103.9795\n",
      "Epoch: 564, Loss: 103.9795\n",
      "Epoch: 565, Loss: 103.9795\n",
      "Epoch: 566, Loss: 103.9795\n",
      "Epoch: 567, Loss: 103.9795\n",
      "Epoch: 568, Loss: 103.9795\n",
      "Epoch: 569, Loss: 103.9795\n",
      "Epoch: 569, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 570, Loss: 103.9795\n",
      "Epoch: 571, Loss: 103.9795\n",
      "Epoch: 572, Loss: 103.9795\n",
      "Epoch: 573, Loss: 103.9795\n",
      "Epoch: 574, Loss: 103.9795\n",
      "Epoch: 575, Loss: 103.9795\n",
      "Epoch: 576, Loss: 103.9795\n",
      "Epoch: 577, Loss: 103.9795\n",
      "Epoch: 578, Loss: 103.9795\n",
      "Epoch: 579, Loss: 103.9795\n",
      "Epoch: 579, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 580, Loss: 103.9795\n",
      "Epoch: 581, Loss: 103.9795\n",
      "Epoch: 582, Loss: 103.9795\n",
      "Epoch: 583, Loss: 103.9795\n",
      "Epoch: 584, Loss: 103.9795\n",
      "Epoch: 585, Loss: 103.9795\n",
      "Epoch: 586, Loss: 103.9795\n",
      "Epoch: 587, Loss: 103.9795\n",
      "Epoch: 588, Loss: 103.9795\n",
      "Epoch: 589, Loss: 103.9795\n",
      "Epoch: 589, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 590, Loss: 103.9795\n",
      "Epoch: 591, Loss: 103.9795\n",
      "Epoch: 592, Loss: 103.9795\n",
      "Epoch: 593, Loss: 103.9795\n",
      "Epoch: 594, Loss: 103.9795\n",
      "Epoch: 595, Loss: 103.9795\n",
      "Epoch: 596, Loss: 103.9795\n",
      "Epoch: 597, Loss: 103.9795\n",
      "Epoch: 598, Loss: 103.9795\n",
      "Epoch: 599, Loss: 103.9795\n",
      "Epoch: 599, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 600, Loss: 103.9795\n",
      "Epoch: 601, Loss: 103.9795\n",
      "Epoch: 602, Loss: 103.9795\n",
      "Epoch: 603, Loss: 103.9795\n",
      "Epoch: 604, Loss: 103.9795\n",
      "Epoch: 605, Loss: 103.9795\n",
      "Epoch: 606, Loss: 103.9795\n",
      "Epoch: 607, Loss: 103.9795\n",
      "Epoch: 608, Loss: 103.9795\n",
      "Epoch: 609, Loss: 103.9795\n",
      "Epoch: 609, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 610, Loss: 103.9795\n",
      "Epoch: 611, Loss: 103.9795\n",
      "Epoch: 612, Loss: 103.9795\n",
      "Epoch: 613, Loss: 103.9795\n",
      "Epoch: 614, Loss: 103.9795\n",
      "Epoch: 615, Loss: 103.9795\n",
      "Epoch: 616, Loss: 103.9795\n",
      "Epoch: 617, Loss: 103.9795\n",
      "Epoch: 618, Loss: 103.9795\n",
      "Epoch: 619, Loss: 103.9795\n",
      "Epoch: 619, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 620, Loss: 103.9795\n",
      "Epoch: 621, Loss: 103.9795\n",
      "Epoch: 622, Loss: 103.9795\n",
      "Epoch: 623, Loss: 103.9795\n",
      "Epoch: 624, Loss: 103.9795\n",
      "Epoch: 625, Loss: 103.9795\n",
      "Epoch: 626, Loss: 103.9795\n",
      "Epoch: 627, Loss: 103.9795\n",
      "Epoch: 628, Loss: 103.9795\n",
      "Epoch: 629, Loss: 103.9795\n",
      "Epoch: 629, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 630, Loss: 103.9795\n",
      "Epoch: 631, Loss: 103.9795\n",
      "Epoch: 632, Loss: 103.9795\n",
      "Epoch: 633, Loss: 103.9795\n",
      "Epoch: 634, Loss: 103.9795\n",
      "Epoch: 635, Loss: 103.9795\n",
      "Epoch: 636, Loss: 103.9795\n",
      "Epoch: 637, Loss: 103.9795\n",
      "Epoch: 638, Loss: 103.9795\n",
      "Epoch: 639, Loss: 103.9795\n",
      "Epoch: 639, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 640, Loss: 103.9795\n",
      "Epoch: 641, Loss: 103.9795\n",
      "Epoch: 642, Loss: 103.9795\n",
      "Epoch: 643, Loss: 103.9795\n",
      "Epoch: 644, Loss: 103.9795\n",
      "Epoch: 645, Loss: 103.9795\n",
      "Epoch: 646, Loss: 103.9795\n",
      "Epoch: 647, Loss: 103.9795\n",
      "Epoch: 648, Loss: 103.9795\n",
      "Epoch: 649, Loss: 103.9795\n",
      "Epoch: 649, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 650, Loss: 103.9795\n",
      "Epoch: 651, Loss: 103.9795\n",
      "Epoch: 652, Loss: 103.9795\n",
      "Epoch: 653, Loss: 103.9795\n",
      "Epoch: 654, Loss: 103.9795\n",
      "Epoch: 655, Loss: 103.9795\n",
      "Epoch: 656, Loss: 103.9795\n",
      "Epoch: 657, Loss: 103.9795\n",
      "Epoch: 658, Loss: 103.9795\n",
      "Epoch: 659, Loss: 103.9795\n",
      "Epoch: 659, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 660, Loss: 103.9795\n",
      "Epoch: 661, Loss: 103.9795\n",
      "Epoch: 662, Loss: 103.9795\n",
      "Epoch: 663, Loss: 103.9795\n",
      "Epoch: 664, Loss: 103.9795\n",
      "Epoch: 665, Loss: 103.9795\n",
      "Epoch: 666, Loss: 103.9795\n",
      "Epoch: 667, Loss: 103.9795\n",
      "Epoch: 668, Loss: 103.9795\n",
      "Epoch: 669, Loss: 103.9795\n",
      "Epoch: 669, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 670, Loss: 103.9795\n",
      "Epoch: 671, Loss: 103.9795\n",
      "Epoch: 672, Loss: 103.9795\n",
      "Epoch: 673, Loss: 103.9795\n",
      "Epoch: 674, Loss: 103.9795\n",
      "Epoch: 675, Loss: 103.9795\n",
      "Epoch: 676, Loss: 103.9795\n",
      "Epoch: 677, Loss: 103.9795\n",
      "Epoch: 678, Loss: 103.9795\n",
      "Epoch: 679, Loss: 103.9795\n",
      "Epoch: 679, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 680, Loss: 103.9795\n",
      "Epoch: 681, Loss: 103.9795\n",
      "Epoch: 682, Loss: 103.9795\n",
      "Epoch: 683, Loss: 103.9795\n",
      "Epoch: 684, Loss: 103.9795\n",
      "Epoch: 685, Loss: 103.9795\n",
      "Epoch: 686, Loss: 103.9795\n",
      "Epoch: 687, Loss: 103.9795\n",
      "Epoch: 688, Loss: 103.9795\n",
      "Epoch: 689, Loss: 103.9795\n",
      "Epoch: 689, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 690, Loss: 103.9795\n",
      "Epoch: 691, Loss: 103.9795\n",
      "Epoch: 692, Loss: 103.9795\n",
      "Epoch: 693, Loss: 103.9795\n",
      "Epoch: 694, Loss: 103.9795\n",
      "Epoch: 695, Loss: 103.9795\n",
      "Epoch: 696, Loss: 103.9795\n",
      "Epoch: 697, Loss: 103.9795\n",
      "Epoch: 698, Loss: 103.9795\n",
      "Epoch: 699, Loss: 103.9795\n",
      "Epoch: 699, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 700, Loss: 103.9795\n",
      "Epoch: 701, Loss: 103.9795\n",
      "Epoch: 702, Loss: 103.9795\n",
      "Epoch: 703, Loss: 103.9795\n",
      "Epoch: 704, Loss: 103.9795\n",
      "Epoch: 705, Loss: 103.9795\n",
      "Epoch: 706, Loss: 103.9795\n",
      "Epoch: 707, Loss: 103.9795\n",
      "Epoch: 708, Loss: 103.9795\n",
      "Epoch: 709, Loss: 103.9795\n",
      "Epoch: 709, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 710, Loss: 103.9795\n",
      "Epoch: 711, Loss: 103.9795\n",
      "Epoch: 712, Loss: 103.9795\n",
      "Epoch: 713, Loss: 103.9795\n",
      "Epoch: 714, Loss: 103.9795\n",
      "Epoch: 715, Loss: 103.9795\n",
      "Epoch: 716, Loss: 103.9795\n",
      "Epoch: 717, Loss: 103.9795\n",
      "Epoch: 718, Loss: 103.9795\n",
      "Epoch: 719, Loss: 103.9795\n",
      "Epoch: 719, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 720, Loss: 103.9795\n",
      "Epoch: 721, Loss: 103.9795\n",
      "Epoch: 722, Loss: 103.9795\n",
      "Epoch: 723, Loss: 103.9795\n",
      "Epoch: 724, Loss: 103.9795\n",
      "Epoch: 725, Loss: 103.9795\n",
      "Epoch: 726, Loss: 103.9795\n",
      "Epoch: 727, Loss: 103.9795\n",
      "Epoch: 728, Loss: 103.9795\n",
      "Epoch: 729, Loss: 103.9795\n",
      "Epoch: 729, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 730, Loss: 103.9795\n",
      "Epoch: 731, Loss: 103.9795\n",
      "Epoch: 732, Loss: 103.9795\n",
      "Epoch: 733, Loss: 103.9795\n",
      "Epoch: 734, Loss: 103.9795\n",
      "Epoch: 735, Loss: 103.9795\n",
      "Epoch: 736, Loss: 103.9795\n",
      "Epoch: 737, Loss: 103.9795\n",
      "Epoch: 738, Loss: 103.9795\n",
      "Epoch: 739, Loss: 103.9795\n",
      "Epoch: 739, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 740, Loss: 103.9795\n",
      "Epoch: 741, Loss: 103.9795\n",
      "Epoch: 742, Loss: 103.9795\n",
      "Epoch: 743, Loss: 103.9795\n",
      "Epoch: 744, Loss: 103.9795\n",
      "Epoch: 745, Loss: 103.9795\n",
      "Epoch: 746, Loss: 103.9795\n",
      "Epoch: 747, Loss: 103.9795\n",
      "Epoch: 748, Loss: 103.9795\n",
      "Epoch: 749, Loss: 103.9795\n",
      "Epoch: 749, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 750, Loss: 103.9795\n",
      "Epoch: 751, Loss: 103.9795\n",
      "Epoch: 752, Loss: 103.9795\n",
      "Epoch: 753, Loss: 103.9795\n",
      "Epoch: 754, Loss: 103.9795\n",
      "Epoch: 755, Loss: 103.9795\n",
      "Epoch: 756, Loss: 103.9795\n",
      "Epoch: 757, Loss: 103.9795\n",
      "Epoch: 758, Loss: 103.9795\n",
      "Epoch: 759, Loss: 103.9795\n",
      "Epoch: 759, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 760, Loss: 103.9795\n",
      "Epoch: 761, Loss: 103.9795\n",
      "Epoch: 762, Loss: 103.9795\n",
      "Epoch: 763, Loss: 103.9795\n",
      "Epoch: 764, Loss: 103.9795\n",
      "Epoch: 765, Loss: 103.9795\n",
      "Epoch: 766, Loss: 103.9795\n",
      "Epoch: 767, Loss: 103.9795\n",
      "Epoch: 768, Loss: 103.9795\n",
      "Epoch: 769, Loss: 103.9795\n",
      "Epoch: 769, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 770, Loss: 103.9795\n",
      "Epoch: 771, Loss: 103.9795\n",
      "Epoch: 772, Loss: 103.9795\n",
      "Epoch: 773, Loss: 103.9795\n",
      "Epoch: 774, Loss: 103.9795\n",
      "Epoch: 775, Loss: 103.9795\n",
      "Epoch: 776, Loss: 103.9795\n",
      "Epoch: 777, Loss: 103.9795\n",
      "Epoch: 778, Loss: 103.9795\n",
      "Epoch: 779, Loss: 103.9795\n",
      "Epoch: 779, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 780, Loss: 103.9795\n",
      "Epoch: 781, Loss: 103.9795\n",
      "Epoch: 782, Loss: 103.9795\n",
      "Epoch: 783, Loss: 103.9795\n",
      "Epoch: 784, Loss: 103.9795\n",
      "Epoch: 785, Loss: 103.9795\n",
      "Epoch: 786, Loss: 103.9795\n",
      "Epoch: 787, Loss: 103.9795\n",
      "Epoch: 788, Loss: 103.9795\n",
      "Epoch: 789, Loss: 103.9795\n",
      "Epoch: 789, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 790, Loss: 103.9795\n",
      "Epoch: 791, Loss: 103.9795\n",
      "Epoch: 792, Loss: 103.9795\n",
      "Epoch: 793, Loss: 103.9795\n",
      "Epoch: 794, Loss: 103.9795\n",
      "Epoch: 795, Loss: 103.9795\n",
      "Epoch: 796, Loss: 103.9795\n",
      "Epoch: 797, Loss: 103.9795\n",
      "Epoch: 798, Loss: 103.9795\n",
      "Epoch: 799, Loss: 103.9795\n",
      "Epoch: 799, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 800, Loss: 103.9795\n",
      "Epoch: 801, Loss: 103.9795\n",
      "Epoch: 802, Loss: 103.9795\n",
      "Epoch: 803, Loss: 103.9795\n",
      "Epoch: 804, Loss: 103.9795\n",
      "Epoch: 805, Loss: 103.9795\n",
      "Epoch: 806, Loss: 103.9795\n",
      "Epoch: 807, Loss: 103.9795\n",
      "Epoch: 808, Loss: 103.9795\n",
      "Epoch: 809, Loss: 103.9795\n",
      "Epoch: 809, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 810, Loss: 103.9795\n",
      "Epoch: 811, Loss: 103.9795\n",
      "Epoch: 812, Loss: 103.9795\n",
      "Epoch: 813, Loss: 103.9795\n",
      "Epoch: 814, Loss: 103.9795\n",
      "Epoch: 815, Loss: 103.9795\n",
      "Epoch: 816, Loss: 103.9795\n",
      "Epoch: 817, Loss: 103.9795\n",
      "Epoch: 818, Loss: 103.9795\n",
      "Epoch: 819, Loss: 103.9795\n",
      "Epoch: 819, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 820, Loss: 103.9795\n",
      "Epoch: 821, Loss: 103.9795\n",
      "Epoch: 822, Loss: 103.9795\n",
      "Epoch: 823, Loss: 103.9795\n",
      "Epoch: 824, Loss: 103.9795\n",
      "Epoch: 825, Loss: 103.9795\n",
      "Epoch: 826, Loss: 103.9795\n",
      "Epoch: 827, Loss: 103.9795\n",
      "Epoch: 828, Loss: 103.9795\n",
      "Epoch: 829, Loss: 103.9795\n",
      "Epoch: 829, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 830, Loss: 103.9795\n",
      "Epoch: 831, Loss: 103.9795\n",
      "Epoch: 832, Loss: 103.9795\n",
      "Epoch: 833, Loss: 103.9795\n",
      "Epoch: 834, Loss: 103.9795\n",
      "Epoch: 835, Loss: 103.9795\n",
      "Epoch: 836, Loss: 103.9795\n",
      "Epoch: 837, Loss: 103.9795\n",
      "Epoch: 838, Loss: 103.9795\n",
      "Epoch: 839, Loss: 103.9795\n",
      "Epoch: 839, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 840, Loss: 103.9795\n",
      "Epoch: 841, Loss: 103.9795\n",
      "Epoch: 842, Loss: 103.9795\n",
      "Epoch: 843, Loss: 103.9795\n",
      "Epoch: 844, Loss: 103.9795\n",
      "Epoch: 845, Loss: 103.9795\n",
      "Epoch: 846, Loss: 103.9795\n",
      "Epoch: 847, Loss: 103.9795\n",
      "Epoch: 848, Loss: 103.9795\n",
      "Epoch: 849, Loss: 103.9795\n",
      "Epoch: 849, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 850, Loss: 103.9795\n",
      "Epoch: 851, Loss: 103.9795\n",
      "Epoch: 852, Loss: 103.9795\n",
      "Epoch: 853, Loss: 103.9795\n",
      "Epoch: 854, Loss: 103.9795\n",
      "Epoch: 855, Loss: 103.9795\n",
      "Epoch: 856, Loss: 103.9795\n",
      "Epoch: 857, Loss: 103.9795\n",
      "Epoch: 858, Loss: 103.9795\n",
      "Epoch: 859, Loss: 103.9795\n",
      "Epoch: 859, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 860, Loss: 103.9795\n",
      "Epoch: 861, Loss: 103.9795\n",
      "Epoch: 862, Loss: 103.9795\n",
      "Epoch: 863, Loss: 103.9795\n",
      "Epoch: 864, Loss: 103.9795\n",
      "Epoch: 865, Loss: 103.9795\n",
      "Epoch: 866, Loss: 103.9795\n",
      "Epoch: 867, Loss: 103.9795\n",
      "Epoch: 868, Loss: 103.9795\n",
      "Epoch: 869, Loss: 103.9795\n",
      "Epoch: 869, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 870, Loss: 103.9795\n",
      "Epoch: 871, Loss: 103.9795\n",
      "Epoch: 872, Loss: 103.9795\n",
      "Epoch: 873, Loss: 103.9795\n",
      "Epoch: 874, Loss: 103.9795\n",
      "Epoch: 875, Loss: 103.9795\n",
      "Epoch: 876, Loss: 103.9795\n",
      "Epoch: 877, Loss: 103.9795\n",
      "Epoch: 878, Loss: 103.9795\n",
      "Epoch: 879, Loss: 103.9795\n",
      "Epoch: 879, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 880, Loss: 103.9795\n",
      "Epoch: 881, Loss: 103.9795\n",
      "Epoch: 882, Loss: 103.9795\n",
      "Epoch: 883, Loss: 103.9795\n",
      "Epoch: 884, Loss: 103.9795\n",
      "Epoch: 885, Loss: 103.9795\n",
      "Epoch: 886, Loss: 103.9795\n",
      "Epoch: 887, Loss: 103.9795\n",
      "Epoch: 888, Loss: 103.9795\n",
      "Epoch: 889, Loss: 103.9795\n",
      "Epoch: 889, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 890, Loss: 103.9795\n",
      "Epoch: 891, Loss: 103.9795\n",
      "Epoch: 892, Loss: 103.9795\n",
      "Epoch: 893, Loss: 103.9795\n",
      "Epoch: 894, Loss: 103.9795\n",
      "Epoch: 895, Loss: 103.9795\n",
      "Epoch: 896, Loss: 103.9795\n",
      "Epoch: 897, Loss: 103.9795\n",
      "Epoch: 898, Loss: 103.9795\n",
      "Epoch: 899, Loss: 103.9795\n",
      "Epoch: 899, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 900, Loss: 103.9795\n",
      "Epoch: 901, Loss: 103.9795\n",
      "Epoch: 902, Loss: 103.9795\n",
      "Epoch: 903, Loss: 103.9795\n",
      "Epoch: 904, Loss: 103.9795\n",
      "Epoch: 905, Loss: 103.9795\n",
      "Epoch: 906, Loss: 103.9795\n",
      "Epoch: 907, Loss: 103.9795\n",
      "Epoch: 908, Loss: 103.9795\n",
      "Epoch: 909, Loss: 103.9795\n",
      "Epoch: 909, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 910, Loss: 103.9795\n",
      "Epoch: 911, Loss: 103.9795\n",
      "Epoch: 912, Loss: 103.9795\n",
      "Epoch: 913, Loss: 103.9795\n",
      "Epoch: 914, Loss: 103.9795\n",
      "Epoch: 915, Loss: 103.9795\n",
      "Epoch: 916, Loss: 103.9795\n",
      "Epoch: 917, Loss: 103.9795\n",
      "Epoch: 918, Loss: 103.9795\n",
      "Epoch: 919, Loss: 103.9795\n",
      "Epoch: 919, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 920, Loss: 103.9795\n",
      "Epoch: 921, Loss: 103.9795\n",
      "Epoch: 922, Loss: 103.9795\n",
      "Epoch: 923, Loss: 103.9795\n",
      "Epoch: 924, Loss: 103.9795\n",
      "Epoch: 925, Loss: 103.9795\n",
      "Epoch: 926, Loss: 103.9795\n",
      "Epoch: 927, Loss: 103.9795\n",
      "Epoch: 928, Loss: 103.9795\n",
      "Epoch: 929, Loss: 103.9795\n",
      "Epoch: 929, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 930, Loss: 103.9795\n",
      "Epoch: 931, Loss: 103.9795\n",
      "Epoch: 932, Loss: 103.9795\n",
      "Epoch: 933, Loss: 103.9795\n",
      "Epoch: 934, Loss: 103.9795\n",
      "Epoch: 935, Loss: 103.9795\n",
      "Epoch: 936, Loss: 103.9795\n",
      "Epoch: 937, Loss: 103.9795\n",
      "Epoch: 938, Loss: 103.9795\n",
      "Epoch: 939, Loss: 103.9795\n",
      "Epoch: 939, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 940, Loss: 103.9795\n",
      "Epoch: 941, Loss: 103.9795\n",
      "Epoch: 942, Loss: 103.9795\n",
      "Epoch: 943, Loss: 103.9795\n",
      "Epoch: 944, Loss: 103.9795\n",
      "Epoch: 945, Loss: 103.9795\n",
      "Epoch: 946, Loss: 103.9795\n",
      "Epoch: 947, Loss: 103.9795\n",
      "Epoch: 948, Loss: 103.9795\n",
      "Epoch: 949, Loss: 103.9795\n",
      "Epoch: 949, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 950, Loss: 103.9795\n",
      "Epoch: 951, Loss: 103.9795\n",
      "Epoch: 952, Loss: 103.9795\n",
      "Epoch: 953, Loss: 103.9795\n",
      "Epoch: 954, Loss: 103.9795\n",
      "Epoch: 955, Loss: 103.9795\n",
      "Epoch: 956, Loss: 103.9795\n",
      "Epoch: 957, Loss: 103.9795\n",
      "Epoch: 958, Loss: 103.9795\n",
      "Epoch: 959, Loss: 103.9795\n",
      "Epoch: 959, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 960, Loss: 103.9795\n",
      "Epoch: 961, Loss: 103.9795\n",
      "Epoch: 962, Loss: 103.9795\n",
      "Epoch: 963, Loss: 103.9795\n",
      "Epoch: 964, Loss: 103.9795\n",
      "Epoch: 965, Loss: 103.9795\n",
      "Epoch: 966, Loss: 103.9795\n",
      "Epoch: 967, Loss: 103.9795\n",
      "Epoch: 968, Loss: 103.9795\n",
      "Epoch: 969, Loss: 103.9795\n",
      "Epoch: 969, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 970, Loss: 103.9795\n",
      "Epoch: 971, Loss: 103.9795\n",
      "Epoch: 972, Loss: 103.9795\n",
      "Epoch: 973, Loss: 103.9795\n",
      "Epoch: 974, Loss: 103.9795\n",
      "Epoch: 975, Loss: 103.9795\n",
      "Epoch: 976, Loss: 103.9795\n",
      "Epoch: 977, Loss: 103.9795\n",
      "Epoch: 978, Loss: 103.9795\n",
      "Epoch: 979, Loss: 103.9795\n",
      "Epoch: 979, Validation Loss: 102.4322, AUC Score: 0.8354\n",
      "Epoch: 980, Loss: 103.9795\n",
      "Epoch: 981, Loss: 103.9795\n",
      "Epoch: 982, Loss: 103.9795\n",
      "Epoch: 983, Loss: 103.9795\n",
      "Epoch: 984, Loss: 103.9795\n",
      "Epoch: 985, Loss: 103.9795\n",
      "Epoch: 986, Loss: 103.9795\n",
      "Epoch: 987, Loss: 103.9795\n",
      "Epoch: 988, Loss: 103.9797\n",
      "Epoch: 989, Loss: 103.9808\n",
      "Epoch: 989, Validation Loss: 102.4046, AUC Score: 0.8354\n",
      "Epoch: 990, Loss: 103.9842\n",
      "Epoch: 991, Loss: 103.9844\n",
      "Epoch: 992, Loss: 103.9796\n",
      "Epoch: 993, Loss: 103.9828\n",
      "Epoch: 994, Loss: 103.9815\n",
      "Epoch: 995, Loss: 103.9802\n",
      "Epoch: 996, Loss: 103.9823\n",
      "Epoch: 997, Loss: 103.9795\n",
      "Epoch: 998, Loss: 103.9817\n",
      "Epoch: 999, Loss: 103.9798\n",
      "Epoch: 999, Validation Loss: 102.4163, AUC Score: 0.8354\n",
      "Best Score: 0.8354\n"
     ]
    }
   ],
   "source": [
    "# number of features and layers\n",
    "n_features = 2\n",
    "n_layers = 3\n",
    "\n",
    "# Random weight initialization\n",
    "weights = 0.01 * np.random.randn(n_layers, n_features, 3, requires_grad=True)\n",
    "\n",
    "# We create a quantum device with n_features \"wires\" (or qubits)\n",
    "provider = IBMProvider()\n",
    "dev = ibm_dev = qml.device('qiskit.ibmq', wires=n_features, backend='ibmq_qasm_simulator', provider=provider) \n",
    "\n",
    "# train the model\n",
    "best_score, best_weights = train (n_features, n_layers,x_train,y_train_arr,0.01, weights, 1000, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8354234198036303\n"
     ]
    }
   ],
   "source": [
    "print (best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8640271361518534\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "\n",
    "def test(n_features, n_layers,x_test,y_test,w_test, weights):\n",
    "        # Remove grad\n",
    "        X_test = np.array(x_test, requires_grad=False)\n",
    "        Y_test = np.array(y_test, requires_grad=False)\n",
    "        W_test = np.array(w_test, requires_grad=False)\n",
    "\n",
    "        # This will be between -1 and 1, we need to convert to between 0 and 1\n",
    "        y_scores = np.array([classifier(n_features, n_layers,weights, x) for x in X_test])\n",
    "        y_scores = (y_scores + 1) / 2\n",
    "\n",
    "        # Renormalize weights\n",
    "        W_test[Y_test == 1] = (W_test[Y_test == 1] / W_test[Y_test == 1].sum()) * W_test.shape[0] / 2\n",
    "        W_test[Y_test == 0] = (W_test[Y_test == 0] / W_test[Y_test == 0].sum()) * W_test.shape[0] / 2\n",
    "\n",
    "        # Calculate ROC\n",
    "        auc_score = roc_auc_score(y_true=Y_test, y_score=y_scores, sample_weight=W_test)\n",
    "        \n",
    "        return auc_score\n",
    "    \n",
    "auc_score = test(n_features, n_layers,x_test,y_test,w_test, best_weights)\n",
    "print (auc_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
