{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.optimize import NesterovMomentumOptimizer\n",
    "from pennylane.optimize import AdamOptimizer\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.templates.embeddings import AngleEmbedding\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from qiskit_ibm_provider import IBMProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 90548\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Electron_Multi</th>\n",
       "      <th>FatJet1_Eta</th>\n",
       "      <th>FatJet1_Mass</th>\n",
       "      <th>FatJet1_PT</th>\n",
       "      <th>FatJet1_Phi</th>\n",
       "      <th>FatJet1_Tau1</th>\n",
       "      <th>FatJet1_Tau2</th>\n",
       "      <th>FatJet1_Tau3</th>\n",
       "      <th>FatJet1_Tau4</th>\n",
       "      <th>FatJet1_Tau5</th>\n",
       "      <th>...</th>\n",
       "      <th>gen_decay2</th>\n",
       "      <th>gen_decay_filter</th>\n",
       "      <th>gen_filter</th>\n",
       "      <th>gen_label</th>\n",
       "      <th>gen_n_btags</th>\n",
       "      <th>gen_sample</th>\n",
       "      <th>gen_sample_filter</th>\n",
       "      <th>gen_split</th>\n",
       "      <th>gen_weights</th>\n",
       "      <th>gen_xsec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1.408853</td>\n",
       "      <td>15.150869</td>\n",
       "      <td>339.182312</td>\n",
       "      <td>2.350262</td>\n",
       "      <td>1.396943</td>\n",
       "      <td>0.710451</td>\n",
       "      <td>0.109013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>PyDelphes</td>\n",
       "      <td>signal</td>\n",
       "      <td>1</td>\n",
       "      <td>tZFCNC</td>\n",
       "      <td>tZFCNC_PyDelphes</td>\n",
       "      <td>test</td>\n",
       "      <td>7.762202e-09</td>\n",
       "      <td>0.001285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-2.481838</td>\n",
       "      <td>7.208333</td>\n",
       "      <td>247.036240</td>\n",
       "      <td>-2.280740</td>\n",
       "      <td>0.428710</td>\n",
       "      <td>0.205213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>PyDelphes</td>\n",
       "      <td>signal</td>\n",
       "      <td>1</td>\n",
       "      <td>tZFCNC</td>\n",
       "      <td>tZFCNC_PyDelphes</td>\n",
       "      <td>val</td>\n",
       "      <td>7.762202e-09</td>\n",
       "      <td>0.001285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.476267</td>\n",
       "      <td>94.220718</td>\n",
       "      <td>238.014694</td>\n",
       "      <td>-1.788097</td>\n",
       "      <td>94.256210</td>\n",
       "      <td>2.418446</td>\n",
       "      <td>1.585315</td>\n",
       "      <td>1.127324</td>\n",
       "      <td>0.431098</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>PyDelphes</td>\n",
       "      <td>signal</td>\n",
       "      <td>1</td>\n",
       "      <td>tZFCNC</td>\n",
       "      <td>tZFCNC_PyDelphes</td>\n",
       "      <td>train</td>\n",
       "      <td>7.762249e-09</td>\n",
       "      <td>0.001285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.647480</td>\n",
       "      <td>13.459283</td>\n",
       "      <td>230.971832</td>\n",
       "      <td>-1.032663</td>\n",
       "      <td>1.227122</td>\n",
       "      <td>0.467150</td>\n",
       "      <td>0.164008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>PyDelphes</td>\n",
       "      <td>signal</td>\n",
       "      <td>1</td>\n",
       "      <td>tZFCNC</td>\n",
       "      <td>tZFCNC_PyDelphes</td>\n",
       "      <td>train</td>\n",
       "      <td>7.762249e-09</td>\n",
       "      <td>0.001285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2.106436</td>\n",
       "      <td>97.490242</td>\n",
       "      <td>698.399902</td>\n",
       "      <td>-3.059983</td>\n",
       "      <td>36.555862</td>\n",
       "      <td>2.937936</td>\n",
       "      <td>1.799140</td>\n",
       "      <td>1.093004</td>\n",
       "      <td>0.589724</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>PyDelphes</td>\n",
       "      <td>signal</td>\n",
       "      <td>1</td>\n",
       "      <td>tZFCNC</td>\n",
       "      <td>tZFCNC_PyDelphes</td>\n",
       "      <td>train</td>\n",
       "      <td>7.762249e-09</td>\n",
       "      <td>0.001285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Electron_Multi  FatJet1_Eta  FatJet1_Mass  FatJet1_PT  FatJet1_Phi  \\\n",
       "0               2     1.408853     15.150869  339.182312     2.350262   \n",
       "1               1    -2.481838      7.208333  247.036240    -2.280740   \n",
       "2               0     1.476267     94.220718  238.014694    -1.788097   \n",
       "3               1     0.647480     13.459283  230.971832    -1.032663   \n",
       "4               0     2.106436     97.490242  698.399902    -3.059983   \n",
       "\n",
       "   FatJet1_Tau1  FatJet1_Tau2  FatJet1_Tau3  FatJet1_Tau4  FatJet1_Tau5  ...  \\\n",
       "0      1.396943      0.710451      0.109013      0.000000      0.000000  ...   \n",
       "1      0.428710      0.205213      0.000000      0.000000      0.000000  ...   \n",
       "2     94.256210      2.418446      1.585315      1.127324      0.431098  ...   \n",
       "3      1.227122      0.467150      0.164008      0.000000      0.000000  ...   \n",
       "4     36.555862      2.937936      1.799140      1.093004      0.589724  ...   \n",
       "\n",
       "   gen_decay2  gen_decay_filter  gen_filter  gen_label  gen_n_btags  \\\n",
       "0           0              None   PyDelphes     signal            1   \n",
       "1           0              None   PyDelphes     signal            1   \n",
       "2           0              None   PyDelphes     signal            1   \n",
       "3           0              None   PyDelphes     signal            1   \n",
       "4           0              None   PyDelphes     signal            1   \n",
       "\n",
       "   gen_sample  gen_sample_filter  gen_split   gen_weights  gen_xsec  \n",
       "0      tZFCNC   tZFCNC_PyDelphes       test  7.762202e-09  0.001285  \n",
       "1      tZFCNC   tZFCNC_PyDelphes        val  7.762202e-09  0.001285  \n",
       "2      tZFCNC   tZFCNC_PyDelphes      train  7.762249e-09  0.001285  \n",
       "3      tZFCNC   tZFCNC_PyDelphes      train  7.762249e-09  0.001285  \n",
       "4      tZFCNC   tZFCNC_PyDelphes      train  7.762249e-09  0.001285  \n",
       "\n",
       "[5 rows x 83 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the HDF5 file using pandas\n",
    "data_frame_fcnc = pd.read_hdf('fcnc_pythia_sanitised_features.h5')\n",
    "\n",
    "# Get the number of rows\n",
    "num_rows = data_frame_fcnc.shape[0]\n",
    "\n",
    "print('Number of rows: {}'.format(num_rows))\n",
    "\n",
    "# Explore the data\n",
    "data_frame_fcnc.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1002490\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Electron_Multi</th>\n",
       "      <th>FatJet1_Eta</th>\n",
       "      <th>FatJet1_Mass</th>\n",
       "      <th>FatJet1_PT</th>\n",
       "      <th>FatJet1_Phi</th>\n",
       "      <th>FatJet1_Tau1</th>\n",
       "      <th>FatJet1_Tau2</th>\n",
       "      <th>FatJet1_Tau3</th>\n",
       "      <th>FatJet1_Tau4</th>\n",
       "      <th>FatJet1_Tau5</th>\n",
       "      <th>...</th>\n",
       "      <th>gen_decay2</th>\n",
       "      <th>gen_decay_filter</th>\n",
       "      <th>gen_filter</th>\n",
       "      <th>gen_label</th>\n",
       "      <th>gen_n_btags</th>\n",
       "      <th>gen_sample</th>\n",
       "      <th>gen_sample_filter</th>\n",
       "      <th>gen_split</th>\n",
       "      <th>gen_weights</th>\n",
       "      <th>gen_xsec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.988600</td>\n",
       "      <td>52.710262</td>\n",
       "      <td>229.350952</td>\n",
       "      <td>0.728242</td>\n",
       "      <td>36.148926</td>\n",
       "      <td>23.039709</td>\n",
       "      <td>16.949991</td>\n",
       "      <td>14.424411</td>\n",
       "      <td>12.000529</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2L</td>\n",
       "      <td>HT250to500</td>\n",
       "      <td>bkg</td>\n",
       "      <td>1</td>\n",
       "      <td>Zjj</td>\n",
       "      <td>Zjj_HT250to500</td>\n",
       "      <td>train</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>11.9635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.528382</td>\n",
       "      <td>61.115589</td>\n",
       "      <td>315.538910</td>\n",
       "      <td>-0.863614</td>\n",
       "      <td>32.592808</td>\n",
       "      <td>22.366640</td>\n",
       "      <td>16.285843</td>\n",
       "      <td>13.938633</td>\n",
       "      <td>11.180016</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2L</td>\n",
       "      <td>HT250to500</td>\n",
       "      <td>bkg</td>\n",
       "      <td>1</td>\n",
       "      <td>Zjj</td>\n",
       "      <td>Zjj_HT250to500</td>\n",
       "      <td>test</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>11.9635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.479911</td>\n",
       "      <td>98.012802</td>\n",
       "      <td>251.109573</td>\n",
       "      <td>-3.133624</td>\n",
       "      <td>90.252274</td>\n",
       "      <td>33.646885</td>\n",
       "      <td>30.612156</td>\n",
       "      <td>27.973904</td>\n",
       "      <td>23.729696</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2L</td>\n",
       "      <td>HT250to500</td>\n",
       "      <td>bkg</td>\n",
       "      <td>1</td>\n",
       "      <td>Zjj</td>\n",
       "      <td>Zjj_HT250to500</td>\n",
       "      <td>val</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>11.9635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.926899</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>240.909348</td>\n",
       "      <td>0.835656</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2L</td>\n",
       "      <td>HT250to500</td>\n",
       "      <td>bkg</td>\n",
       "      <td>1</td>\n",
       "      <td>Zjj</td>\n",
       "      <td>Zjj_HT250to500</td>\n",
       "      <td>val</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>11.9635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.781194</td>\n",
       "      <td>72.234299</td>\n",
       "      <td>206.020386</td>\n",
       "      <td>-0.320449</td>\n",
       "      <td>48.886372</td>\n",
       "      <td>20.743645</td>\n",
       "      <td>16.572512</td>\n",
       "      <td>13.070706</td>\n",
       "      <td>11.269534</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2L</td>\n",
       "      <td>HT250to500</td>\n",
       "      <td>bkg</td>\n",
       "      <td>1</td>\n",
       "      <td>Zjj</td>\n",
       "      <td>Zjj_HT250to500</td>\n",
       "      <td>val</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>11.9635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Electron_Multi  FatJet1_Eta  FatJet1_Mass  FatJet1_PT  FatJet1_Phi  \\\n",
       "0               2    -1.988600     52.710262  229.350952     0.728242   \n",
       "1               0     0.528382     61.115589  315.538910    -0.863614   \n",
       "2               0     1.479911     98.012802  251.109573    -3.133624   \n",
       "3               2     0.926899     -0.000007  240.909348     0.835656   \n",
       "4               0     0.781194     72.234299  206.020386    -0.320449   \n",
       "\n",
       "   FatJet1_Tau1  FatJet1_Tau2  FatJet1_Tau3  FatJet1_Tau4  FatJet1_Tau5  ...  \\\n",
       "0     36.148926     23.039709     16.949991     14.424411     12.000529  ...   \n",
       "1     32.592808     22.366640     16.285843     13.938633     11.180016  ...   \n",
       "2     90.252274     33.646885     30.612156     27.973904     23.729696  ...   \n",
       "3      0.000000      0.000000      0.000000      0.000000      0.000000  ...   \n",
       "4     48.886372     20.743645     16.572512     13.070706     11.269534  ...   \n",
       "\n",
       "   gen_decay2  gen_decay_filter  gen_filter  gen_label  gen_n_btags  \\\n",
       "0           0                2L  HT250to500        bkg            1   \n",
       "1           0                2L  HT250to500        bkg            1   \n",
       "2           0                2L  HT250to500        bkg            1   \n",
       "3           0                2L  HT250to500        bkg            1   \n",
       "4           0                2L  HT250to500        bkg            1   \n",
       "\n",
       "   gen_sample  gen_sample_filter  gen_split  gen_weights  gen_xsec  \n",
       "0         Zjj     Zjj_HT250to500      train     0.000018   11.9635  \n",
       "1         Zjj     Zjj_HT250to500       test     0.000018   11.9635  \n",
       "2         Zjj     Zjj_HT250to500        val     0.000018   11.9635  \n",
       "3         Zjj     Zjj_HT250to500        val     0.000018   11.9635  \n",
       "4         Zjj     Zjj_HT250to500        val     0.000018   11.9635  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the HDF5 file using pandas\n",
    "data_frame_bkg = pd.read_hdf('bkg_pythia_sanitised_features.h5')\n",
    "\n",
    "# Get the number of rows\n",
    "num_rows = data_frame_bkg.shape[0]\n",
    "\n",
    "print('Number of rows: {}'.format(num_rows))\n",
    "\n",
    "# Explore the data\n",
    "data_frame_bkg.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VQC"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pennylane (V1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the original data into train, validation and test sets for each dataset\n",
    "data_frame_fcnc_train = data_frame_fcnc.loc[data_frame_fcnc['gen_split'] == 'train']\n",
    "data_frame_bkg_train =  data_frame_bkg.loc[data_frame_bkg['gen_split'] == 'train']\n",
    "\n",
    "data_frame_fcnc_test = data_frame_fcnc.loc[data_frame_fcnc['gen_split'] == 'test']\n",
    "data_frame_bkg_test =  data_frame_bkg.loc[data_frame_bkg['gen_split'] == 'test']\n",
    "\n",
    "data_frame_fcnc_val = data_frame_fcnc.loc[data_frame_fcnc['gen_split'] == 'val']\n",
    "data_frame_bkg_val =  data_frame_bkg.loc[data_frame_bkg['gen_split'] == 'val']\n",
    "\n",
    "\n",
    "# get 5'' points of each dataset and  join the datasets (randomly)\n",
    "train_fcnc = data_frame_fcnc_train.sample(n=500)\n",
    "train_bkg = data_frame_bkg_train.sample(n=500)\n",
    "train = pd.concat([train_fcnc, train_bkg])\n",
    "train = train.sample(frac=1)\n",
    "\n",
    "test_fcnc = data_frame_fcnc_test.sample(n=500)\n",
    "test_bkg = data_frame_bkg_test.sample(n=500)\n",
    "test= pd.concat([test_fcnc, test_bkg])\n",
    "test = test.sample(frac=1)\n",
    "\n",
    "val_fnc = data_frame_fcnc_val.sample(n=500)\n",
    "val_bkg = data_frame_bkg_val.sample(n=500)\n",
    "val = pd.concat([val_fnc, val_bkg])\n",
    "val = val.sample(frac=1)\n",
    "\n",
    "# get the weights for each dataset\n",
    "w_train = train[['gen_xsec']]\n",
    "w_test = test[['gen_xsec']]\n",
    "w_val = val[['gen_xsec']]\n",
    "\n",
    "\n",
    "# change the signal and bkg labels to 0 and 1 and get the labels for each dataset\n",
    "train = train.replace(['signal'], 1)\n",
    "train= train.replace(['bkg'], 0)\n",
    "y_train = train[['gen_label']]\n",
    "x_train = train[['MissingET_MET', 'Jet1_BTag']]\n",
    "\n",
    "test = test.replace(['signal'], 1)\n",
    "test= test.replace(['bkg'], 0)\n",
    "y_test = test[['gen_label']]\n",
    "x_test = test[['MissingET_MET', 'Jet1_BTag']]\n",
    "\n",
    "val = val.replace(['signal'], 1)\n",
    "val= val.replace(['bkg'], 0)\n",
    "y_val = val[['gen_label']]\n",
    "x_val = val[['MissingET_MET', 'Jet1_BTag']]\n",
    "\n",
    "y_train_arr = np.concatenate(y_train.values, axis=0 )\n",
    "y_val_arr = np.concatenate( y_val.values, axis=0 )\n",
    "y_test_arr = np.concatenate( y_test.values, axis=0 )\n",
    "\n",
    "# Renormalize data weights\n",
    "w_train[y_train_arr == 1] = (w_train[y_train_arr == 1] / w_train[y_train_arr == 1].sum()) * y_train_arr.shape[0] / 2\n",
    "w_train[y_train_arr == 0] = (w_train[y_train_arr == 0] / w_train[y_train_arr == 0].sum()) * y_train_arr.shape[0] / 2\n",
    "        \n",
    "w_test[y_test_arr == 1] = (w_test[y_test_arr == 1] / w_test[y_test_arr == 1].sum()) * w_test.shape[0] / 2\n",
    "w_test[y_test_arr == 0] = (w_test[y_test_arr == 0] / w_test[y_test_arr == 0].sum()) * w_test.shape[0] / 2\n",
    "        \n",
    "w_val[y_test_arr == 1] = (w_val[y_test_arr == 1] / w_val[y_test_arr == 1].sum()) * w_val.shape[0] / 2\n",
    "w_val[y_test_arr == 0] = (w_val[y_test_arr == 0] / w_val[y_test_arr == 0].sum()) * w_val.shape[0] / 2\n",
    "\n",
    "# Concatenate features\n",
    "X = np.concatenate([x_train, x_test,x_val])\n",
    "\n",
    "# Normalize the data for angle embeding  (Put the data between -pi and pi)\n",
    "X = (((X - X.min()) / (X.max() - X.min())) * 2 - 1) * (np.pi)\n",
    "\n",
    "# Split the features array into train, validation and test sets\n",
    "x_train = X[:1000]\n",
    "x_test = X[1000:2000]\n",
    "x_val = X[2000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy function\n",
    "def accuracy(labels, predictions):\n",
    "\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        if abs(l - p) < 1e-5:\n",
    "            loss = loss + 1\n",
    "    loss = loss / len(labels)\n",
    "\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# loss function\n",
    "def square_loss(labels, predictions):\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        loss = loss + (l - p) ** 2\n",
    "\n",
    "    loss = loss / len(labels)\n",
    "    return loss\n",
    "\n",
    "# quantum circuit function\n",
    "def circuit(n_features, n_layers, weights, x):\n",
    "        # Embedding\n",
    "        \n",
    "        qml.AngleEmbedding(x,range (0, n_features),rotation=\"X\" )\n",
    "\n",
    "        # For every layer\n",
    "        for layer in range(n_layers):\n",
    "            W1 = weights[layer]\n",
    "\n",
    "            # Define Rotations\n",
    "            for i in range(0,n_features):\n",
    "                qml.Rot(W1[i, 0], W1[i, 1], W1[i, 2], wires=i)\n",
    "\n",
    "            # Entanglement\n",
    "            if n_features != 1:\n",
    "                if n_features > 2:\n",
    "                    for i in range(n_features):\n",
    "                        if i == n_features - 1:\n",
    "                            qml.CNOT(wires=[i, 0])\n",
    "                        else:\n",
    "                            qml.CNOT(wires=[i, i + 1])\n",
    "                else:\n",
    "                    qml.CNOT(wires=[1, 0])\n",
    "\n",
    "        return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "# classifier function    \n",
    "def classifier(n_features, n_layers, weights, x):\n",
    "        #c = circuit(n_features, n_layers, weights, x)\n",
    "        dev=qml.device(\"default.qubit\", wires=n_features)\n",
    "        return qml.QNode(circuit, dev)(n_features, n_layers, weights, x)\n",
    "    \n",
    "# cost function    \n",
    "def cost(n_features, n_layers,weights,X,Y,W):  \n",
    "        # Compute predictions\n",
    "        y_scores = [(classifier(n_features, n_layers,weights, x) + 1) / 2 for x in X]\n",
    "\n",
    "        loss = square_loss(Y, y_scores)\n",
    "        loss = loss * W\n",
    "        loss = loss.sum()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "# train step function    \n",
    "def train_step(n_features, n_layers,x_train,y_train, w_train, weights, opt,desc='Training'):\n",
    "        \n",
    "        # Only require grad if necessary\n",
    "        x_train = np.array(x_train, requires_grad=False)\n",
    "        y_train = np.array(y_train, requires_grad=True)\n",
    "        w_train = np.array(w_train, requires_grad=False)\n",
    "\n",
    "        # Compute cost and update weights\n",
    "        weights, loss = opt.step_and_cost(cost, n_features, n_layers,weights, X=x_train, Y=y_train, W=w_train)\n",
    "\n",
    "        return loss, weights\n",
    "    \n",
    "# validation step function\n",
    "def validation_step(n_features, n_layers, x_val, y_val, w_val, weights, best_score, epoch_number, best_score_epoch,best_weights,desc='Validation'):\n",
    "    X_val = np.array(x_val, requires_grad=False)\n",
    "    Y_val = np.array(y_val, requires_grad=False)\n",
    "    W_val = np.array(w_val, requires_grad=False)\n",
    "\n",
    "    y_scores = np.array([classifier(n_features, n_layers, weights, x) for x in X_val])\n",
    "    y_scores = (y_scores + 1) / 2\n",
    "\n",
    "    W_val[Y_val == 1] = (W_val[Y_val == 1] / W_val[Y_val == 1].sum()) * W_val.shape[0] / 2\n",
    "    W_val[Y_val == 0] = (W_val[Y_val == 0] / W_val[Y_val == 0].sum()) * W_val.shape[0] / 2\n",
    "\n",
    "    auc_score = roc_auc_score(y_true=Y_val, y_score=y_scores, sample_weight=W_val)\n",
    "    loss = cost(n_features, n_layers, weights, X_val, Y_val, W_val)\n",
    "\n",
    "\n",
    "    if best_score is None or auc_score > best_score:\n",
    "        best_score = auc_score\n",
    "        best_score_epoch = epoch_number\n",
    "        best_weights = weights\n",
    "\n",
    "    tqdm.write(f\"Epoch: {epoch_number}, Validation Loss: {loss:.4f}, AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "    return best_score, best_score_epoch, best_weights\n",
    "        \n",
    "        \n",
    "        \n",
    "# train function\n",
    "def train(n_features, n_layers, x_train, y_train, learning_rate, weights, max_epochs, epoch_number):\n",
    "    opt = AdamOptimizer(learning_rate)\n",
    "    best_score = None\n",
    "    best_weights = None\n",
    "    best_score_epoch = None\n",
    "\n",
    "    with tqdm(total=max_epochs, desc='Epoch', unit='epoch') as pbar:\n",
    "        for epoch in range(epoch_number, max_epochs):\n",
    "            epoch_number = epoch\n",
    "\n",
    "            loss, nf_nl_weights = train_step(n_features, n_layers, x_train, y_train, w_train, weights, opt, desc='Training')\n",
    "            \n",
    "            # Log variable values using tqdm.write\n",
    "            tqdm.write(f\"Epoch: {epoch_number:}, Loss: {loss:.4f}\")\n",
    "            \n",
    "            \n",
    "            weights = nf_nl_weights[2:]\n",
    "            weights = weights[0]\n",
    "\n",
    "            if epoch_number == max_epochs - 1 or (epoch_number+1)%5==0:\n",
    "                best_score, best_score_epoch, best_weights = validation_step(n_features, n_layers, x_val, y_val, w_val, weights, best_score, epoch_number, best_score_epoch, best_weights,desc='Validation')\n",
    "                # early stopping\n",
    "                if epoch_number - best_score_epoch > 30 and epoch_number > 80:\n",
    "                    tqdm.write(f\"Early stopping at epoch {epoch_number}\")\n",
    "                    break\n",
    "\n",
    "            pbar.update(1)  # Update progress bar\n",
    "        tqdm.write(f\"Best Score: {best_score:.4f}\")            \n",
    "        \n",
    "    return best_score, best_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Default Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa310174e7b44d38af0387a74f59622f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1000 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 510.0425\n",
      "Epoch: 1, Loss: 509.8386\n",
      "Epoch: 2, Loss: 509.2260\n",
      "Epoch: 3, Loss: 508.1480\n",
      "Epoch: 4, Loss: 506.5491\n",
      "Epoch: 4, Validation Loss: 510.0878, AUC Score: 0.2049\n",
      "Epoch: 5, Loss: 504.3978\n",
      "Epoch: 6, Loss: 501.6759\n",
      "Epoch: 7, Loss: 498.3757\n",
      "Epoch: 8, Loss: 494.4996\n",
      "Epoch: 9, Loss: 490.0604\n",
      "Epoch: 9, Validation Loss: 490.7805, AUC Score: 0.2045\n",
      "Epoch: 10, Loss: 485.0823\n",
      "Epoch: 11, Loss: 479.6010\n",
      "Epoch: 12, Loss: 473.6635\n",
      "Epoch: 13, Loss: 467.3264\n",
      "Epoch: 14, Loss: 460.6516\n",
      "Epoch: 14, Validation Loss: 459.4497, AUC Score: 0.2053\n",
      "Epoch: 15, Loss: 453.6963\n",
      "Epoch: 16, Loss: 446.5009\n",
      "Epoch: 17, Loss: 439.0881\n",
      "Epoch: 18, Loss: 431.4735\n",
      "Epoch: 19, Loss: 423.6696\n",
      "Epoch: 19, Validation Loss: 421.4074, AUC Score: 0.2063\n",
      "Epoch: 20, Loss: 415.6888\n",
      "Epoch: 21, Loss: 407.5482\n",
      "Epoch: 22, Loss: 399.2709\n",
      "Epoch: 23, Loss: 390.8836\n",
      "Epoch: 24, Loss: 382.4153\n",
      "Epoch: 24, Validation Loss: 379.1753, AUC Score: 0.2060\n",
      "Epoch: 25, Loss: 373.8958\n",
      "Epoch: 26, Loss: 365.3563\n",
      "Epoch: 27, Loss: 356.8299\n",
      "Epoch: 28, Loss: 348.3527\n",
      "Epoch: 29, Loss: 339.9630\n",
      "Epoch: 29, Validation Loss: 336.0212, AUC Score: 0.2061\n",
      "Epoch: 30, Loss: 331.7012\n",
      "Epoch: 31, Loss: 323.6084\n",
      "Epoch: 32, Loss: 315.7255\n",
      "Epoch: 33, Loss: 308.0925\n",
      "Epoch: 34, Loss: 300.7473\n",
      "Epoch: 34, Validation Loss: 296.7526, AUC Score: 0.2045\n",
      "Epoch: 35, Loss: 293.7244\n",
      "Epoch: 36, Loss: 287.0542\n",
      "Epoch: 37, Loss: 280.7613\n",
      "Epoch: 38, Loss: 274.8642\n",
      "Epoch: 39, Loss: 269.3746\n",
      "Epoch: 39, Validation Loss: 266.1235, AUC Score: 0.3542\n",
      "Epoch: 40, Loss: 264.2965\n",
      "Epoch: 41, Loss: 259.6260\n",
      "Epoch: 42, Loss: 255.3509\n",
      "Epoch: 43, Loss: 251.4513\n",
      "Epoch: 44, Loss: 247.9006\n",
      "Epoch: 44, Validation Loss: 245.6110, AUC Score: 0.7037\n",
      "Epoch: 45, Loss: 244.6686\n",
      "Epoch: 46, Loss: 241.7237\n",
      "Epoch: 47, Loss: 239.0356\n",
      "Epoch: 48, Loss: 236.5774\n",
      "Epoch: 49, Loss: 234.3259\n",
      "Epoch: 49, Validation Loss: 232.4415, AUC Score: 0.7560\n",
      "Epoch: 50, Loss: 232.2615\n",
      "Epoch: 51, Loss: 230.3675\n",
      "Epoch: 52, Loss: 228.6299\n",
      "Epoch: 53, Loss: 227.0366\n",
      "Epoch: 54, Loss: 225.5769\n",
      "Epoch: 54, Validation Loss: 223.6401, AUC Score: 0.7716\n",
      "Epoch: 55, Loss: 224.2405\n",
      "Epoch: 56, Loss: 223.0176\n",
      "Epoch: 57, Loss: 221.8982\n",
      "Epoch: 58, Loss: 220.8727\n",
      "Epoch: 59, Loss: 219.9316\n",
      "Epoch: 59, Validation Loss: 217.7994, AUC Score: 0.7790\n",
      "Epoch: 60, Loss: 219.0659\n",
      "Epoch: 61, Loss: 218.2672\n",
      "Epoch: 62, Loss: 217.5282\n",
      "Epoch: 63, Loss: 216.8422\n",
      "Epoch: 64, Loss: 216.2038\n",
      "Epoch: 64, Validation Loss: 213.9083, AUC Score: 0.7827\n",
      "Epoch: 65, Loss: 215.6081\n",
      "Epoch: 66, Loss: 215.0515\n",
      "Epoch: 67, Loss: 214.5308\n",
      "Epoch: 68, Loss: 214.0431\n",
      "Epoch: 69, Loss: 213.5863\n",
      "Epoch: 69, Validation Loss: 211.2371, AUC Score: 0.7840\n",
      "Epoch: 70, Loss: 213.1579\n",
      "Epoch: 71, Loss: 212.7559\n",
      "Epoch: 72, Loss: 212.3781\n",
      "Epoch: 73, Loss: 212.0223\n",
      "Epoch: 74, Loss: 211.6864\n",
      "Epoch: 74, Validation Loss: 209.3173, AUC Score: 0.7850\n",
      "Epoch: 75, Loss: 211.3683\n",
      "Epoch: 76, Loss: 211.0662\n",
      "Epoch: 77, Loss: 210.7786\n",
      "Epoch: 78, Loss: 210.5041\n",
      "Epoch: 79, Loss: 210.2415\n",
      "Epoch: 79, Validation Loss: 207.8007, AUC Score: 0.7868\n",
      "Epoch: 80, Loss: 209.9898\n",
      "Epoch: 81, Loss: 209.7483\n",
      "Epoch: 82, Loss: 209.5161\n",
      "Epoch: 83, Loss: 209.2924\n",
      "Epoch: 84, Loss: 209.0764\n",
      "Epoch: 84, Validation Loss: 206.5439, AUC Score: 0.7871\n",
      "Epoch: 85, Loss: 208.8674\n",
      "Epoch: 86, Loss: 208.6646\n",
      "Epoch: 87, Loss: 208.4674\n",
      "Epoch: 88, Loss: 208.2754\n",
      "Epoch: 89, Loss: 208.0880\n",
      "Epoch: 89, Validation Loss: 205.5060, AUC Score: 0.7882\n",
      "Epoch: 90, Loss: 207.9050\n",
      "Epoch: 91, Loss: 207.7263\n",
      "Epoch: 92, Loss: 207.5516\n",
      "Epoch: 93, Loss: 207.3807\n",
      "Epoch: 94, Loss: 207.2136\n",
      "Epoch: 94, Validation Loss: 204.6185, AUC Score: 0.7883\n",
      "Epoch: 95, Loss: 207.0499\n",
      "Epoch: 96, Loss: 206.8894\n",
      "Epoch: 97, Loss: 206.7321\n",
      "Epoch: 98, Loss: 206.5777\n",
      "Epoch: 99, Loss: 206.4261\n",
      "Epoch: 99, Validation Loss: 203.7982, AUC Score: 0.7882\n",
      "Epoch: 100, Loss: 206.2773\n",
      "Epoch: 101, Loss: 206.1313\n",
      "Epoch: 102, Loss: 205.9879\n",
      "Epoch: 103, Loss: 205.8472\n",
      "Epoch: 104, Loss: 205.7091\n",
      "Epoch: 104, Validation Loss: 203.0313, AUC Score: 0.7884\n",
      "Epoch: 105, Loss: 205.5735\n",
      "Epoch: 106, Loss: 205.4404\n",
      "Epoch: 107, Loss: 205.3096\n",
      "Epoch: 108, Loss: 205.1812\n",
      "Epoch: 109, Loss: 205.0551\n",
      "Epoch: 109, Validation Loss: 202.3490, AUC Score: 0.7884\n",
      "Epoch: 110, Loss: 204.9313\n",
      "Epoch: 111, Loss: 204.8098\n",
      "Epoch: 112, Loss: 204.6905\n",
      "Epoch: 113, Loss: 204.5735\n",
      "Epoch: 114, Loss: 204.4588\n",
      "Epoch: 114, Validation Loss: 201.7390, AUC Score: 0.7885\n",
      "Epoch: 115, Loss: 204.3462\n",
      "Epoch: 116, Loss: 204.2358\n",
      "Epoch: 117, Loss: 204.1275\n",
      "Epoch: 118, Loss: 204.0214\n",
      "Epoch: 119, Loss: 203.9174\n",
      "Epoch: 119, Validation Loss: 201.1721, AUC Score: 0.7885\n",
      "Epoch: 120, Loss: 203.8155\n",
      "Epoch: 121, Loss: 203.7157\n",
      "Epoch: 122, Loss: 203.6180\n",
      "Epoch: 123, Loss: 203.5223\n",
      "Epoch: 124, Loss: 203.4287\n",
      "Epoch: 124, Validation Loss: 200.6571, AUC Score: 0.7890\n",
      "Epoch: 125, Loss: 203.3371\n",
      "Epoch: 126, Loss: 203.2475\n",
      "Epoch: 127, Loss: 203.1598\n",
      "Epoch: 128, Loss: 203.0742\n",
      "Epoch: 129, Loss: 202.9906\n",
      "Epoch: 129, Validation Loss: 200.2049, AUC Score: 0.7890\n",
      "Epoch: 130, Loss: 202.9089\n",
      "Epoch: 131, Loss: 202.8292\n",
      "Epoch: 132, Loss: 202.7515\n",
      "Epoch: 133, Loss: 202.6757\n",
      "Epoch: 134, Loss: 202.6019\n",
      "Epoch: 134, Validation Loss: 199.8020, AUC Score: 0.7890\n",
      "Epoch: 135, Loss: 202.5301\n",
      "Epoch: 136, Loss: 202.4602\n",
      "Epoch: 137, Loss: 202.3923\n",
      "Epoch: 138, Loss: 202.3263\n",
      "Epoch: 139, Loss: 202.2623\n",
      "Epoch: 139, Validation Loss: 199.4439, AUC Score: 0.7891\n",
      "Epoch: 140, Loss: 202.2002\n",
      "Epoch: 141, Loss: 202.1401\n",
      "Epoch: 142, Loss: 202.0819\n",
      "Epoch: 143, Loss: 202.0256\n",
      "Epoch: 144, Loss: 201.9713\n",
      "Epoch: 144, Validation Loss: 199.1391, AUC Score: 0.7891\n",
      "Epoch: 145, Loss: 201.9189\n",
      "Epoch: 146, Loss: 201.8684\n",
      "Epoch: 147, Loss: 201.8198\n",
      "Epoch: 148, Loss: 201.7730\n",
      "Epoch: 149, Loss: 201.7282\n",
      "Epoch: 149, Validation Loss: 198.8855, AUC Score: 0.7883\n",
      "Epoch: 150, Loss: 201.6852\n",
      "Epoch: 151, Loss: 201.6441\n",
      "Epoch: 152, Loss: 201.6048\n",
      "Epoch: 153, Loss: 201.5672\n",
      "Epoch: 154, Loss: 201.5315\n",
      "Epoch: 154, Validation Loss: 198.6767, AUC Score: 0.7880\n",
      "Epoch: 155, Loss: 201.4975\n",
      "Epoch: 156, Loss: 201.4651\n",
      "Epoch: 157, Loss: 201.4345\n",
      "Epoch: 158, Loss: 201.4054\n",
      "Epoch: 159, Loss: 201.3780\n",
      "Epoch: 159, Validation Loss: 198.5125, AUC Score: 0.7879\n",
      "Epoch: 160, Loss: 201.3521\n",
      "Epoch: 161, Loss: 201.3276\n",
      "Epoch: 162, Loss: 201.3046\n",
      "Epoch: 163, Loss: 201.2830\n",
      "Epoch: 164, Loss: 201.2628\n",
      "Epoch: 164, Validation Loss: 198.3898, AUC Score: 0.7875\n",
      "Epoch: 165, Loss: 201.2438\n",
      "Epoch: 166, Loss: 201.2260\n",
      "Epoch: 167, Loss: 201.2094\n",
      "Epoch: 168, Loss: 201.1939\n",
      "Epoch: 169, Loss: 201.1795\n",
      "Epoch: 169, Validation Loss: 198.2998, AUC Score: 0.7874\n",
      "Epoch: 170, Loss: 201.1660\n",
      "Epoch: 171, Loss: 201.1535\n",
      "Epoch: 172, Loss: 201.1419\n",
      "Epoch: 173, Loss: 201.1311\n",
      "Epoch: 174, Loss: 201.1211\n",
      "Epoch: 174, Validation Loss: 198.2354, AUC Score: 0.7874\n",
      "Early stopping at epoch 174\n",
      "Best Score: 0.7891\n"
     ]
    }
   ],
   "source": [
    "# number of features and layers\n",
    "n_features = 2\n",
    "n_layers = 3\n",
    "\n",
    "# Random weight initialization\n",
    "weights = 0.01 * np.random.randn(n_layers, n_features, 3, requires_grad=True)\n",
    "\n",
    "# We create a quantum device with n_features \"wires\" (or qubits)\n",
    "dev = qml.device('default.qubit', wires=n_features)\n",
    "\n",
    "# train the model\n",
    "best_score, best_weights = train (n_features, n_layers,x_train,y_train_arr,0.01, weights, 1000, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7890943370681786\n"
     ]
    }
   ],
   "source": [
    "print (best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7568898558963438\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "\n",
    "def test(n_features, n_layers,x_test,y_test,w_test, weights):\n",
    "        # Remove grad\n",
    "        X_test = np.array(x_test, requires_grad=False)\n",
    "        Y_test = np.array(y_test, requires_grad=False)\n",
    "        W_test = np.array(w_test, requires_grad=False)\n",
    "\n",
    "        # This will be between -1 and 1, we need to convert to between 0 and 1\n",
    "        y_scores = np.array([classifier(n_features, n_layers,weights, x) for x in X_test])\n",
    "        y_scores = (y_scores + 1) / 2\n",
    "\n",
    "        # Renormalize weights\n",
    "        W_test[Y_test == 1] = (W_test[Y_test == 1] / W_test[Y_test == 1].sum()) * W_test.shape[0] / 2\n",
    "        W_test[Y_test == 0] = (W_test[Y_test == 0] / W_test[Y_test == 0].sum()) * W_test.shape[0] / 2\n",
    "\n",
    "        # Calculate ROC\n",
    "        auc_score = roc_auc_score(y_true=Y_test, y_score=y_scores, sample_weight=W_test)\n",
    "        \n",
    "        return auc_score\n",
    "    \n",
    "auc_score = test(n_features, n_layers,x_test,y_test,w_test, best_weights)\n",
    "print (auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### default.qubit.tf Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-12 10:29:57.323912: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-07-12 10:29:57.325997: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-12 10:29:57.362884: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-12 10:29:57.363899: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-12 10:29:57.836434: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-07-12 10:29:58.330266: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1960] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "658e2d1d47434ea786b7cd64bd69856f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1000 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 509.9651\n",
      "Epoch: 1, Loss: 509.3178\n",
      "Epoch: 2, Loss: 508.1373\n",
      "Epoch: 3, Loss: 506.3903\n",
      "Epoch: 4, Loss: 504.0652\n",
      "Epoch: 4, Validation Loss: 506.8460, AUC Score: 0.2050\n",
      "Epoch: 5, Loss: 501.1568\n",
      "Epoch: 6, Loss: 497.6660\n",
      "Epoch: 7, Loss: 493.6026\n",
      "Epoch: 8, Loss: 488.9864\n",
      "Epoch: 9, Loss: 483.8470\n",
      "Epoch: 9, Validation Loss: 483.9308, AUC Score: 0.2040\n",
      "Epoch: 10, Loss: 478.2245\n",
      "Epoch: 11, Loss: 472.1687\n",
      "Epoch: 12, Loss: 465.7373\n",
      "Epoch: 13, Loss: 458.9918\n",
      "Epoch: 14, Loss: 451.9898\n",
      "Epoch: 14, Validation Loss: 450.5407, AUC Score: 0.2048\n",
      "Epoch: 15, Loss: 444.7758\n",
      "Epoch: 16, Loss: 437.3762\n",
      "Epoch: 17, Loss: 429.7998\n",
      "Epoch: 18, Loss: 422.0462\n",
      "Epoch: 19, Loss: 414.1195\n",
      "Epoch: 19, Validation Loss: 411.7187, AUC Score: 0.2063\n",
      "Epoch: 20, Loss: 406.0341\n",
      "Epoch: 21, Loss: 397.8136\n",
      "Epoch: 22, Loss: 389.4869\n",
      "Epoch: 23, Loss: 381.0853\n",
      "Epoch: 24, Loss: 372.6400\n",
      "Epoch: 24, Validation Loss: 369.3196, AUC Score: 0.2059\n",
      "Epoch: 25, Loss: 364.1818\n",
      "Epoch: 26, Loss: 355.7426\n",
      "Epoch: 27, Loss: 347.3559\n",
      "Epoch: 28, Loss: 339.0588\n",
      "Epoch: 29, Loss: 330.8909\n",
      "Epoch: 29, Validation Loss: 326.9885, AUC Score: 0.2056\n",
      "Epoch: 30, Loss: 322.8936\n",
      "Epoch: 31, Loss: 315.1081\n",
      "Epoch: 32, Loss: 307.5742\n",
      "Epoch: 33, Loss: 300.3290\n",
      "Epoch: 34, Loss: 293.4059\n",
      "Epoch: 34, Validation Loss: 289.6465, AUC Score: 0.2035\n",
      "Epoch: 35, Loss: 286.8336\n",
      "Epoch: 36, Loss: 280.6353\n",
      "Epoch: 37, Loss: 274.8278\n",
      "Epoch: 38, Loss: 269.4209\n",
      "Epoch: 39, Loss: 264.4169\n",
      "Epoch: 39, Validation Loss: 261.4963, AUC Score: 0.4968\n",
      "Epoch: 40, Loss: 259.8099\n",
      "Epoch: 41, Loss: 255.5867\n",
      "Epoch: 42, Loss: 251.7271\n",
      "Epoch: 43, Loss: 248.2055\n",
      "Epoch: 44, Loss: 244.9934\n",
      "Epoch: 44, Validation Loss: 242.9045, AUC Score: 0.7203\n",
      "Epoch: 45, Loss: 242.0612\n",
      "Epoch: 46, Loss: 239.3805\n",
      "Epoch: 47, Loss: 236.9255\n",
      "Epoch: 48, Loss: 234.6740\n",
      "Epoch: 49, Loss: 232.6069\n",
      "Epoch: 49, Validation Loss: 230.7726, AUC Score: 0.7604\n",
      "Epoch: 50, Loss: 230.7083\n",
      "Epoch: 51, Loss: 228.9647\n",
      "Epoch: 52, Loss: 227.3642\n",
      "Epoch: 53, Loss: 225.8960\n",
      "Epoch: 54, Loss: 224.5498\n",
      "Epoch: 54, Validation Loss: 222.6049, AUC Score: 0.7728\n",
      "Epoch: 55, Loss: 223.3156\n",
      "Epoch: 56, Loss: 222.1836\n",
      "Epoch: 57, Loss: 221.1440\n",
      "Epoch: 58, Loss: 220.1877\n",
      "Epoch: 59, Loss: 219.3059\n",
      "Epoch: 59, Validation Loss: 217.1571, AUC Score: 0.7796\n",
      "Epoch: 60, Loss: 218.4905\n",
      "Epoch: 61, Loss: 217.7344\n",
      "Epoch: 62, Loss: 217.0313\n",
      "Epoch: 63, Loss: 216.3761\n",
      "Epoch: 64, Loss: 215.7642\n",
      "Epoch: 64, Validation Loss: 213.4752, AUC Score: 0.7830\n",
      "Epoch: 65, Loss: 215.1920\n",
      "Epoch: 66, Loss: 214.6563\n",
      "Epoch: 67, Loss: 214.1544\n",
      "Epoch: 68, Loss: 213.6839\n",
      "Epoch: 69, Loss: 213.2423\n",
      "Epoch: 69, Validation Loss: 210.9113, AUC Score: 0.7843\n",
      "Epoch: 70, Loss: 212.8275\n",
      "Epoch: 71, Loss: 212.4370\n",
      "Epoch: 72, Loss: 212.0686\n",
      "Epoch: 73, Loss: 211.7202\n",
      "Epoch: 74, Loss: 211.3897\n",
      "Epoch: 74, Validation Loss: 209.0185, AUC Score: 0.7856\n",
      "Epoch: 75, Loss: 211.0754\n",
      "Epoch: 76, Loss: 210.7756\n",
      "Epoch: 77, Loss: 210.4891\n",
      "Epoch: 78, Loss: 210.2147\n",
      "Epoch: 79, Loss: 209.9515\n",
      "Epoch: 79, Validation Loss: 207.4899, AUC Score: 0.7864\n",
      "Epoch: 80, Loss: 209.6986\n",
      "Epoch: 81, Loss: 209.4553\n",
      "Epoch: 82, Loss: 209.2205\n",
      "Epoch: 83, Loss: 208.9937\n",
      "Epoch: 84, Loss: 208.7739\n",
      "Epoch: 84, Validation Loss: 206.2223, AUC Score: 0.7871\n",
      "Epoch: 85, Loss: 208.5605\n",
      "Epoch: 86, Loss: 208.3528\n",
      "Epoch: 87, Loss: 208.1504\n",
      "Epoch: 88, Loss: 207.9529\n",
      "Epoch: 89, Loss: 207.7600\n",
      "Epoch: 89, Validation Loss: 205.1670, AUC Score: 0.7872\n",
      "Epoch: 90, Loss: 207.5714\n",
      "Epoch: 91, Loss: 207.3871\n",
      "Epoch: 92, Loss: 207.2069\n",
      "Epoch: 93, Loss: 207.0306\n",
      "Epoch: 94, Loss: 206.8581\n",
      "Epoch: 94, Validation Loss: 204.2501, AUC Score: 0.7876\n",
      "Epoch: 95, Loss: 206.6891\n",
      "Epoch: 96, Loss: 206.5234\n",
      "Epoch: 97, Loss: 206.3611\n",
      "Epoch: 98, Loss: 206.2018\n",
      "Epoch: 99, Loss: 206.0457\n",
      "Epoch: 99, Validation Loss: 203.4004, AUC Score: 0.7877\n",
      "Epoch: 100, Loss: 205.8926\n",
      "Epoch: 101, Loss: 205.7424\n",
      "Epoch: 102, Loss: 205.5953\n",
      "Epoch: 103, Loss: 205.4511\n",
      "Epoch: 104, Loss: 205.3098\n",
      "Epoch: 104, Validation Loss: 202.6163, AUC Score: 0.7878\n",
      "Epoch: 105, Loss: 205.1713\n",
      "Epoch: 106, Loss: 205.0355\n",
      "Epoch: 107, Loss: 204.9025\n",
      "Epoch: 108, Loss: 204.7721\n",
      "Epoch: 109, Loss: 204.6443\n",
      "Epoch: 109, Validation Loss: 201.9239, AUC Score: 0.7882\n",
      "Epoch: 110, Loss: 204.5192\n",
      "Epoch: 111, Loss: 204.3967\n",
      "Epoch: 112, Loss: 204.2767\n",
      "Epoch: 113, Loss: 204.1594\n",
      "Epoch: 114, Loss: 204.0447\n",
      "Epoch: 114, Validation Loss: 201.3108, AUC Score: 0.7883\n",
      "Epoch: 115, Loss: 203.9325\n",
      "Epoch: 116, Loss: 203.8228\n",
      "Epoch: 117, Loss: 203.7156\n",
      "Epoch: 118, Loss: 203.6109\n",
      "Epoch: 119, Loss: 203.5087\n",
      "Epoch: 119, Validation Loss: 200.7517, AUC Score: 0.7883\n",
      "Epoch: 120, Loss: 203.4089\n",
      "Epoch: 121, Loss: 203.3117\n",
      "Epoch: 122, Loss: 203.2168\n",
      "Epoch: 123, Loss: 203.1244\n",
      "Epoch: 124, Loss: 203.0343\n",
      "Epoch: 124, Validation Loss: 200.2518, AUC Score: 0.7884\n",
      "Epoch: 125, Loss: 202.9467\n",
      "Epoch: 126, Loss: 202.8614\n",
      "Epoch: 127, Loss: 202.7784\n",
      "Epoch: 128, Loss: 202.6978\n",
      "Epoch: 129, Loss: 202.6195\n",
      "Epoch: 129, Validation Loss: 199.8219, AUC Score: 0.7884\n",
      "Epoch: 130, Loss: 202.5435\n",
      "Epoch: 131, Loss: 202.4698\n",
      "Epoch: 132, Loss: 202.3985\n",
      "Epoch: 133, Loss: 202.3294\n",
      "Epoch: 134, Loss: 202.2626\n",
      "Epoch: 134, Validation Loss: 199.4528, AUC Score: 0.7880\n",
      "Epoch: 135, Loss: 202.1980\n",
      "Epoch: 136, Loss: 202.1357\n",
      "Epoch: 137, Loss: 202.0756\n",
      "Epoch: 138, Loss: 202.0178\n",
      "Epoch: 139, Loss: 201.9621\n",
      "Epoch: 139, Validation Loss: 199.1357, AUC Score: 0.7880\n",
      "Epoch: 140, Loss: 201.9086\n",
      "Epoch: 141, Loss: 201.8572\n",
      "Epoch: 142, Loss: 201.8080\n",
      "Epoch: 143, Loss: 201.7609\n",
      "Epoch: 144, Loss: 201.7158\n",
      "Epoch: 144, Validation Loss: 198.8747, AUC Score: 0.7879\n",
      "Epoch: 145, Loss: 201.6727\n",
      "Epoch: 146, Loss: 201.6317\n",
      "Epoch: 147, Loss: 201.5926\n",
      "Epoch: 148, Loss: 201.5554\n",
      "Epoch: 149, Loss: 201.5201\n",
      "Epoch: 149, Validation Loss: 198.6690, AUC Score: 0.7875\n",
      "Epoch: 150, Loss: 201.4866\n",
      "Epoch: 151, Loss: 201.4549\n",
      "Epoch: 152, Loss: 201.4250\n",
      "Epoch: 153, Loss: 201.3967\n",
      "Epoch: 154, Loss: 201.3700\n",
      "Epoch: 154, Validation Loss: 198.5093, AUC Score: 0.7874\n",
      "Epoch: 155, Loss: 201.3449\n",
      "Epoch: 156, Loss: 201.3214\n",
      "Epoch: 157, Loss: 201.2992\n",
      "Epoch: 158, Loss: 201.2785\n",
      "Epoch: 159, Loss: 201.2590\n",
      "Epoch: 159, Validation Loss: 198.3884, AUC Score: 0.7874\n",
      "Early stopping at epoch 159\n",
      "Best Score: 0.7884\n"
     ]
    }
   ],
   "source": [
    "# number of features and layers\n",
    "n_features = 2\n",
    "n_layers = 3\n",
    "\n",
    "# Random weight initialization\n",
    "weights = 0.01 * np.random.randn(n_layers, n_features, 3, requires_grad=True)\n",
    "\n",
    "# We create a quantum device with n_features \"wires\" (or qubits) \n",
    "dev =qml.device('default.qubit.tf', wires=n_features) \n",
    " \n",
    "# train the model\n",
    "best_score, best_weights = train (n_features, n_layers,x_train,y_train_arr,0.01, weights, 1000, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7884000148725709\n"
     ]
    }
   ],
   "source": [
    "print (best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7571378334842642\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "\n",
    "def test(n_features, n_layers,x_test,y_test,w_test, weights):\n",
    "        # Remove grad\n",
    "        X_test = np.array(x_test, requires_grad=False)\n",
    "        Y_test = np.array(y_test, requires_grad=False)\n",
    "        W_test = np.array(w_test, requires_grad=False)\n",
    "\n",
    "        # This will be between -1 and 1, we need to convert to between 0 and 1\n",
    "        y_scores = np.array([classifier(n_features, n_layers,weights, x) for x in X_test])\n",
    "        y_scores = (y_scores + 1) / 2\n",
    "\n",
    "        # Renormalize weights\n",
    "        W_test[Y_test == 1] = (W_test[Y_test == 1] / W_test[Y_test == 1].sum()) * W_test.shape[0] / 2\n",
    "        W_test[Y_test == 0] = (W_test[Y_test == 0] / W_test[Y_test == 0].sum()) * W_test.shape[0] / 2\n",
    "\n",
    "        # Calculate ROC\n",
    "        auc_score = roc_auc_score(y_true=Y_test, y_score=y_scores, sample_weight=W_test)\n",
    "        \n",
    "        return auc_score\n",
    "    \n",
    "auc_score = test(n_features, n_layers,x_test,y_test,w_test, best_weights)\n",
    "print (auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### default.qubit.autograd Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6704f443e9140db8cdd2a9d48ff7384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1000 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 509.9799\n",
      "Epoch: 1, Loss: 509.3251\n",
      "Epoch: 2, Loss: 508.1497\n",
      "Epoch: 3, Loss: 506.4076\n",
      "Epoch: 4, Loss: 504.0879\n",
      "Epoch: 4, Validation Loss: 506.8911, AUC Score: 0.2050\n",
      "Epoch: 5, Loss: 501.1887\n",
      "Epoch: 6, Loss: 497.7149\n",
      "Epoch: 7, Loss: 493.6801\n",
      "Epoch: 8, Loss: 489.1062\n",
      "Epoch: 9, Loss: 484.0243\n",
      "Epoch: 9, Validation Loss: 484.2152, AUC Score: 0.2048\n",
      "Epoch: 10, Loss: 478.4757\n",
      "Epoch: 11, Loss: 472.5115\n",
      "Epoch: 12, Loss: 466.1895\n",
      "Epoch: 13, Loss: 459.5695\n",
      "Epoch: 14, Loss: 452.7036\n",
      "Epoch: 14, Validation Loss: 451.4496, AUC Score: 0.2055\n",
      "Epoch: 15, Loss: 445.6253\n",
      "Epoch: 16, Loss: 438.3510\n",
      "Epoch: 17, Loss: 430.8926\n",
      "Epoch: 18, Loss: 423.2593\n",
      "Epoch: 19, Loss: 415.4589\n",
      "Epoch: 19, Validation Loss: 413.2539, AUC Score: 0.2065\n",
      "Epoch: 20, Loss: 407.5015\n",
      "Epoch: 21, Loss: 399.4032\n",
      "Epoch: 22, Loss: 391.1853\n",
      "Epoch: 23, Loss: 382.8728\n",
      "Epoch: 24, Loss: 374.4922\n",
      "Epoch: 24, Validation Loss: 371.2985, AUC Score: 0.2062\n",
      "Epoch: 25, Loss: 366.0723\n",
      "Epoch: 26, Loss: 357.6441\n",
      "Epoch: 27, Loss: 349.2430\n",
      "Epoch: 28, Loss: 340.9078\n",
      "Epoch: 29, Loss: 332.6802\n",
      "Epoch: 29, Validation Loss: 328.7609, AUC Score: 0.2058\n",
      "Epoch: 30, Loss: 324.6019\n",
      "Epoch: 31, Loss: 316.7144\n",
      "Epoch: 32, Loss: 309.0581\n",
      "Epoch: 33, Loss: 301.6717\n",
      "Epoch: 34, Loss: 294.5907\n",
      "Epoch: 34, Validation Loss: 290.6683, AUC Score: 0.2039\n",
      "Epoch: 35, Loss: 287.8469\n",
      "Epoch: 36, Loss: 281.4664\n",
      "Epoch: 37, Loss: 275.4699\n",
      "Epoch: 38, Loss: 269.8712\n",
      "Epoch: 39, Loss: 264.6772\n",
      "Epoch: 39, Validation Loss: 261.5139, AUC Score: 0.4723\n",
      "Epoch: 40, Loss: 259.8874\n",
      "Epoch: 41, Loss: 255.4934\n",
      "Epoch: 42, Loss: 251.4796\n",
      "Epoch: 43, Loss: 247.8240\n",
      "Epoch: 44, Loss: 244.4996\n",
      "Epoch: 44, Validation Loss: 242.2332, AUC Score: 0.7244\n",
      "Epoch: 45, Loss: 241.4767\n",
      "Epoch: 46, Loss: 238.7250\n",
      "Epoch: 47, Loss: 236.2152\n",
      "Epoch: 48, Loss: 233.9210\n",
      "Epoch: 49, Loss: 231.8191\n",
      "Epoch: 49, Validation Loss: 229.9053, AUC Score: 0.7626\n",
      "Epoch: 50, Loss: 229.8903\n",
      "Epoch: 51, Loss: 228.1182\n",
      "Epoch: 52, Loss: 226.4895\n",
      "Epoch: 53, Loss: 224.9929\n",
      "Epoch: 54, Loss: 223.6183\n",
      "Epoch: 54, Validation Loss: 221.6320, AUC Score: 0.7736\n",
      "Epoch: 55, Loss: 222.3568\n",
      "Epoch: 56, Loss: 221.1999\n",
      "Epoch: 57, Loss: 220.1394\n",
      "Epoch: 58, Loss: 219.1675\n",
      "Epoch: 59, Loss: 218.2762\n",
      "Epoch: 59, Validation Loss: 216.0902, AUC Score: 0.7807\n",
      "Epoch: 60, Loss: 217.4578\n",
      "Epoch: 61, Loss: 216.7051\n",
      "Epoch: 62, Loss: 216.0111\n",
      "Epoch: 63, Loss: 215.3695\n",
      "Epoch: 64, Loss: 214.7746\n",
      "Epoch: 64, Validation Loss: 212.4186, AUC Score: 0.7834\n",
      "Epoch: 65, Loss: 214.2216\n",
      "Epoch: 66, Loss: 213.7062\n",
      "Epoch: 67, Loss: 213.2249\n",
      "Epoch: 68, Loss: 212.7748\n",
      "Epoch: 69, Loss: 212.3533\n",
      "Epoch: 69, Validation Loss: 209.9344, AUC Score: 0.7848\n",
      "Epoch: 70, Loss: 211.9583\n",
      "Epoch: 71, Loss: 211.5876\n",
      "Epoch: 72, Loss: 211.2392\n",
      "Epoch: 73, Loss: 210.9113\n",
      "Epoch: 74, Loss: 210.6017\n",
      "Epoch: 74, Validation Loss: 208.1730, AUC Score: 0.7863\n",
      "Epoch: 75, Loss: 210.3086\n",
      "Epoch: 76, Loss: 210.0301\n",
      "Epoch: 77, Loss: 209.7646\n",
      "Epoch: 78, Loss: 209.5108\n",
      "Epoch: 79, Loss: 209.2676\n",
      "Epoch: 79, Validation Loss: 206.7795, AUC Score: 0.7871\n",
      "Epoch: 80, Loss: 209.0342\n",
      "Epoch: 81, Loss: 208.8098\n",
      "Epoch: 82, Loss: 208.5937\n",
      "Epoch: 83, Loss: 208.3853\n",
      "Epoch: 84, Loss: 208.1838\n",
      "Epoch: 84, Validation Loss: 205.6106, AUC Score: 0.7876\n",
      "Epoch: 85, Loss: 207.9887\n",
      "Epoch: 86, Loss: 207.7992\n",
      "Epoch: 87, Loss: 207.6149\n",
      "Epoch: 88, Loss: 207.4353\n",
      "Epoch: 89, Loss: 207.2601\n",
      "Epoch: 89, Validation Loss: 204.6440, AUC Score: 0.7875\n",
      "Epoch: 90, Loss: 207.0890\n",
      "Epoch: 91, Loss: 206.9220\n",
      "Epoch: 92, Loss: 206.7589\n",
      "Epoch: 93, Loss: 206.5995\n",
      "Epoch: 94, Loss: 206.4437\n",
      "Epoch: 94, Validation Loss: 203.8187, AUC Score: 0.7876\n",
      "Epoch: 95, Loss: 206.2913\n",
      "Epoch: 96, Loss: 206.1422\n",
      "Epoch: 97, Loss: 205.9961\n",
      "Epoch: 98, Loss: 205.8530\n",
      "Epoch: 99, Loss: 205.7127\n",
      "Epoch: 99, Validation Loss: 203.0569, AUC Score: 0.7877\n",
      "Epoch: 100, Loss: 205.5752\n",
      "Epoch: 101, Loss: 205.4405\n",
      "Epoch: 102, Loss: 205.3085\n",
      "Epoch: 103, Loss: 205.1792\n",
      "Epoch: 104, Loss: 205.0525\n",
      "Epoch: 104, Validation Loss: 202.3521, AUC Score: 0.7878\n",
      "Epoch: 105, Loss: 204.9283\n",
      "Epoch: 106, Loss: 204.8065\n",
      "Epoch: 107, Loss: 204.6872\n",
      "Epoch: 108, Loss: 204.5702\n",
      "Epoch: 109, Loss: 204.4555\n",
      "Epoch: 109, Validation Loss: 201.7320, AUC Score: 0.7886\n",
      "Epoch: 110, Loss: 204.3431\n",
      "Epoch: 111, Loss: 204.2330\n",
      "Epoch: 112, Loss: 204.1251\n",
      "Epoch: 113, Loss: 204.0195\n",
      "Epoch: 114, Loss: 203.9161\n",
      "Epoch: 114, Validation Loss: 201.1808, AUC Score: 0.7889\n",
      "Epoch: 115, Loss: 203.8149\n",
      "Epoch: 116, Loss: 203.7158\n",
      "Epoch: 117, Loss: 203.6188\n",
      "Epoch: 118, Loss: 203.5240\n",
      "Epoch: 119, Loss: 203.4312\n",
      "Epoch: 119, Validation Loss: 200.6730, AUC Score: 0.7890\n",
      "Epoch: 120, Loss: 203.3405\n",
      "Epoch: 121, Loss: 203.2519\n",
      "Epoch: 122, Loss: 203.1654\n",
      "Epoch: 123, Loss: 203.0808\n",
      "Epoch: 124, Loss: 202.9983\n",
      "Epoch: 124, Validation Loss: 200.2172, AUC Score: 0.7884\n",
      "Epoch: 125, Loss: 202.9178\n",
      "Epoch: 126, Loss: 202.8392\n",
      "Epoch: 127, Loss: 202.7625\n",
      "Epoch: 128, Loss: 202.6879\n",
      "Epoch: 129, Loss: 202.6151\n",
      "Epoch: 129, Validation Loss: 199.8214, AUC Score: 0.7884\n",
      "Epoch: 130, Loss: 202.5443\n",
      "Epoch: 131, Loss: 202.4754\n",
      "Epoch: 132, Loss: 202.4085\n",
      "Epoch: 133, Loss: 202.3435\n",
      "Epoch: 134, Loss: 202.2803\n",
      "Epoch: 134, Validation Loss: 199.4740, AUC Score: 0.7884\n",
      "Epoch: 135, Loss: 202.2191\n",
      "Epoch: 136, Loss: 202.1598\n",
      "Epoch: 137, Loss: 202.1023\n",
      "Epoch: 138, Loss: 202.0468\n",
      "Epoch: 139, Loss: 201.9931\n",
      "Epoch: 139, Validation Loss: 199.1701, AUC Score: 0.7883\n",
      "Epoch: 140, Loss: 201.9413\n",
      "Epoch: 141, Loss: 201.8913\n",
      "Epoch: 142, Loss: 201.8432\n",
      "Epoch: 143, Loss: 201.7969\n",
      "Epoch: 144, Loss: 201.7524\n",
      "Epoch: 144, Validation Loss: 198.9158, AUC Score: 0.7880\n",
      "Epoch: 145, Loss: 201.7097\n",
      "Epoch: 146, Loss: 201.6688\n",
      "Epoch: 147, Loss: 201.6296\n",
      "Epoch: 148, Loss: 201.5922\n",
      "Epoch: 149, Loss: 201.5565\n",
      "Epoch: 149, Validation Loss: 198.7097, AUC Score: 0.7875\n",
      "Epoch: 150, Loss: 201.5225\n",
      "Epoch: 151, Loss: 201.4901\n",
      "Epoch: 152, Loss: 201.4594\n",
      "Epoch: 153, Loss: 201.4303\n",
      "Epoch: 154, Loss: 201.4027\n",
      "Epoch: 154, Validation Loss: 198.5449, AUC Score: 0.7875\n",
      "Early stopping at epoch 154\n",
      "Best Score: 0.7890\n"
     ]
    }
   ],
   "source": [
    "# number of features and layers\n",
    "n_features = 2\n",
    "n_layers = 3\n",
    "\n",
    "# Random weight initialization\n",
    "weights = 0.01 * np.random.randn(n_layers, n_features, 3, requires_grad=True)\n",
    "\n",
    "# We create a quantum device with n_features \"wires\" (or qubits) \n",
    "dev =qml.device('default.qubit.autograd', wires=n_features) \n",
    " \n",
    "# train the model\n",
    "best_score, best_weights = train (n_features, n_layers,x_train,y_train_arr,0.01, weights, 1000, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7890060504725337\n"
     ]
    }
   ],
   "source": [
    "print (best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7569029164302329\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "\n",
    "def test(n_features, n_layers,x_test,y_test,w_test, weights):\n",
    "        # Remove grad\n",
    "        X_test = np.array(x_test, requires_grad=False)\n",
    "        Y_test = np.array(y_test, requires_grad=False)\n",
    "        W_test = np.array(w_test, requires_grad=False)\n",
    "\n",
    "        # This will be between -1 and 1, we need to convert to between 0 and 1\n",
    "        y_scores = np.array([classifier(n_features, n_layers,weights, x) for x in X_test])\n",
    "        y_scores = (y_scores + 1) / 2\n",
    "\n",
    "        # Renormalize weights\n",
    "        W_test[Y_test == 1] = (W_test[Y_test == 1] / W_test[Y_test == 1].sum()) * W_test.shape[0] / 2\n",
    "        W_test[Y_test == 0] = (W_test[Y_test == 0] / W_test[Y_test == 0].sum()) * W_test.shape[0] / 2\n",
    "\n",
    "        # Calculate ROC\n",
    "        auc_score = roc_auc_score(y_true=Y_test, y_score=y_scores, sample_weight=W_test)\n",
    "        \n",
    "        return auc_score\n",
    "    \n",
    "auc_score = test(n_features, n_layers,x_test,y_test,w_test, best_weights)\n",
    "print (auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### qiskit.ibmq Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "731940683b02423d9da3e9f0bd3a9e5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1000 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 506.2308\n",
      "Epoch: 1, Loss: 505.6761\n",
      "Epoch: 2, Loss: 504.6832\n",
      "Epoch: 3, Loss: 503.1412\n",
      "Epoch: 4, Loss: 501.0252\n",
      "Epoch: 4, Validation Loss: 490.5050, AUC Score: 0.2402\n",
      "Epoch: 5, Loss: 498.3190\n",
      "Epoch: 6, Loss: 495.0135\n",
      "Epoch: 7, Loss: 491.1091\n",
      "Epoch: 8, Loss: 486.6154\n",
      "Epoch: 9, Loss: 481.5520\n",
      "Epoch: 9, Validation Loss: 468.3165, AUC Score: 0.2402\n",
      "Epoch: 10, Loss: 475.9496\n",
      "Epoch: 11, Loss: 469.8487\n",
      "Epoch: 12, Loss: 463.2999\n",
      "Epoch: 13, Loss: 456.3634\n",
      "Epoch: 14, Loss: 449.1050\n",
      "Epoch: 14, Validation Loss: 434.2062, AUC Score: 0.2402\n",
      "Epoch: 15, Loss: 441.5860\n",
      "Epoch: 16, Loss: 433.8502\n",
      "Epoch: 17, Loss: 425.9247\n",
      "Epoch: 18, Loss: 417.8288\n",
      "Epoch: 19, Loss: 409.5780\n",
      "Epoch: 19, Validation Loss: 394.4034, AUC Score: 0.2404\n",
      "Epoch: 20, Loss: 401.1905\n",
      "Epoch: 21, Loss: 392.6924\n",
      "Epoch: 22, Loss: 384.1167\n",
      "Epoch: 23, Loss: 375.4988\n",
      "Epoch: 24, Loss: 366.8731\n",
      "Epoch: 24, Validation Loss: 352.6883, AUC Score: 0.2409\n",
      "Epoch: 25, Loss: 358.2738\n",
      "Epoch: 26, Loss: 349.7364\n",
      "Epoch: 27, Loss: 341.2965\n",
      "Epoch: 28, Loss: 332.9901\n",
      "Epoch: 29, Loss: 324.8538\n",
      "Epoch: 29, Validation Loss: 313.0640, AUC Score: 0.2407\n",
      "Epoch: 30, Loss: 316.9243\n",
      "Epoch: 31, Loss: 309.2368\n",
      "Epoch: 32, Loss: 301.8233\n",
      "Epoch: 33, Loss: 294.7123\n",
      "Epoch: 34, Loss: 287.9282\n",
      "Epoch: 34, Validation Loss: 279.6584, AUC Score: 0.2386\n",
      "Epoch: 35, Loss: 281.4913\n",
      "Epoch: 36, Loss: 275.4175\n",
      "Epoch: 37, Loss: 269.7174\n",
      "Epoch: 38, Loss: 264.3972\n",
      "Epoch: 39, Loss: 259.4586\n",
      "Epoch: 39, Validation Loss: 255.1260, AUC Score: 0.5933\n",
      "Epoch: 40, Loss: 254.8987\n",
      "Epoch: 41, Loss: 250.7103\n",
      "Epoch: 42, Loss: 246.8810\n",
      "Epoch: 43, Loss: 243.3947\n",
      "Epoch: 44, Loss: 240.2315\n",
      "Epoch: 44, Validation Loss: 239.3923, AUC Score: 0.7200\n",
      "Epoch: 45, Loss: 237.3689\n",
      "Epoch: 46, Loss: 234.7824\n",
      "Epoch: 47, Loss: 232.4464\n",
      "Epoch: 48, Loss: 230.3354\n",
      "Epoch: 49, Loss: 228.4250\n",
      "Epoch: 49, Validation Loss: 230.0347, AUC Score: 0.7342\n",
      "Epoch: 50, Loss: 226.6926\n",
      "Epoch: 51, Loss: 225.1180\n",
      "Epoch: 52, Loss: 223.6832\n",
      "Epoch: 53, Loss: 222.3725\n",
      "Epoch: 54, Loss: 221.1723\n",
      "Epoch: 54, Validation Loss: 224.2857, AUC Score: 0.7386\n",
      "Epoch: 55, Loss: 220.0707\n",
      "Epoch: 56, Loss: 219.0572\n",
      "Epoch: 57, Loss: 218.1227\n",
      "Epoch: 58, Loss: 217.2589\n",
      "Epoch: 59, Loss: 216.4586\n",
      "Epoch: 59, Validation Loss: 220.5028, AUC Score: 0.7416\n",
      "Epoch: 60, Loss: 215.7150\n",
      "Epoch: 61, Loss: 215.0225\n",
      "Epoch: 62, Loss: 214.3756\n",
      "Epoch: 63, Loss: 213.7695\n",
      "Epoch: 64, Loss: 213.2000\n",
      "Epoch: 64, Validation Loss: 217.8549, AUC Score: 0.7432\n",
      "Epoch: 65, Loss: 212.6632\n",
      "Epoch: 66, Loss: 212.1557\n",
      "Epoch: 67, Loss: 211.6746\n",
      "Epoch: 68, Loss: 211.2173\n",
      "Epoch: 69, Loss: 210.7816\n",
      "Epoch: 69, Validation Loss: 215.8774, AUC Score: 0.7456\n",
      "Epoch: 70, Loss: 210.3657\n",
      "Epoch: 71, Loss: 209.9680\n",
      "Epoch: 72, Loss: 209.5869\n",
      "Epoch: 73, Loss: 209.2213\n",
      "Epoch: 74, Loss: 208.8699\n",
      "Epoch: 74, Validation Loss: 214.3201, AUC Score: 0.7475\n",
      "Epoch: 75, Loss: 208.5318\n",
      "Epoch: 76, Loss: 208.2059\n",
      "Epoch: 77, Loss: 207.8912\n",
      "Epoch: 78, Loss: 207.5869\n",
      "Epoch: 79, Loss: 207.2923\n",
      "Epoch: 79, Validation Loss: 213.0409, AUC Score: 0.7488\n",
      "Epoch: 80, Loss: 207.0065\n",
      "Epoch: 81, Loss: 206.7290\n",
      "Epoch: 82, Loss: 206.4592\n",
      "Epoch: 83, Loss: 206.1966\n",
      "Epoch: 84, Loss: 205.9406\n",
      "Epoch: 84, Validation Loss: 211.9525, AUC Score: 0.7495\n",
      "Epoch: 85, Loss: 205.6908\n",
      "Epoch: 86, Loss: 205.4467\n",
      "Epoch: 87, Loss: 205.2078\n",
      "Epoch: 88, Loss: 204.9738\n",
      "Epoch: 89, Loss: 204.7444\n",
      "Epoch: 89, Validation Loss: 211.0086, AUC Score: 0.7499\n",
      "Epoch: 90, Loss: 204.5192\n"
     ]
    }
   ],
   "source": [
    "# number of features and layers\n",
    "n_features = 2\n",
    "n_layers = 3\n",
    "\n",
    "# Random weight initialization\n",
    "weights = 0.01 * np.random.randn(n_layers, n_features, 3, requires_grad=True)\n",
    "\n",
    "# We create a quantum device with n_features \"wires\" (or qubits)\n",
    "provider = IBMProvider()\n",
    "dev = ibm_dev = qml.device('qiskit.ibmq', wires=n_features, backend='ibmq_qasm_simulator', provider=provider) \n",
    "\n",
    "# train the model\n",
    "best_score, best_weights = train (n_features, n_layers,x_train,y_train_arr,0.01, weights, 1000, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8354234198036303\n"
     ]
    }
   ],
   "source": [
    "print (best_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8640271361518534\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "\n",
    "def test(n_features, n_layers,x_test,y_test,w_test, weights):\n",
    "        # Remove grad\n",
    "        X_test = np.array(x_test, requires_grad=False)\n",
    "        Y_test = np.array(y_test, requires_grad=False)\n",
    "        W_test = np.array(w_test, requires_grad=False)\n",
    "\n",
    "        # This will be between -1 and 1, we need to convert to between 0 and 1\n",
    "        y_scores = np.array([classifier(n_features, n_layers,weights, x) for x in X_test])\n",
    "        y_scores = (y_scores + 1) / 2\n",
    "\n",
    "        # Renormalize weights\n",
    "        W_test[Y_test == 1] = (W_test[Y_test == 1] / W_test[Y_test == 1].sum()) * W_test.shape[0] / 2\n",
    "        W_test[Y_test == 0] = (W_test[Y_test == 0] / W_test[Y_test == 0].sum()) * W_test.shape[0] / 2\n",
    "\n",
    "        # Calculate ROC\n",
    "        auc_score = roc_auc_score(y_true=Y_test, y_score=y_scores, sample_weight=W_test)\n",
    "        \n",
    "        return auc_score\n",
    "    \n",
    "auc_score = test(n_features, n_layers,x_test,y_test,w_test, best_weights)\n",
    "print (auc_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
