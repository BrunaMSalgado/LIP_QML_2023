{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projeto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.optimize import NesterovMomentumOptimizer\n",
    "from pennylane.optimize import AdamOptimizer\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "from pennylane.templates.embeddings import AngleEmbedding\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 90548\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Electron_Multi</th>\n",
       "      <th>FatJet1_Eta</th>\n",
       "      <th>FatJet1_Mass</th>\n",
       "      <th>FatJet1_PT</th>\n",
       "      <th>FatJet1_Phi</th>\n",
       "      <th>FatJet1_Tau1</th>\n",
       "      <th>FatJet1_Tau2</th>\n",
       "      <th>FatJet1_Tau3</th>\n",
       "      <th>FatJet1_Tau4</th>\n",
       "      <th>FatJet1_Tau5</th>\n",
       "      <th>...</th>\n",
       "      <th>gen_decay2</th>\n",
       "      <th>gen_decay_filter</th>\n",
       "      <th>gen_filter</th>\n",
       "      <th>gen_label</th>\n",
       "      <th>gen_n_btags</th>\n",
       "      <th>gen_sample</th>\n",
       "      <th>gen_sample_filter</th>\n",
       "      <th>gen_split</th>\n",
       "      <th>gen_weights</th>\n",
       "      <th>gen_xsec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1.408853</td>\n",
       "      <td>15.150869</td>\n",
       "      <td>339.182312</td>\n",
       "      <td>2.350262</td>\n",
       "      <td>1.396943</td>\n",
       "      <td>0.710451</td>\n",
       "      <td>0.109013</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>PyDelphes</td>\n",
       "      <td>signal</td>\n",
       "      <td>1</td>\n",
       "      <td>tZFCNC</td>\n",
       "      <td>tZFCNC_PyDelphes</td>\n",
       "      <td>test</td>\n",
       "      <td>7.762202e-09</td>\n",
       "      <td>0.001285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-2.481838</td>\n",
       "      <td>7.208333</td>\n",
       "      <td>247.036240</td>\n",
       "      <td>-2.280740</td>\n",
       "      <td>0.428710</td>\n",
       "      <td>0.205213</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>PyDelphes</td>\n",
       "      <td>signal</td>\n",
       "      <td>1</td>\n",
       "      <td>tZFCNC</td>\n",
       "      <td>tZFCNC_PyDelphes</td>\n",
       "      <td>val</td>\n",
       "      <td>7.762202e-09</td>\n",
       "      <td>0.001285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.476267</td>\n",
       "      <td>94.220718</td>\n",
       "      <td>238.014694</td>\n",
       "      <td>-1.788097</td>\n",
       "      <td>94.256210</td>\n",
       "      <td>2.418446</td>\n",
       "      <td>1.585315</td>\n",
       "      <td>1.127324</td>\n",
       "      <td>0.431098</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>PyDelphes</td>\n",
       "      <td>signal</td>\n",
       "      <td>1</td>\n",
       "      <td>tZFCNC</td>\n",
       "      <td>tZFCNC_PyDelphes</td>\n",
       "      <td>train</td>\n",
       "      <td>7.762249e-09</td>\n",
       "      <td>0.001285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0.647480</td>\n",
       "      <td>13.459283</td>\n",
       "      <td>230.971832</td>\n",
       "      <td>-1.032663</td>\n",
       "      <td>1.227122</td>\n",
       "      <td>0.467150</td>\n",
       "      <td>0.164008</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>PyDelphes</td>\n",
       "      <td>signal</td>\n",
       "      <td>1</td>\n",
       "      <td>tZFCNC</td>\n",
       "      <td>tZFCNC_PyDelphes</td>\n",
       "      <td>train</td>\n",
       "      <td>7.762249e-09</td>\n",
       "      <td>0.001285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>2.106436</td>\n",
       "      <td>97.490242</td>\n",
       "      <td>698.399902</td>\n",
       "      <td>-3.059983</td>\n",
       "      <td>36.555862</td>\n",
       "      <td>2.937936</td>\n",
       "      <td>1.799140</td>\n",
       "      <td>1.093004</td>\n",
       "      <td>0.589724</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>PyDelphes</td>\n",
       "      <td>signal</td>\n",
       "      <td>1</td>\n",
       "      <td>tZFCNC</td>\n",
       "      <td>tZFCNC_PyDelphes</td>\n",
       "      <td>train</td>\n",
       "      <td>7.762249e-09</td>\n",
       "      <td>0.001285</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 83 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Electron_Multi  FatJet1_Eta  FatJet1_Mass  FatJet1_PT  FatJet1_Phi  \\\n",
       "0               2     1.408853     15.150869  339.182312     2.350262   \n",
       "1               1    -2.481838      7.208333  247.036240    -2.280740   \n",
       "2               0     1.476267     94.220718  238.014694    -1.788097   \n",
       "3               1     0.647480     13.459283  230.971832    -1.032663   \n",
       "4               0     2.106436     97.490242  698.399902    -3.059983   \n",
       "\n",
       "   FatJet1_Tau1  FatJet1_Tau2  FatJet1_Tau3  FatJet1_Tau4  FatJet1_Tau5  ...  \\\n",
       "0      1.396943      0.710451      0.109013      0.000000      0.000000  ...   \n",
       "1      0.428710      0.205213      0.000000      0.000000      0.000000  ...   \n",
       "2     94.256210      2.418446      1.585315      1.127324      0.431098  ...   \n",
       "3      1.227122      0.467150      0.164008      0.000000      0.000000  ...   \n",
       "4     36.555862      2.937936      1.799140      1.093004      0.589724  ...   \n",
       "\n",
       "   gen_decay2  gen_decay_filter  gen_filter  gen_label  gen_n_btags  \\\n",
       "0           0              None   PyDelphes     signal            1   \n",
       "1           0              None   PyDelphes     signal            1   \n",
       "2           0              None   PyDelphes     signal            1   \n",
       "3           0              None   PyDelphes     signal            1   \n",
       "4           0              None   PyDelphes     signal            1   \n",
       "\n",
       "   gen_sample  gen_sample_filter  gen_split   gen_weights  gen_xsec  \n",
       "0      tZFCNC   tZFCNC_PyDelphes       test  7.762202e-09  0.001285  \n",
       "1      tZFCNC   tZFCNC_PyDelphes        val  7.762202e-09  0.001285  \n",
       "2      tZFCNC   tZFCNC_PyDelphes      train  7.762249e-09  0.001285  \n",
       "3      tZFCNC   tZFCNC_PyDelphes      train  7.762249e-09  0.001285  \n",
       "4      tZFCNC   tZFCNC_PyDelphes      train  7.762249e-09  0.001285  \n",
       "\n",
       "[5 rows x 83 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# Read the HDF5 file using pandas\n",
    "data_frame_fcnc = pd.read_hdf('fcnc_pythia_sanitised_features.h5')\n",
    "\n",
    "# Get the number of rows\n",
    "num_rows = data_frame_fcnc.shape[0]\n",
    "\n",
    "print('Number of rows: {}'.format(num_rows))\n",
    "\n",
    "# Explore the data\n",
    "data_frame_fcnc.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 1002490\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Electron_Multi</th>\n",
       "      <th>FatJet1_Eta</th>\n",
       "      <th>FatJet1_Mass</th>\n",
       "      <th>FatJet1_PT</th>\n",
       "      <th>FatJet1_Phi</th>\n",
       "      <th>FatJet1_Tau1</th>\n",
       "      <th>FatJet1_Tau2</th>\n",
       "      <th>FatJet1_Tau3</th>\n",
       "      <th>FatJet1_Tau4</th>\n",
       "      <th>FatJet1_Tau5</th>\n",
       "      <th>...</th>\n",
       "      <th>gen_decay2</th>\n",
       "      <th>gen_decay_filter</th>\n",
       "      <th>gen_filter</th>\n",
       "      <th>gen_label</th>\n",
       "      <th>gen_n_btags</th>\n",
       "      <th>gen_sample</th>\n",
       "      <th>gen_sample_filter</th>\n",
       "      <th>gen_split</th>\n",
       "      <th>gen_weights</th>\n",
       "      <th>gen_xsec</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>-1.988600</td>\n",
       "      <td>52.710262</td>\n",
       "      <td>229.350952</td>\n",
       "      <td>0.728242</td>\n",
       "      <td>36.148926</td>\n",
       "      <td>23.039709</td>\n",
       "      <td>16.949991</td>\n",
       "      <td>14.424411</td>\n",
       "      <td>12.000529</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2L</td>\n",
       "      <td>HT250to500</td>\n",
       "      <td>bkg</td>\n",
       "      <td>1</td>\n",
       "      <td>Zjj</td>\n",
       "      <td>Zjj_HT250to500</td>\n",
       "      <td>train</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>11.9635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.528382</td>\n",
       "      <td>61.115589</td>\n",
       "      <td>315.538910</td>\n",
       "      <td>-0.863614</td>\n",
       "      <td>32.592808</td>\n",
       "      <td>22.366640</td>\n",
       "      <td>16.285843</td>\n",
       "      <td>13.938633</td>\n",
       "      <td>11.180016</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2L</td>\n",
       "      <td>HT250to500</td>\n",
       "      <td>bkg</td>\n",
       "      <td>1</td>\n",
       "      <td>Zjj</td>\n",
       "      <td>Zjj_HT250to500</td>\n",
       "      <td>test</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>11.9635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1.479911</td>\n",
       "      <td>98.012802</td>\n",
       "      <td>251.109573</td>\n",
       "      <td>-3.133624</td>\n",
       "      <td>90.252274</td>\n",
       "      <td>33.646885</td>\n",
       "      <td>30.612156</td>\n",
       "      <td>27.973904</td>\n",
       "      <td>23.729696</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2L</td>\n",
       "      <td>HT250to500</td>\n",
       "      <td>bkg</td>\n",
       "      <td>1</td>\n",
       "      <td>Zjj</td>\n",
       "      <td>Zjj_HT250to500</td>\n",
       "      <td>val</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>11.9635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>0.926899</td>\n",
       "      <td>-0.000007</td>\n",
       "      <td>240.909348</td>\n",
       "      <td>0.835656</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2L</td>\n",
       "      <td>HT250to500</td>\n",
       "      <td>bkg</td>\n",
       "      <td>1</td>\n",
       "      <td>Zjj</td>\n",
       "      <td>Zjj_HT250to500</td>\n",
       "      <td>val</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>11.9635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.781194</td>\n",
       "      <td>72.234299</td>\n",
       "      <td>206.020386</td>\n",
       "      <td>-0.320449</td>\n",
       "      <td>48.886372</td>\n",
       "      <td>20.743645</td>\n",
       "      <td>16.572512</td>\n",
       "      <td>13.070706</td>\n",
       "      <td>11.269534</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>2L</td>\n",
       "      <td>HT250to500</td>\n",
       "      <td>bkg</td>\n",
       "      <td>1</td>\n",
       "      <td>Zjj</td>\n",
       "      <td>Zjj_HT250to500</td>\n",
       "      <td>val</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>11.9635</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Electron_Multi  FatJet1_Eta  FatJet1_Mass  FatJet1_PT  FatJet1_Phi  \\\n",
       "0               2    -1.988600     52.710262  229.350952     0.728242   \n",
       "1               0     0.528382     61.115589  315.538910    -0.863614   \n",
       "2               0     1.479911     98.012802  251.109573    -3.133624   \n",
       "3               2     0.926899     -0.000007  240.909348     0.835656   \n",
       "4               0     0.781194     72.234299  206.020386    -0.320449   \n",
       "\n",
       "   FatJet1_Tau1  FatJet1_Tau2  FatJet1_Tau3  FatJet1_Tau4  FatJet1_Tau5  ...  \\\n",
       "0     36.148926     23.039709     16.949991     14.424411     12.000529  ...   \n",
       "1     32.592808     22.366640     16.285843     13.938633     11.180016  ...   \n",
       "2     90.252274     33.646885     30.612156     27.973904     23.729696  ...   \n",
       "3      0.000000      0.000000      0.000000      0.000000      0.000000  ...   \n",
       "4     48.886372     20.743645     16.572512     13.070706     11.269534  ...   \n",
       "\n",
       "   gen_decay2  gen_decay_filter  gen_filter  gen_label  gen_n_btags  \\\n",
       "0           0                2L  HT250to500        bkg            1   \n",
       "1           0                2L  HT250to500        bkg            1   \n",
       "2           0                2L  HT250to500        bkg            1   \n",
       "3           0                2L  HT250to500        bkg            1   \n",
       "4           0                2L  HT250to500        bkg            1   \n",
       "\n",
       "   gen_sample  gen_sample_filter  gen_split  gen_weights  gen_xsec  \n",
       "0         Zjj     Zjj_HT250to500      train     0.000018   11.9635  \n",
       "1         Zjj     Zjj_HT250to500       test     0.000018   11.9635  \n",
       "2         Zjj     Zjj_HT250to500        val     0.000018   11.9635  \n",
       "3         Zjj     Zjj_HT250to500        val     0.000018   11.9635  \n",
       "4         Zjj     Zjj_HT250to500        val     0.000018   11.9635  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read the HDF5 file using pandas\n",
    "data_frame_bkg = pd.read_hdf('bkg_pythia_sanitised_features.h5')\n",
    "\n",
    "# Get the number of rows\n",
    "num_rows = data_frame_bkg.shape[0]\n",
    "\n",
    "print('Number of rows: {}'.format(num_rows))\n",
    "\n",
    "# Explore the data\n",
    "data_frame_bkg.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VQE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA (2 Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_fcnc_pca = data_frame_fcnc.copy()\n",
    "data_frame_bkg_pca = data_frame_bkg.copy()\n",
    "\n",
    "# Drop the categorical features except label, weights and gen_split\n",
    "data_frame_fcnc_pca.drop(['gen_decay_filter', 'gen_filter', 'gen_n_btags', 'gen_sample', 'gen_sample_filter','gen_decay2','gen_decay1'], axis=1, inplace=True)\n",
    "data_frame_bkg_pca.drop(['gen_decay_filter', 'gen_filter', 'gen_n_btags', 'gen_sample', 'gen_sample_filter','gen_decay2','gen_decay1'], axis=1, inplace=True)\n",
    "\n",
    "# Drop the features that are not in both dataframes\n",
    "for feature in data_frame_fcnc_pca.columns.values:\n",
    "    if feature not in data_frame_bkg_pca.columns.values:\n",
    "        data_frame_fcnc_pca.drop([feature], axis=1, inplace=True)\n",
    "\n",
    "for feature in data_frame_bkg_pca.columns.values:\n",
    "    if feature not in data_frame_fcnc_pca.columns.values:\n",
    "        data_frame_bkg_pca.drop([feature], axis=1, inplace=True)\n",
    "        \n",
    "# Join the dataframes\n",
    "data = pd.concat([data_frame_fcnc_pca, data_frame_bkg_pca])\n",
    "\n",
    "# Substitute the labels \"signal\" and \"bkg\" by 1 and 0\n",
    "data = data.replace(['signal'], 1)\n",
    "data= data.replace(['bkg'], 0)\n",
    "\n",
    "\n",
    "# train, test and validation sets\n",
    "train = data.loc[data['gen_split'] == 'train']\n",
    "test = data.loc[data['gen_split'] == 'test']\n",
    "val = data.loc[data['gen_split'] == 'val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def perform_PCA (DataFeatures, pca_n_features, train, data):\n",
    "\n",
    "    ## Fit PCA to train data & rank components by AUC\n",
    "    pca = PCA(n_components=len(DataFeatures))\n",
    "    pca.fit(train[DataFeatures])\n",
    "\n",
    "    ## Transform the desired dataset to get its principal components\n",
    "    # Get ranked components by AUC from the train data\n",
    "    principalComponents = pca.transform(train[DataFeatures])\n",
    "\n",
    "    # Book will be a dictiorary with the AUC (values) of each component (keys)\n",
    "    book = {}\n",
    "\n",
    "    # Get values for AUC computation\n",
    "    y_true = train['gen_label'].values\n",
    "    weights = train[\"gen_xsec\"].values\n",
    "\n",
    "    # Renormalise weights\n",
    "    weights[y_true == 1] = (weights[y_true == 1] / weights[y_true == 1].sum()) * weights.shape[0] / 2\n",
    "    weights[y_true == 0] = (weights[y_true == 0] / weights[y_true == 0].sum()) * weights.shape[0] / 2\n",
    "\n",
    "    for feature_idx in range(principalComponents.shape[1]):\n",
    "        book[f\"Component {feature_idx}\"] = roc_auc_score(y_true=y_true, y_score=principalComponents[:, feature_idx], sample_weight=weights)\n",
    "\n",
    "    # Give me the best features\n",
    "    book = pd.DataFrame.from_dict(book, orient=\"index\")\n",
    "    book.columns = [\"AUC\"]\n",
    "    book.sort_values(by=\"AUC\", ascending=False, inplace=True)\n",
    "    book.reset_index(inplace=True)\n",
    "    book.rename(columns={\"index\": \"Feature\"}, inplace=True)\n",
    "\n",
    "    ## Replace current data by its components ##\n",
    "    # Get components for the current set we want\n",
    "    principalComponents = pca.transform(data[DataFeatures])\n",
    "\n",
    "    # Create a new dataframe with PCA data\n",
    "    newdf = pd.DataFrame(principalComponents, columns=[f\"Component {i}\" for i in range(principalComponents.shape[1])])\n",
    "\n",
    "    # Select the best components given their AUC performance in training data\n",
    "    newdf = newdf[book[\"Feature\"][0 : pca_n_features]]\n",
    "\n",
    "    # Add the other relevant features\n",
    "    newdf[\"gen_xsec\"] = data[\"gen_xsec\"].values\n",
    "    newdf[\"gen_label\"] = data[\"gen_label\"].values\n",
    "    newdf[\"gen_split\"] = data[\"gen_split\"].values\n",
    "\n",
    "    # Finally, replace self.data with newdf\n",
    "    data = newdf\n",
    "\n",
    "    # Update DataFeatures\n",
    "    DataFeatures = pd.Index(list(set(data.columns) - set([\"gen_label\", \"gen_xsec\", \"gen_split\"])))\n",
    "    \n",
    "    return data, DataFeatures, book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### perform PCA on the train data\n",
    "# Normalize the data for angle embeding  (Put the data between -pi and pi)\n",
    "DataFeatures_vqe = pd.Index(list(set(data.columns) - set([\"gen_label\", \"gen_xsec\", \"gen_split\"])))\n",
    "pca_n_features = 2\n",
    "data_vqe, DataFeatures_vqe, book = perform_PCA (DataFeatures_vqe, pca_n_features, train, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the features array into train, validation and test sets\n",
    "data_vqe [DataFeatures_vqe] = (((data_vqe [DataFeatures_vqe] - data_vqe [DataFeatures_vqe].min()) / (data_vqe [DataFeatures_vqe].max() - data_vqe [DataFeatures_vqe].min())) * 2 - 1) * (np.pi)\n",
    "\n",
    "# divide the new data into train, test and validation sets\n",
    "train = data_vqe.loc[data_vqe['gen_split'] == 'train']\n",
    "test = data_vqe.loc[data_vqe['gen_split'] == 'test']\n",
    "val = data_vqe.loc[data_vqe['gen_split'] == 'val']\n",
    "\n",
    "# divide the train data into signal and background and get 500 samples of each\n",
    "train_sgn = train.loc[train['gen_label'] == 1].sample(n=500)\n",
    "train_bkg = train.loc[train['gen_label'] == 0].sample(n=500)\n",
    "x_train = pd.concat([train_sgn, train_bkg])\n",
    "x_train = x_train.sample(frac=1)\n",
    "\n",
    "# divide the validation data into signal and background and get 500 samples of each\n",
    "val_sgn = val.loc[val['gen_label'] == 1].sample(n=500)\n",
    "val_bkg = val.loc[val['gen_label'] == 0].sample(n=500)\n",
    "x_val = pd.concat([val_sgn, val_bkg])\n",
    "x_val = x_val.sample(frac=1)\n",
    "\n",
    "# divide the test data into signal and background and get 500 samples of each\n",
    "test_sgn = test.loc[test['gen_label'] == 1].sample(n=500)\n",
    "test_bkg = test.loc[test['gen_label'] == 0].sample(n=500)\n",
    "x_test = pd.concat([test_sgn, test_bkg])\n",
    "x_test = x_test.sample(frac=1)\n",
    "\n",
    "# get an array with the labels for each set\n",
    "y_train = x_train['gen_label'].values\n",
    "y_val = x_val['gen_label'].values\n",
    "y_test = x_test['gen_label'].values\n",
    "\n",
    "# get an array with the weights for each set\n",
    "w_train = x_train[\"gen_xsec\"].values\n",
    "w_val = x_val[\"gen_xsec\"].values\n",
    "w_test = x_test[\"gen_xsec\"].values\n",
    "\n",
    "# get an array with the features for each set\n",
    "x_train = x_train[DataFeatures_vqe].values\n",
    "x_val = x_val[DataFeatures_vqe].values\n",
    "x_test = x_test[DataFeatures_vqe].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "def square_loss(labels, predictions):\n",
    "    loss = 0\n",
    "    for l, p in zip(labels, predictions):\n",
    "        loss = loss + (l - p) ** 2\n",
    "\n",
    "    loss = loss / len(labels)\n",
    "    return loss\n",
    "\n",
    "# quantum circuit function\n",
    "def circuit(n_features, n_layers, weights, x):\n",
    "        # Embedding\n",
    "        \n",
    "        qml.AngleEmbedding(x,range (0, n_features),rotation=\"X\" )\n",
    "\n",
    "        # For every layer\n",
    "        for layer in range(n_layers):\n",
    "            W1 = weights[layer]\n",
    "\n",
    "            # Define Rotations\n",
    "            for i in range(0,n_features):\n",
    "                qml.Rot(W1[i, 0], W1[i, 1], W1[i, 2], wires=i)\n",
    "\n",
    "            # Entanglement\n",
    "            if n_features != 1:\n",
    "                if n_features > 2:\n",
    "                    for i in range(n_features):\n",
    "                        if i == n_features - 1:\n",
    "                            qml.CNOT(wires=[i, 0])\n",
    "                        else:\n",
    "                            qml.CNOT(wires=[i, i + 1])\n",
    "                else:\n",
    "                    qml.CNOT(wires=[1, 0])\n",
    "\n",
    "        return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "# classifier function    \n",
    "def classifier(n_features, n_layers, weights, x):\n",
    "        #c = circuit(n_features, n_layers, weights, x)\n",
    "        dev=qml.device(\"default.qubit\", wires=n_features)\n",
    "        return qml.QNode(circuit, dev)(n_features, n_layers, weights, x)\n",
    "    \n",
    "# cost function    \n",
    "def cost(n_features, n_layers,weights,X,Y,W):  \n",
    "        # Compute predictions\n",
    "        y_scores = [(classifier(n_features, n_layers,weights, x) + 1) / 2 for x in X]\n",
    "\n",
    "        loss = square_loss(Y, y_scores)\n",
    "        loss = loss * W\n",
    "        loss = loss.sum()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "# train step function    \n",
    "def train_step(n_features, n_layers,x_train,y_train, w_train, weights, opt,desc='Training'):\n",
    "        \n",
    "        # Only require grad if necessary\n",
    "        x_train = np.array(x_train, requires_grad=False)\n",
    "        y_train = np.array(y_train, requires_grad=True)\n",
    "        w_train = np.array(w_train, requires_grad=False)\n",
    "\n",
    "        # Compute cost and update weights\n",
    "        weights, loss = opt.step_and_cost(cost, n_features, n_layers,weights, X=x_train, Y=y_train, W=w_train)\n",
    "\n",
    "        return loss, weights\n",
    "    \n",
    "# validation step function\n",
    "def validation_step(n_features, n_layers, x_val, y_val, w_val, weights, best_score, epoch_number, best_score_epoch,best_weights,desc='Validation'):\n",
    "    X_val = np.array(x_val, requires_grad=False)\n",
    "    Y_val = np.array(y_val, requires_grad=False)\n",
    "    W_val = np.array(w_val, requires_grad=False)\n",
    "\n",
    "    y_scores = np.array([classifier(n_features, n_layers, weights, x) for x in X_val])\n",
    "    y_scores = (y_scores + 1) / 2\n",
    "\n",
    "    W_val[Y_val == 1] = (W_val[Y_val == 1] / W_val[Y_val == 1].sum()) * W_val.shape[0] / 2\n",
    "    W_val[Y_val == 0] = (W_val[Y_val == 0] / W_val[Y_val == 0].sum()) * W_val.shape[0] / 2\n",
    "\n",
    "    auc_score = roc_auc_score(y_true=Y_val, y_score=y_scores, sample_weight=W_val)\n",
    "    loss = cost(n_features, n_layers, weights, X_val, Y_val, W_val)\n",
    "\n",
    "\n",
    "    if best_score is None or auc_score > best_score:\n",
    "        best_score = auc_score\n",
    "        best_score_epoch = epoch_number\n",
    "        best_weights = weights\n",
    "\n",
    "    tqdm.write(f\"Epoch: {epoch_number}, Validation Loss: {loss:.4f}, AUC Score: {auc_score:.4f}\")\n",
    "\n",
    "    return best_score, best_score_epoch, best_weights\n",
    "     \n",
    "# train function\n",
    "def train(n_features, n_layers, x_train, y_train, learning_rate, weights, max_epochs):\n",
    "    opt = AdamOptimizer(learning_rate)\n",
    "    best_score = None\n",
    "    best_weights = None\n",
    "    best_score_epoch = None\n",
    "    epoch_number = 0\n",
    "\n",
    "    with tqdm(total=max_epochs, desc='Epoch', unit='epoch') as pbar:\n",
    "        for epoch in range(epoch_number, max_epochs):\n",
    "            epoch_number = epoch\n",
    "\n",
    "            loss, nf_nl_weights = train_step(n_features, n_layers, x_train, y_train, w_train, weights, opt, desc='Training')\n",
    "            \n",
    "            # Log variable values using tqdm.write\n",
    "            tqdm.write(f\"Epoch: {epoch_number:}, Loss: {loss:.4f}\")\n",
    "            \n",
    "            \n",
    "            weights = nf_nl_weights[2:]\n",
    "            weights = weights[0]\n",
    "\n",
    "            if epoch_number == max_epochs - 1 or (epoch_number+1)%5==0:\n",
    "                best_score, best_score_epoch, best_weights = validation_step(n_features, n_layers, x_val, y_val, w_val, weights, best_score, epoch_number, best_score_epoch, best_weights,desc='Validation')\n",
    "                # early stopping\n",
    "                if epoch_number - best_score_epoch > 30 and epoch_number > 80:\n",
    "                    tqdm.write(f\"Early stopping at epoch {epoch_number}\")\n",
    "                    break\n",
    "\n",
    "            pbar.update(1)  # Update progress bar\n",
    "        tqdm.write(f\"Best Score: {best_score:.4f}\")            \n",
    "        \n",
    "    return best_score, best_weights\n",
    "\n",
    "def test(n_features, n_layers,x_test,y_test,w_test, weights):\n",
    "        # Remove grad\n",
    "        X_test = np.array(x_test, requires_grad=False)\n",
    "        Y_test = np.array(y_test, requires_grad=False)\n",
    "        W_test = np.array(w_test, requires_grad=False)\n",
    "\n",
    "        # This will be between -1 and 1, we need to convert to between 0 and 1\n",
    "        y_scores = np.array([classifier(n_features, n_layers,weights, x) for x in X_test])\n",
    "        y_scores = (y_scores + 1) / 2\n",
    "\n",
    "        # Renormalize weights\n",
    "        W_test[Y_test == 1] = (W_test[Y_test == 1] / W_test[Y_test == 1].sum()) * W_test.shape[0] / 2\n",
    "        W_test[Y_test == 0] = (W_test[Y_test == 0] / W_test[Y_test == 0].sum()) * W_test.shape[0] / 2\n",
    "\n",
    "        # Calculate ROC\n",
    "        auc_score = roc_auc_score(y_true=Y_test, y_score=y_scores, sample_weight=W_test)\n",
    "        \n",
    "        return auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a139775c09b34692892287691978dcbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1000 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 153.9020\n",
      "Epoch: 1, Loss: 153.8595\n",
      "Epoch: 2, Loss: 153.8473\n",
      "Epoch: 3, Loss: 153.8185\n",
      "Epoch: 4, Loss: 153.7733\n",
      "Epoch: 4, Validation Loss: 259.3311, AUC Score: 0.6769\n",
      "Epoch: 5, Loss: 153.7092\n",
      "Epoch: 6, Loss: 153.6190\n",
      "Epoch: 7, Loss: 153.4807\n",
      "Epoch: 8, Loss: 153.2700\n",
      "Epoch: 9, Loss: 152.9715\n",
      "Epoch: 9, Validation Loss: 257.3385, AUC Score: 0.6801\n",
      "Epoch: 10, Loss: 152.5673\n",
      "Epoch: 11, Loss: 152.0302\n",
      "Epoch: 12, Loss: 151.3347\n",
      "Epoch: 13, Loss: 150.4694\n",
      "Epoch: 14, Loss: 149.4358\n",
      "Epoch: 14, Validation Loss: 249.8120, AUC Score: 0.6936\n",
      "Epoch: 15, Loss: 148.2449\n",
      "Epoch: 16, Loss: 146.9170\n",
      "Epoch: 17, Loss: 145.4817\n",
      "Epoch: 18, Loss: 143.9781\n",
      "Epoch: 19, Loss: 142.4545\n",
      "Epoch: 19, Validation Loss: 237.0511, AUC Score: 0.7201\n",
      "Epoch: 20, Loss: 140.9670\n",
      "Epoch: 21, Loss: 139.5760\n",
      "Epoch: 22, Loss: 138.3412\n",
      "Epoch: 23, Loss: 137.3155\n",
      "Epoch: 24, Loss: 136.5359\n",
      "Epoch: 24, Validation Loss: 227.8735, AUC Score: 0.7461\n",
      "Epoch: 25, Loss: 136.0150\n",
      "Epoch: 26, Loss: 135.7327\n",
      "Epoch: 27, Loss: 135.6345\n",
      "Epoch: 28, Loss: 135.6408\n",
      "Epoch: 29, Loss: 135.6676\n",
      "Epoch: 29, Validation Loss: 226.5445, AUC Score: 0.7669\n",
      "Epoch: 30, Loss: 135.6472\n",
      "Epoch: 31, Loss: 135.5380\n",
      "Epoch: 32, Loss: 135.3249\n",
      "Epoch: 33, Loss: 135.0132\n",
      "Epoch: 34, Loss: 134.6219\n",
      "Epoch: 34, Validation Loss: 224.1007, AUC Score: 0.7887\n",
      "Epoch: 35, Loss: 134.1767\n",
      "Epoch: 36, Loss: 133.7047\n",
      "Epoch: 37, Loss: 133.2313\n",
      "Epoch: 38, Loss: 132.7776\n",
      "Epoch: 39, Loss: 132.3587\n",
      "Epoch: 39, Validation Loss: 220.8030, AUC Score: 0.8023\n",
      "Epoch: 40, Loss: 131.9837\n",
      "Epoch: 41, Loss: 131.6553\n",
      "Epoch: 42, Loss: 131.3714\n",
      "Epoch: 43, Loss: 131.1254\n",
      "Epoch: 44, Loss: 130.9084\n",
      "Epoch: 44, Validation Loss: 218.9482, AUC Score: 0.8158\n",
      "Epoch: 45, Loss: 130.7102\n",
      "Epoch: 46, Loss: 130.5213\n",
      "Epoch: 47, Loss: 130.3335\n",
      "Epoch: 48, Loss: 130.1409\n",
      "Epoch: 49, Loss: 129.9402\n",
      "Epoch: 49, Validation Loss: 217.3225, AUC Score: 0.8276\n",
      "Epoch: 50, Loss: 129.7309\n",
      "Epoch: 51, Loss: 129.5145\n",
      "Epoch: 52, Loss: 129.2943\n",
      "Epoch: 53, Loss: 129.0745\n",
      "Epoch: 54, Loss: 128.8597\n",
      "Epoch: 54, Validation Loss: 215.3194, AUC Score: 0.8389\n",
      "Epoch: 55, Loss: 128.6543\n",
      "Epoch: 56, Loss: 128.4617\n",
      "Epoch: 57, Loss: 128.2841\n",
      "Epoch: 58, Loss: 128.1224\n",
      "Epoch: 59, Loss: 127.9762\n",
      "Epoch: 59, Validation Loss: 213.6861, AUC Score: 0.8474\n",
      "Epoch: 60, Loss: 127.8439\n",
      "Epoch: 61, Loss: 127.7233\n",
      "Epoch: 62, Loss: 127.6118\n",
      "Epoch: 63, Loss: 127.5069\n",
      "Epoch: 64, Loss: 127.4065\n",
      "Epoch: 64, Validation Loss: 212.5848, AUC Score: 0.8523\n",
      "Epoch: 65, Loss: 127.3094\n",
      "Epoch: 66, Loss: 127.2148\n",
      "Epoch: 67, Loss: 127.1231\n",
      "Epoch: 68, Loss: 127.0349\n",
      "Epoch: 69, Loss: 126.9511\n",
      "Epoch: 69, Validation Loss: 211.7628, AUC Score: 0.8559\n",
      "Epoch: 70, Loss: 126.8730\n",
      "Epoch: 71, Loss: 126.8014\n",
      "Epoch: 72, Loss: 126.7369\n",
      "Epoch: 73, Loss: 126.6796\n",
      "Epoch: 74, Loss: 126.6293\n",
      "Epoch: 74, Validation Loss: 211.2318, AUC Score: 0.8555\n",
      "Epoch: 75, Loss: 126.5854\n",
      "Epoch: 76, Loss: 126.5469\n",
      "Epoch: 77, Loss: 126.5129\n",
      "Epoch: 78, Loss: 126.4823\n",
      "Epoch: 79, Loss: 126.4545\n",
      "Epoch: 79, Validation Loss: 210.8823, AUC Score: 0.8584\n",
      "Epoch: 80, Loss: 126.4287\n",
      "Epoch: 81, Loss: 126.4047\n",
      "Epoch: 82, Loss: 126.3824\n",
      "Epoch: 83, Loss: 126.3617\n",
      "Epoch: 84, Loss: 126.3429\n",
      "Epoch: 84, Validation Loss: 210.5774, AUC Score: 0.8604\n",
      "Epoch: 85, Loss: 126.3261\n",
      "Epoch: 86, Loss: 126.3115\n",
      "Epoch: 87, Loss: 126.2989\n",
      "Epoch: 88, Loss: 126.2883\n",
      "Epoch: 89, Loss: 126.2795\n",
      "Epoch: 89, Validation Loss: 210.3607, AUC Score: 0.8628\n",
      "Epoch: 90, Loss: 126.2720\n",
      "Epoch: 91, Loss: 126.2657\n",
      "Epoch: 92, Loss: 126.2601\n",
      "Epoch: 93, Loss: 126.2550\n",
      "Epoch: 94, Loss: 126.2501\n",
      "Epoch: 94, Validation Loss: 210.2489, AUC Score: 0.8633\n",
      "Epoch: 95, Loss: 126.2454\n",
      "Epoch: 96, Loss: 126.2409\n",
      "Epoch: 97, Loss: 126.2366\n",
      "Epoch: 98, Loss: 126.2324\n",
      "Epoch: 99, Loss: 126.2286\n",
      "Epoch: 99, Validation Loss: 210.2065, AUC Score: 0.8629\n",
      "Epoch: 100, Loss: 126.2251\n",
      "Epoch: 101, Loss: 126.2218\n",
      "Epoch: 102, Loss: 126.2188\n",
      "Epoch: 103, Loss: 126.2160\n",
      "Epoch: 104, Loss: 126.2132\n",
      "Epoch: 104, Validation Loss: 210.1942, AUC Score: 0.8630\n",
      "Epoch: 105, Loss: 126.2104\n",
      "Epoch: 106, Loss: 126.2076\n",
      "Epoch: 107, Loss: 126.2047\n",
      "Epoch: 108, Loss: 126.2017\n",
      "Epoch: 109, Loss: 126.1987\n",
      "Epoch: 109, Validation Loss: 210.1766, AUC Score: 0.8630\n",
      "Epoch: 110, Loss: 126.1957\n",
      "Epoch: 111, Loss: 126.1927\n",
      "Epoch: 112, Loss: 126.1898\n",
      "Epoch: 113, Loss: 126.1870\n",
      "Epoch: 114, Loss: 126.1843\n",
      "Epoch: 114, Validation Loss: 210.1610, AUC Score: 0.8630\n",
      "Epoch: 115, Loss: 126.1816\n",
      "Epoch: 116, Loss: 126.1790\n",
      "Epoch: 117, Loss: 126.1765\n",
      "Epoch: 118, Loss: 126.1740\n",
      "Epoch: 119, Loss: 126.1715\n",
      "Epoch: 119, Validation Loss: 210.1635, AUC Score: 0.8630\n",
      "Epoch: 120, Loss: 126.1690\n",
      "Epoch: 121, Loss: 126.1666\n",
      "Epoch: 122, Loss: 126.1642\n",
      "Epoch: 123, Loss: 126.1619\n",
      "Epoch: 124, Loss: 126.1597\n",
      "Epoch: 124, Validation Loss: 210.1805, AUC Score: 0.8633\n",
      "Epoch: 125, Loss: 126.1576\n",
      "Epoch: 126, Loss: 126.1555\n",
      "Epoch: 127, Loss: 126.1536\n",
      "Epoch: 128, Loss: 126.1516\n",
      "Epoch: 129, Loss: 126.1498\n",
      "Epoch: 129, Validation Loss: 210.1976, AUC Score: 0.8631\n",
      "Early stopping at epoch 129\n",
      "Best Score: 0.8633\n"
     ]
    }
   ],
   "source": [
    "# number of features and layers\n",
    "n_features = 2\n",
    "n_layers = 3\n",
    "\n",
    "# Random weight initialization\n",
    "weights = 0.01 * np.random.randn(n_layers, n_features, 3, requires_grad=True)\n",
    "\n",
    "# We create a quantum device with n_features \"wires\" (or qubits)\n",
    "dev = qml.device('default.qubit', wires=n_features)\n",
    "\n",
    "lr=0.01\n",
    "\n",
    "# train the model\n",
    "best_score, best_weights = train (n_features, n_layers,x_train,y_train,lr, weights, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7181271805947544\n"
     ]
    }
   ],
   "source": [
    "# test the model\n",
    "auc_score = test(n_features, n_layers,x_test,y_test,w_test, best_weights)\n",
    "print (auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bcbb91f107d4117b618730a8c11ceb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/1000 [00:00<?, ?epoch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 153.8807\n",
      "Epoch: 1, Loss: 153.8620\n",
      "Epoch: 2, Loss: 153.8439\n",
      "Epoch: 3, Loss: 153.8262\n",
      "Epoch: 4, Loss: 153.8086\n",
      "Epoch: 4, Validation Loss: 259.4717, AUC Score: 0.6768\n",
      "Epoch: 5, Loss: 153.7904\n",
      "Epoch: 6, Loss: 153.7707\n",
      "Epoch: 7, Loss: 153.7492\n",
      "Epoch: 8, Loss: 153.7259\n",
      "Epoch: 9, Loss: 153.7004\n",
      "Epoch: 9, Validation Loss: 259.2710, AUC Score: 0.6768\n",
      "Epoch: 10, Loss: 153.6728\n",
      "Epoch: 11, Loss: 153.6430\n",
      "Epoch: 12, Loss: 153.6108\n",
      "Epoch: 13, Loss: 153.5764\n",
      "Epoch: 14, Loss: 153.5394\n",
      "Epoch: 14, Validation Loss: 258.9752, AUC Score: 0.6768\n",
      "Epoch: 15, Loss: 153.5000\n",
      "Epoch: 16, Loss: 153.4581\n",
      "Epoch: 17, Loss: 153.4135\n",
      "Epoch: 18, Loss: 153.3663\n",
      "Epoch: 19, Loss: 153.3163\n",
      "Epoch: 19, Validation Loss: 258.5701, AUC Score: 0.6769\n",
      "Epoch: 20, Loss: 153.2636\n",
      "Epoch: 21, Loss: 153.2081\n",
      "Epoch: 22, Loss: 153.1498\n",
      "Epoch: 23, Loss: 153.0885\n",
      "Epoch: 24, Loss: 153.0244\n",
      "Epoch: 24, Validation Loss: 258.0449, AUC Score: 0.6769\n",
      "Epoch: 25, Loss: 152.9573\n",
      "Epoch: 26, Loss: 152.8872\n",
      "Epoch: 27, Loss: 152.8142\n",
      "Epoch: 28, Loss: 152.7381\n",
      "Epoch: 29, Loss: 152.6590\n",
      "Epoch: 29, Validation Loss: 257.3922, AUC Score: 0.6771\n",
      "Epoch: 30, Loss: 152.5768\n",
      "Epoch: 31, Loss: 152.4916\n"
     ]
    }
   ],
   "source": [
    "# load the best score, best weights and best learning rate\n",
    "with open('best_score_pca_vqc.pickle', 'wb') as handle:\n",
    "    best_score = pickle.load(handle)\n",
    "\n",
    "with open('best_weights_pca_vqc.pickle', 'wb') as handle:\n",
    "    best_weights = pickle.load(handle)\n",
    "\n",
    "with open('best_lr_pca_vqc.pickle', 'wb') as handle:\n",
    "    best_lr = pickle.load(handle)\n",
    "    \n",
    "print (best_score)\n",
    "print (best_weights)\n",
    "print (best_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "auc_score = test(n_features, n_layers,x_test,y_test,w_test, best_weights)\n",
    "print (auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Without PCA (2 Features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_fcnc_pca = data_frame_fcnc.copy()\n",
    "data_frame_bkg_pca = data_frame_bkg.copy()\n",
    "\n",
    "# Drop the categorical features except label, weights and gen_split\n",
    "data_frame_fcnc_pca.drop(['gen_decay_filter', 'gen_filter', 'gen_n_btags', 'gen_sample', 'gen_sample_filter','gen_decay2','gen_decay1'], axis=1, inplace=True)\n",
    "data_frame_bkg_pca.drop(['gen_decay_filter', 'gen_filter', 'gen_n_btags', 'gen_sample', 'gen_sample_filter','gen_decay2','gen_decay1'], axis=1, inplace=True)\n",
    "\n",
    "# Drop the features that are not in both dataframes\n",
    "for feature in data_frame_fcnc_pca.columns.values:\n",
    "    if feature not in data_frame_bkg_pca.columns.values:\n",
    "        data_frame_fcnc_pca.drop([feature], axis=1, inplace=True)\n",
    "\n",
    "for feature in data_frame_bkg_pca.columns.values:\n",
    "    if feature not in data_frame_fcnc_pca.columns.values:\n",
    "        data_frame_bkg_pca.drop([feature], axis=1, inplace=True)\n",
    "        \n",
    "# Join the dataframes\n",
    "data_vqe = pd.concat([data_frame_fcnc_pca, data_frame_bkg_pca])\n",
    "\n",
    "# Substitute the labels \"signal\" and \"bkg\" by 1 and 0\n",
    "data_vqe = data_vqe.replace(['signal'], 1)\n",
    "data_vqe= data_vqe.replace(['bkg'], 0)\n",
    "\n",
    "#normalize the data except the categorical features and the weights\n",
    "DataFeatures_vqe = pd.Index(list(set(data_vqe.columns) - set([\"gen_label\", \"gen_xsec\", \"gen_split\"])))\n",
    "data_vqe [DataFeatures_vqe] = (((data_vqe [DataFeatures_vqe] - data_vqe [DataFeatures_vqe].min()) / (data_vqe [DataFeatures_vqe].max() - data_vqe [DataFeatures_vqe].min())) * 2 - 1) * (np.pi)\n",
    "\n",
    "# train, test and validation sets\n",
    "train = data_vqe.loc[data['gen_split'] == 'train']\n",
    "test = data_vqe.loc[data['gen_split'] == 'test']\n",
    "val = data_vqe.loc[data['gen_split'] == 'val']\n",
    "\n",
    "# divide the train data into signal and background and get 500 samples of each\n",
    "train_sgn = train.loc[train['gen_label'] == 1].sample(n=500, random_state=42)\n",
    "train_bkg = train.loc[train['gen_label'] == 0].sample(n=500, random_state=42)\n",
    "x_train = pd.concat([train_sgn, train_bkg])\n",
    "x_train = x_train.sample(frac=1, random_state=42)\n",
    "\n",
    "# divide the validation data into signal and background and get 500 samples of each\n",
    "val_sgn = val.loc[val['gen_label'] == 1].sample(n=500, random_state=42)\n",
    "val_bkg = val.loc[val['gen_label'] == 0].sample(n=500, random_state=42)\n",
    "x_val = pd.concat([val_sgn, val_bkg])\n",
    "x_val = x_val.sample(frac=1, random_state=42)\n",
    "\n",
    "# divide the test data into signal and background and get 500 samples of each\n",
    "test_sgn = test.loc[test['gen_label'] == 1].sample(n=500, random_state=42)\n",
    "test_bkg = test.loc[test['gen_label'] == 0].sample(n=500, random_state=42)\n",
    "x_test = pd.concat([test_sgn, test_bkg])\n",
    "x_test = x_test.sample(frac=1, random_state=42)\n",
    "\n",
    "# get an array with the labels for each set\n",
    "y_train = x_train['gen_label'].values\n",
    "y_val = x_val['gen_label'].values\n",
    "y_test = x_test['gen_label'].values\n",
    "\n",
    "# get an array with the weights for each set\n",
    "w_train = x_train[\"gen_xsec\"].values\n",
    "w_val = x_val[\"gen_xsec\"].values\n",
    "w_test = x_test[\"gen_xsec\"].values\n",
    "\n",
    "# get an array with the features for each set\n",
    "x_train = x_train[['MissingET_MET', 'Jet1_BTag']].values\n",
    "x_val = x_val[['MissingET_MET', 'Jet1_BTag']].values\n",
    "x_test = x_test[['MissingET_MET', 'Jet1_BTag']].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of features and layers\n",
    "n_features = 2\n",
    "n_layers = 3\n",
    "\n",
    "# Random weight initialization\n",
    "weights = 0.01 * np.random.randn(n_layers, n_features, 3, requires_grad=True)\n",
    "\n",
    "# We create a quantum device with n_features \"wires\" (or qubits)\n",
    "dev = qml.device('default.qubit', wires=n_features)\n",
    "\n",
    "# train the model\n",
    "best_score, best_weights = train (n_features, n_layers,x_train,y_train,0.01, weights, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "auc_score = test(n_features, n_layers,x_test,y_test,w_test, best_weights)\n",
    "print (auc_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grid search to find the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the best score, best weights and best learning rate\n",
    "with open('best_score_sem_pca_vqc.pickle', 'wb') as handle:\n",
    "    best_score = pickle.load(handle)\n",
    "\n",
    "with open('best_weights_sem_pca_vqc.pickle', 'wb') as handle:\n",
    "    best_weights = pickle.load(handle)\n",
    " \n",
    "with open('best_lr_sem_pca_vqc.pickle', 'wb') as handle:\n",
    "    pickle.dump(best_lr, handle)\n",
    "    \n",
    "print (best_score)\n",
    "print (best_weights)\n",
    "print (best_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the model\n",
    "auc_score = test(n_features, n_layers,x_test,y_test,w_test, best_weights)\n",
    "print (auc_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
